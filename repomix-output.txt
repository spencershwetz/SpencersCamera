This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-31T04:51:54.866Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
camera/
  App/
    AppDelegate.swift
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    Contents.json
  camera/
    Features/
      Camera/
        Services/
          VideoFormatService.swift
  camera.xcdatamodeld/
    camera.xcdatamodel/
      contents
    .xccurrentversion
  Core/
    Extensions/
      CIContext+Shared.swift
      UIDeviceOrientation+Extensions.swift
      View+Extensions.swift
    Orientation/
      CameraOrientationLock.swift
      DeviceOrientationViewModel.swift
      DeviceRotationViewModifier.swift
      OrientationFixView.swift
      RotatingView.swift
  Features/
    Camera/
      Extensions/
        AVFoundationExtensions.swift
      Models/
        CameraError.swift
        CameraLens.swift
        ShutterAngle.swift
      Services/
        CameraDeviceService.swift
        CameraSetupService.swift
        ExposureService.swift
        RecordingService.swift
        VideoFormatService.swift
        VideoOutputDelegate.swift
        VolumeButtonHandler.swift
      Utilities/
        DocumentPicker.swift
      Views/
        CameraPreviewImplementation.swift
        CameraPreviewView.swift
        CameraView.swift
        ContentView.swift
        FunctionButtonsView.swift
        LensSelectionView.swift
        SettingsView.swift
        TestDynamicIslandOverlayView.swift
        ZoomSliderView.swift
      CameraViewModel.swift
      CameraViewModel.swift.old
      FlashlightManager.swift
    LUT/
      Utils/
        LUTProcessor.swift
      Views/
        LUTVideoPreviewView.swift
      CubeLUTLoader.swift
      LUTManager.swift
    Settings/
      FlashlightSettingsView.swift
      SettingsModel.swift
    VideoLibrary/
      VideoLibraryView.swift
      VideoLibraryViewModel.swift
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  cameraApp.swift
  Info.plist
  Persistence.swift
camera.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcuserdata/
    spencer.xcuserdatad/
      xcdebugger/
        Breakpoints_v2.xcbkptlist
      xcschemes/
        xcschememanagement.plist
  project.pbxproj
.gitignore

================================================================
Repository Files
================================================================

================
File: camera/App/AppDelegate.swift
================
import UIKit
import SwiftUI

/// Main application delegate that handles orientation locking and other app-level functionality
class AppDelegate: UIResponder, UIApplicationDelegate {
    var window: UIWindow?

    // MARK: - Orientation Lock Properties
    
    /// Static variable to track view controllers that need landscape support
    static var landscapeEnabledViewControllers: [String] = [
        "VideoLibraryView", 
        "VideoPlayerView",
        "OrientationFixViewController",
        "PresentationHostingController"
    ]
    
    // Track if the video library is currently being presented
    static var isVideoLibraryPresented: Bool = false
    
    // MARK: - Application Lifecycle
    
    func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {
        print("DEBUG: AppDelegate - Application launching")
        
        // Force dark mode at UIApplication level
        window?.enforceDarkMode()
            
        // Update to use UIWindowScene.windows
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
            windowScene.windows.forEach { window in
                window.enforceDarkMode()
                
                // Disable safe area insets for all windows
                window.rootViewController?.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
            }
        }
        
        // Create and configure window
        window = UIWindow(frame: UIScreen.main.bounds)
        window?.backgroundColor = .black
        
        // Configure root view controller
        let contentView = ContentView()
        let hostingController = UIHostingController(rootView: contentView)
        hostingController.view.backgroundColor = .black
        
        // Force dark mode for view controller
        hostingController.overrideUserInterfaceStyle = .dark
        
        // Set modal presentation style
        hostingController.modalPresentationStyle = .overFullScreen
        
        // Disable safe area insets completely
        hostingController.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        hostingController.view.frame = UIScreen.main.bounds
        
        // Set window properties
        window?.rootViewController = hostingController
        window?.makeKeyAndVisible()
        
        // Disable safe area insets at window level again
        window?.rootViewController?.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        
        // Force dark mode again after window is visible
        window?.enforceDarkMode()
        
        // Force black backgrounds
        if let rootView = window?.rootViewController?.view {
            forceBlackBackgrounds(rootView)
        }
        
        // Inspect view hierarchy colors
        inspectViewHierarchyBackgroundColors(hostingController.view)
        
        // Register for device orientation notifications
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
        
        // Setup orientation lock observer
        UIWindowScene.setupOrientationLockSupport()
        
        // Remove debug observer for orientation
        NotificationCenter.default.removeObserver(self, name: UIDevice.orientationDidChangeNotification, object: nil)
        
        return true
    }
    
    func applicationWillTerminate(_ application: UIApplication) {
        // Stop device orientation notifications to clean up
        UIDevice.current.endGeneratingDeviceOrientationNotifications()
    }
    
    // MARK: - Debug Helpers
    
    private func inspectViewHierarchyBackgroundColors(_ view: UIView, level: Int = 0) {
        let indent = String(repeating: "  ", count: level)
        print("\(indent)DEBUG: View \(type(of: view)) - backgroundColor: \(view.backgroundColor?.debugDescription ?? "nil")")
        
        // Get superview chain
        if level == 0 {
            var currentView: UIView? = view
            var superviewLevel = 0
            while let superview = currentView?.superview {
                print("\(indent)DEBUG: Superview \(superviewLevel) - Type: \(type(of: superview)) - backgroundColor: \(superview.backgroundColor?.debugDescription ?? "nil")")
                currentView = superview
                superviewLevel += 1
            }
        }
        
        for subview in view.subviews {
            inspectViewHierarchyBackgroundColors(subview, level: level + 1)
        }
    }
    
    // MARK: - Helper to force black backgrounds
    
    private func forceBlackBackgrounds(_ view: UIView) {
        // Force black background on the view itself
        view.backgroundColor = .black
        
        // Special handling for system views
        let systemViewClasses = [
            "UIDropShadowView",
            "UITransitionView",
            "UINavigationTransitionView",
            "_UIInteractiveHighlightEffectWindow"
        ]
        
        for className in systemViewClasses {
            if let viewClass = NSClassFromString(className),
               view.isKind(of: viewClass) {
                view.backgroundColor = .black
                view.layer.backgroundColor = UIColor.black.cgColor
            }
        }
        
        // Handle status bar background
        if view.bounds.height <= 50 && view.bounds.minY == 0 {
            view.backgroundColor = .black
            view.layer.backgroundColor = UIColor.black.cgColor
        }
        
        // Recursively process subviews
        for subview in view.subviews {
            forceBlackBackgrounds(subview)
        }
    }
    
    // MARK: - Orientation Support
    
    /// Handle orientation lock dynamically based on the current view controller
    func application(_ application: UIApplication, supportedInterfaceOrientationsFor window: UIWindow?) -> UIInterfaceOrientationMask {
        // If video library is flagged as presented, always allow landscape
        if AppDelegate.isVideoLibraryPresented {
            print("DEBUG: AppDelegate allowing landscape for video library")
            return [.portrait, .landscapeLeft, .landscapeRight]
        }
        
        // Get the top view controller
        if let topViewController = window?.rootViewController?.topMostViewController() {
            let vcName = String(describing: type(of: topViewController))
            
            // Check if this is a presentation controller that contains our view
            if vcName.contains("PresentationHostingController") {
                // For SwiftUI presentation controllers, we need to check their content
                if let childController = topViewController.children.first {
                    let childName = String(describing: type(of: childController))
                    print("DEBUG: PresentationHostingController contains: \(childName)")
                    
                    // Check if any child view controller supports landscape orientation
                    for controller in topViewController.children {
                        let controllerName = String(describing: type(of: controller))
                        if AppDelegate.landscapeEnabledViewControllers.contains(where: { controllerName.contains($0) }) {
                            print("DEBUG: AppDelegate allowing landscape for child: \(controllerName)")
                            return [.portrait, .landscapeLeft, .landscapeRight]
                        }
                    }
                    
                    // If the child name contains any of our landscape enabled controllers, allow landscape
                    if AppDelegate.landscapeEnabledViewControllers.contains(where: { childName.contains($0) }) {
                        print("DEBUG: AppDelegate allowing landscape for child: \(childName)")
                        return [.portrait, .landscapeLeft, .landscapeRight]
                    }
                }
                
                // If we can't determine the content, check if it's a full screen presentation
                if topViewController.modalPresentationStyle == .fullScreen {
                    print("DEBUG: AppDelegate allowing landscape for full screen presentation")
                    return [.portrait, .landscapeLeft, .landscapeRight]
                }
            }
            
            // Check if the current view controller allows landscape
            if let orientationViewController = topViewController as? OrientationFixViewController {
                // Use property from our custom view controller
                if orientationViewController.allowsLandscapeMode {
                    print("DEBUG: AppDelegate allowing landscape for OrientationFixViewController")
                    return [.portrait, .landscapeLeft, .landscapeRight]
                }
            }
            
            // Check the VC name against our list
            if AppDelegate.landscapeEnabledViewControllers.contains(where: { vcName.contains($0) }) {
                print("DEBUG: AppDelegate allowing landscape for \(vcName)")
                return [.portrait, .landscapeLeft, .landscapeRight]
            }
            
            print("DEBUG: AppDelegate enforcing portrait for \(vcName)")
        }
        
        // Default to portrait only
        return .portrait
    }
}

// MARK: - Extensions

extension UIViewController {
    /// Get the top-most presented view controller
    func topMostViewController() -> UIViewController {
        if let presented = self.presentedViewController {
            return presented.topMostViewController()
        }
        
        if let navigation = self as? UINavigationController {
            return navigation.visibleViewController?.topMostViewController() ?? navigation
        }
        
        if let tab = self as? UITabBarController {
            return tab.selectedViewController?.topMostViewController() ?? tab
        }
        
        return self
    }
}

// MARK: - UIWindow Extension
extension UIWindow {
    /// Enforce dark mode for the window
    func enforceDarkMode() {
        if #available(iOS 13.0, *) {
            self.overrideUserInterfaceStyle = .dark
        }
    }
}

================
File: camera/Assets.xcassets/AccentColor.colorset/Contents.json
================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/AppIcon.appiconset/Contents.json
================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/camera/Features/Camera/Services/VideoFormatService.swift
================
// ... existing code ...
            guard !formats.isEmpty else {
                logger.error("No suitable Apple Log format found for \(device.localizedName) (\(String(describing: device.deviceType)))")
                
                // Additional logging for debugging ultrawide lens issues
                if device.deviceType == .builtInUltraWideCamera {
                    logger.error("‚ùå Ultra-wide (0.5√ó) lens does not support Apple Log on this device model")
                    
                    // Throw a specific error for the ultrawide lens
                    throw CameraError.configurationFailed(message: "Apple Log is not supported on the 0.5√ó ultra-wide lens")
                }
                
                throw CameraError.configurationFailed
            }
// ... existing code ...

================
File: camera/camera.xcdatamodeld/camera.xcdatamodel/contents
================
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<model type="com.apple.IDECoreDataModeler.DataModel" documentVersion="1.0" lastSavedToolsVersion="1" systemVersion="11A491" minimumToolsVersion="Automatic" sourceLanguage="Swift" usedWithCloudKit="false" userDefinedModelVersionIdentifier="">
    <entity name="Item" representedClassName="Item" syncable="YES" codeGenerationType="class">
        <attribute name="timestamp" optional="YES" attributeType="Date" usesScalarValueType="NO"/>
    </entity>
    <elements>
        <element name="Item" positionX="-63" positionY="-18" width="128" height="44"/>
    </elements>
</model>

================
File: camera/camera.xcdatamodeld/.xccurrentversion
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>_XCCurrentVersionName</key>
	<string>camera.xcdatamodel</string>
</dict>
</plist>

================
File: camera/Core/Extensions/CIContext+Shared.swift
================
import CoreImage

extension CIContext {
    static let shared: CIContext = {
        let options = [
            CIContextOption.workingColorSpace: CGColorSpace(name: CGColorSpace.displayP3)!,
            CIContextOption.useSoftwareRenderer: false
        ]
        return CIContext(options: options)
    }()
}

================
File: camera/Core/Extensions/UIDeviceOrientation+Extensions.swift
================
import UIKit
import SwiftUI

// MARK: - UIDeviceOrientation Extensions
extension UIDeviceOrientation {
    /// Check if the orientation is portrait (portrait or portraitUpsideDown)
    var isPortrait: Bool {
        return self == .portrait || self == .portraitUpsideDown
    }
    
    /// Check if the orientation is landscape (landscapeLeft or landscapeRight)
    var isLandscape: Bool {
        return self == .landscapeLeft || self == .landscapeRight
    }
    
    /// Check if the orientation is a valid interface orientation
    var isValidInterfaceOrientation: Bool {
        return isPortrait || isLandscape
    }
    
    /// Returns the rotation angle in degrees (0, 90, 180, 270) for video rotation
    var videoRotationAngleValue: CGFloat {
        switch self {
        case .portrait:
            return 90.0  // Portrait mode: rotate 90¬∞ clockwise
        case .landscapeRight:  // USB port on left
            return 180.0  // Landscape with USB on left: rotate 180¬∞
        case .landscapeLeft:  // USB port on right
            return 0.0  // Landscape with USB on right: no rotation
        case .portraitUpsideDown:
            return 270.0
        case .unknown, .faceUp, .faceDown:
            return 90.0  // Default to portrait mode
        @unknown default:
            return 90.0
        }
    }
    
    /// Transform to apply for video orientation based on device orientation
    var videoTransform: CGAffineTransform {
        let angle: CGFloat
        switch self {
        case .portrait:
            angle = .pi / 2  // 90¬∞ clockwise
        case .landscapeRight:  // USB port on left
            angle = .pi  // 180¬∞
        case .landscapeLeft:  // USB port on right
            angle = 0  // No rotation
        case .portraitUpsideDown:
            angle = -.pi / 2  // 270¬∞
        case .unknown, .faceUp, .faceDown:
            angle = .pi / 2  // Default to portrait mode
        @unknown default:
            angle = .pi / 2
        }
        return CGAffineTransform(rotationAngle: angle)
    }
}

// ADD: StatusBarHidingModifier
struct StatusBarHidingModifier: ViewModifier {
    func body(content: Content) -> some View {
        content
            .statusBar(hidden: true)
    }
}

extension View {
    func hideStatusBar() -> some View {
        modifier(StatusBarHidingModifier())
    }
}

================
File: camera/Core/Extensions/View+Extensions.swift
================
import SwiftUI
import UIKit

// View extension to disable safe area insets completely
extension View {
    func disableSafeArea() -> some View {
        self.modifier(SafeAreaDisabler())
    }
}

// A UIViewControllerRepresentable that completely disables safe area insets
struct SafeAreaDisabler: ViewModifier {
    func body(content: Content) -> some View {
        content
            .background(SafeAreaDisablerView())
            .ignoresSafeArea(.all)
    }
}

struct SafeAreaDisablerView: UIViewControllerRepresentable {
    func makeUIViewController(context: Context) -> UIViewController {
        SafeAreaDisablerController()
    }
    
    func updateUIViewController(_ uiViewController: UIViewController, context: Context) {}
    
    private class SafeAreaDisablerController: UIViewController {
        override func viewDidLoad() {
            super.viewDidLoad()
            // Make view transparent
            view.backgroundColor = .clear
            // Disable safe area insets for this controller
            self.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        }
        
        override func viewWillAppear(_ animated: Bool) {
            super.viewWillAppear(animated)
            // Force negative insets to override the system safe areas
            self.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        }
        
        override func viewDidLayoutSubviews() {
            super.viewDidLayoutSubviews()
            // Apply negative insets after layout
            self.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        }
        
        override var preferredStatusBarStyle: UIStatusBarStyle {
            return .lightContent
        }
        
        override var prefersStatusBarHidden: Bool {
            return true
        }
    }
}

================
File: camera/Core/Orientation/CameraOrientationLock.swift
================
import UIKit

/// Utility class to lock the device orientation for camera operations
final class CameraOrientationLock {
    
    private static var isLocked = false
    private static var lastLockTime: TimeInterval = 0
    private static var activeTransitionTimer: Timer?
    
    /// Lock the device orientation to portrait - this is the only orientation we support
    static func lockToPortrait() {
        isLocked = true
        lastLockTime = CACurrentMediaTime()
        
        // Cancel any existing transition timer
        activeTransitionTimer?.invalidate()
        
        // Force the orientation to portrait
        if #available(iOS 16.0, *) {
            // Use the correct approach for iOS 16+
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                // Request geometry update to lock to portrait
                windowScene.requestGeometryUpdate(.iOS(interfaceOrientations: .portrait))
                
                // Update all root view controllers in all windows
                for window in windowScene.windows {
                    window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                }
            }
            
            // Update orientation for all scenes and windows
            for scene in UIApplication.shared.connectedScenes {
                if let windowScene = scene as? UIWindowScene {
                    for window in windowScene.windows {
                        window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                    }
                }
            }
        } else {
            // Fallback for older iOS versions
            // Direct orientation setting (for pre-iOS 16 only)
            UIDevice.current.setValue(UIDeviceOrientation.portrait.rawValue, forKey: "orientation")
            UIViewController.attemptRotationToDeviceOrientation()
        }
        
        // Schedule a sequence of re-locks to handle system auto-rotation attempts
        // This helps ensure consistent behavior during portrait-to-landscape transitions
        scheduleReorientationSequence()
        
        print("üîí Camera preview locked to portrait orientation (90¬∞)")
    }
    
    /// Schedule a sequence of orientation reapplications to handle transition edge cases
    private static func scheduleReorientationSequence() {
        // Cancel any existing timer
        activeTransitionTimer?.invalidate()
        
        // Create a repeating timer that will check and reapply orientation lock as needed
        activeTransitionTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { timer in
            // Get the current device orientation
            let currentOrientation = UIDevice.current.orientation
            
            // If device is in landscape but our interface should be portrait, force a stronger update
            if currentOrientation.isLandscape {
                if #available(iOS 16.0, *) {
                    // Force all windows to update
                    for scene in UIApplication.shared.connectedScenes {
                        if let windowScene = scene as? UIWindowScene {
                            // Apply the geometry update more aggressively during landscape detection
                            windowScene.requestGeometryUpdate(.iOS(interfaceOrientations: .portrait))
                            
                            for window in windowScene.windows {
                                // Force update the view controller orientation
                                window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                                
                                // Also notify any child view controllers that might need updating
                                notifyChildViewControllersOfOrientationChange(window.rootViewController)
                            }
                        }
                    }
                    
                    // Post a notification to inform custom views that they may need to update
                    NotificationCenter.default.post(name: .orientationLockEnforced, object: nil)
                }
            }
            
            // Stop the timer after 2 seconds (20 checks at 0.1s interval)
            // This prevents unnecessary CPU usage while still covering the transition period
            if CACurrentMediaTime() - lastLockTime > 2.0 {
                timer.invalidate()
            }
        }
    }
    
    /// Notify all child view controllers to update their orientations
    private static func notifyChildViewControllersOfOrientationChange(_ viewController: UIViewController?) {
        guard let viewController = viewController else { return }
        
        // Update this view controller
        viewController.setNeedsUpdateOfSupportedInterfaceOrientations()
        
        // Update presented view controller if any
        if let presented = viewController.presentedViewController {
            notifyChildViewControllersOfOrientationChange(presented)
        }
        
        // Update children
        for child in viewController.children {
            notifyChildViewControllersOfOrientationChange(child)
        }
    }
    
    /// Force immediate application of the portrait orientation lock
    static func forceOrientationUpdate() {
        if #available(iOS 16.0, *) {
            // Use the proper iOS 16+ approach
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                // Request geometry update with portrait orientation
                windowScene.requestGeometryUpdate(.iOS(interfaceOrientations: .portrait))
                
                // Update all windows in the scene
                for window in windowScene.windows {
                    window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                    
                    // Notify child view controllers
                    notifyChildViewControllersOfOrientationChange(window.rootViewController)
                }
            }
            
            // Update all scenes and windows
            for scene in UIApplication.shared.connectedScenes {
                if let windowScene = scene as? UIWindowScene {
                    for window in windowScene.windows {
                        window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                    }
                }
            }
            
            // Post notification for custom views
            NotificationCenter.default.post(name: .orientationLockEnforced, object: nil)
        } else {
            // Fallback for older iOS versions
            UIDevice.current.setValue(UIDeviceOrientation.portrait.rawValue, forKey: "orientation")
            UIViewController.attemptRotationToDeviceOrientation()
        }
        
        print("üîÑ Forced orientation update to portrait")
    }
    
    /// Handle device orientation change - call this from orientation change handlers
    static func handleDeviceOrientationChange(_ newOrientation: UIDeviceOrientation) {
        print("üîÑ Device orientation changed: \(newOrientation) (value: \(newOrientation.rawValue))")
        
        // If the device is in landscape, enforce portrait lock
        if newOrientation.isLandscape {
            // Refresh the lock to ensure it's maintained during landscape transition
            lockToPortrait()
        }
    }
}

/// Extension to be used in SceneDelegate or App to enforce orientation lock
extension UIWindowScene {
    static var orientationLockObserver: NSObjectProtocol? = nil
    
    /// Setup orientation lock support in your SceneDelegate or SwiftUI App
    static func setupOrientationLockSupport() {
        if orientationLockObserver == nil {
            orientationLockObserver = NotificationCenter.default.addObserver(
                forName: UIDevice.orientationDidChangeNotification,
                object: nil,
                queue: .main
            ) { _ in
                // Handle device orientation change
                let newOrientation = UIDevice.current.orientation
                CameraOrientationLock.handleDeviceOrientationChange(newOrientation)
                
                if #available(iOS 16.0, *) {
                    // For iOS 16+, iterate through all scenes and call setNeedsUpdateOfSupportedInterfaceOrientations
                    for scene in UIApplication.shared.connectedScenes {
                        if let windowScene = scene as? UIWindowScene {
                            for window in windowScene.windows {
                                window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                            }
                        }
                    }
                } else {
                    // Fallback for older iOS versions
                    UIViewController.attemptRotationToDeviceOrientation()
                }
            }
        }
    }
}

// Define a notification name for custom views to listen for orientation lock enforcement
extension Notification.Name {
    static let orientationLockEnforced = Notification.Name("orientationLockEnforced")
}

================
File: camera/Core/Orientation/DeviceOrientationViewModel.swift
================
import SwiftUI
import Combine

class DeviceOrientationViewModel: ObservableObject {
    @Published var orientation: UIDeviceOrientation = .portrait
    private var cancellables = Set<AnyCancellable>()
    
    init() {
        print("DEBUG: [OrientationVM] Initializing DeviceOrientationViewModel")
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
        
        NotificationCenter.default.publisher(for: UIDevice.orientationDidChangeNotification)
            .compactMap { _ in UIDevice.current.orientation }
            .filter { orientation in
                // Only handle valid interface orientations
                switch orientation {
                case .portrait, .portraitUpsideDown, .landscapeLeft, .landscapeRight:
                    print("DEBUG: [OrientationVM] Valid orientation detected: \(orientation.rawValue)")
                    return true
                default:
                    print("DEBUG: [OrientationVM] Ignoring invalid orientation: \(orientation.rawValue)")
                    return false
                }
            }
            .sink { [weak self] newOrientation in
                print("DEBUG: [OrientationVM] Updating orientation to: \(newOrientation.rawValue)")
                withAnimation(.easeInOut(duration: 0.3)) {
                    self?.orientation = newOrientation
                }
            }
            .store(in: &cancellables)
    }
    
    deinit {
        print("DEBUG: [OrientationVM] Deinitializing DeviceOrientationViewModel")
        UIDevice.current.endGeneratingDeviceOrientationNotifications()
    }
    
    var rotationAngle: Angle {
        switch orientation {
        case .landscapeLeft:
            print("DEBUG: [OrientationVM] Rotating left (90¬∞)")
            return .degrees(-90)
        case .landscapeRight:
            print("DEBUG: [OrientationVM] Rotating right (-90¬∞)")
            return .degrees(90)
        case .portraitUpsideDown:
            print("DEBUG: [OrientationVM] Rotating upside down (180¬∞)")
            return .degrees(180)
        default:
            print("DEBUG: [OrientationVM] No rotation (0¬∞)")
            return .degrees(0)
        }
    }
    
    var rotationOffset: CGSize {
        switch orientation {
        case .landscapeLeft, .landscapeRight:
            print("DEBUG: [OrientationVM] Applying landscape offset")
            return CGSize(width: 0, height: 0)
        default:
            print("DEBUG: [OrientationVM] Applying portrait offset")
            return .zero
        }
    }
}

================
File: camera/Core/Orientation/DeviceRotationViewModifier.swift
================
import SwiftUI

struct DeviceRotationViewModifier: ViewModifier {
    let orientationViewModel: DeviceOrientationViewModel
    
    func body(content: Content) -> some View {
        GeometryReader { geometry in
            content
                .position(x: geometry.size.width / 2, y: geometry.size.height / 2)
                .rotationEffect(orientationViewModel.rotationAngle)
                .animation(.easeInOut(duration: 0.3), value: orientationViewModel.orientation)
                .onChange(of: orientationViewModel.orientation) { oldValue, newValue in
                    print("DEBUG: [RotationModifier] Orientation changed from \(oldValue.rawValue) to \(newValue.rawValue)")
                    print("DEBUG: [RotationModifier] Applying rotation angle: \(orientationViewModel.rotationAngle.degrees)¬∞")
                    print("DEBUG: [RotationModifier] View frame: \(geometry.size)")
                }
        }
        .frame(width: 60, height: 60) // Match the button size
    }
}

extension View {
    func rotateWithDeviceOrientation(using orientationViewModel: DeviceOrientationViewModel) -> some View {
        print("DEBUG: [RotationModifier] Applying rotation modifier to view")
        return modifier(DeviceRotationViewModifier(orientationViewModel: orientationViewModel))
    }
}

================
File: camera/Core/Orientation/OrientationFixView.swift
================
import UIKit
import SwiftUI

/// A UIViewController that restricts orientation and hosts the camera preview
class OrientationFixViewController: UIViewController {
    private let contentView: UIView
    private(set) var allowsLandscapeMode: Bool
    private var hasAppliedInitialOrientation = false
    
    init(rootView: UIView, allowLandscape: Bool = false) {
        self.contentView = rootView
        self.allowsLandscapeMode = allowLandscape
        super.init(nibName: nil, bundle: nil)
        
        // Set black background color
        self.view.backgroundColor = .black
        
        // Configure modal presentation style based on landscape allowance
        if !allowsLandscapeMode {
            self.modalPresentationStyle = .fullScreen
            print("DEBUG: OrientationFixViewController initializing with portrait-only mode")
        } else {
            print("DEBUG: OrientationFixViewController initializing with all orientations allowed")
        }
    }
    
    required init?(coder: NSCoder) {
        fatalError("init(coder:) has not been implemented")
    }
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        // Ensure view background is black
        view.backgroundColor = .black
        
        // Add content view to controller's view
        contentView.frame = view.bounds
        contentView.autoresizingMask = [.flexibleWidth, .flexibleHeight]
        view.addSubview(contentView)
        
        // Disable safe area insets
        contentView.insetsLayoutMarginsFromSafeArea = false
        additionalSafeAreaInsets = .zero
        
        // Set black background for all parent views up the hierarchy
        setBlackBackgroundForAllParentViews()
        
        print("DEBUG: OrientationFixViewController viewDidLoad - background set to black")
    }
    
    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        
        // Set background to black
        view.backgroundColor = .black
        
        // Apply orientation settings
        if !allowsLandscapeMode {
            enforcePortraitOrientation()
            print("DEBUG: OrientationFixViewController viewWillAppear - enforcing portrait mode")
        } else {
            enableAllOrientations()
            print("DEBUG: OrientationFixViewController viewWillAppear - allowing all orientations")
        }
        
        // Set black background for all parent views
        setBlackBackgroundForAllParentViews()
    }
    
    override func viewDidAppear(_ animated: Bool) {
        super.viewDidAppear(animated)
        
        // Set black background for all parent views again after appearing
        setBlackBackgroundForAllParentViews()
        
        // Apply orientation settings again after a short delay
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            if !self.allowsLandscapeMode {
                self.enforcePortraitOrientation()
            } else {
                self.enableAllOrientations()
            }
        }
    }
    
    override func viewWillLayoutSubviews() {
        super.viewWillLayoutSubviews()
        
        // Force black background during layout
        view.backgroundColor = .black
        setBlackBackgroundForAllParentViews()
    }
    
    // Force zero safe area insets
    override var additionalSafeAreaInsets: UIEdgeInsets {
        get {
            return .zero
        }
        set {
            super.additionalSafeAreaInsets = .zero
        }
    }
    
    private func enforcePortraitOrientation() {
        // Use UIDevice notification-based orientation instead of the unsafe KVC approach
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
        
        // Set preferred orientation via UIApplication
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
            if #available(iOS 16.0, *) {
                windowScene.requestGeometryUpdate(.iOS(interfaceOrientations: .portrait))
            }
        }
        
        // Report current orientation settings
        print("DEBUG: Enforcing portrait orientation")
    }
    
    private func enableAllOrientations() {
        // Allow all orientations
        print("DEBUG: Enabling all orientations")
    }
    
    override var supportedInterfaceOrientations: UIInterfaceOrientationMask {
        if allowsLandscapeMode {
            return .all
        } else {
            return .portrait
        }
    }
    
    override var preferredInterfaceOrientationForPresentation: UIInterfaceOrientation {
        return .portrait
    }
    
    private func setBlackBackgroundForAllParentViews() {
        // Recursively set black background color on all parent views
        var currentView: UIView? = self.view
        while let view = currentView {
            view.backgroundColor = .black
            currentView = view.superview
        }
        
        // Also ensure all window backgrounds are black
        UIApplication.shared.connectedScenes
            .compactMap { $0 as? UIWindowScene }
            .flatMap { $0.windows }
            .forEach { $0.backgroundColor = .black }
        
        print("DEBUG: Set black background for all parent views")
    }
}

// Extension to make this available in SwiftUI
extension UIViewController {
    // Helper method to find the current active window scene
    func findActiveWindowScene() -> UIWindowScene? {
        return UIApplication.shared.connectedScenes
            .filter { $0.activationState == .foregroundActive }
            .compactMap { $0 as? UIWindowScene }
            .first
    }
}

// MARK: - SwiftUI Integration

/// A SwiftUI wrapper for the orientation fix view controller
struct OrientationFixView<Content: View>: UIViewControllerRepresentable {
    var content: Content
    var allowsLandscapeMode: Bool
    
    init(allowsLandscapeMode: Bool = false, @ViewBuilder content: () -> Content) {
        self.content = content()
        self.allowsLandscapeMode = allowsLandscapeMode
        
        // Set AppDelegate flag if we're initializing with landscape allowed
        if allowsLandscapeMode {
            AppDelegate.isVideoLibraryPresented = true
        }
    }
    
    func makeUIViewController(context: Context) -> OrientationFixViewController {
        // Create a hosting controller for the SwiftUI content
        let hostingController = UIHostingController(rootView: content)
        
        // Extract the UIView from the hosting controller
        let contentView = hostingController.view!
        contentView.backgroundColor = .black // Force black background
        
        // Force UIHostingController background to black as well
        hostingController.view.backgroundColor = .black
        
        // Create and return the orientation fix view controller
        return OrientationFixViewController(rootView: contentView, allowLandscape: allowsLandscapeMode)
    }
    
    func updateUIViewController(_ uiViewController: OrientationFixViewController, context: Context) {
        // If we allow landscape, ensure orientation is updated
        if allowsLandscapeMode {
            AppDelegate.isVideoLibraryPresented = true
            uiViewController.setNeedsUpdateOfSupportedInterfaceOrientations()
        }
    }
}

================
File: camera/Core/Orientation/RotatingView.swift
================
import SwiftUI
import UIKit

struct RotatingView<Content: View>: UIViewControllerRepresentable {
    let content: Content
    @ObservedObject var orientationViewModel: DeviceOrientationViewModel
    let invertRotation: Bool
    
    init(orientationViewModel: DeviceOrientationViewModel, invertRotation: Bool = false, @ViewBuilder content: () -> Content) {
        self.content = content()
        self._orientationViewModel = ObservedObject(wrappedValue: orientationViewModel)
        self.invertRotation = invertRotation
    }
    
    func makeUIViewController(context: Context) -> RotatingViewController<Content> {
        return RotatingViewController(rootView: content, orientationViewModel: orientationViewModel, invertRotation: invertRotation)
    }
    
    func updateUIViewController(_ uiViewController: RotatingViewController<Content>, context: Context) {
        uiViewController.updateContent(content)
        uiViewController.updateOrientation(orientationViewModel.orientation)
    }
}

class RotatingViewController<Content: View>: UIViewController {
    private var hostingController: UIHostingController<Content>
    private var orientationViewModel: DeviceOrientationViewModel
    private var invertRotation: Bool
    
    init(rootView: Content, orientationViewModel: DeviceOrientationViewModel, invertRotation: Bool) {
        self.hostingController = UIHostingController(rootView: rootView)
        self.orientationViewModel = orientationViewModel
        self.invertRotation = invertRotation
        super.init(nibName: nil, bundle: nil)
    }
    
    required init?(coder: NSCoder) {
        fatalError("init(coder:) has not been implemented")
    }
    
    override func viewDidLoad() {
        super.viewDidLoad()
        setupHostingController()
    }
    
    private func setupHostingController() {
        addChild(hostingController)
        view.addSubview(hostingController.view)
        hostingController.view.translatesAutoresizingMaskIntoConstraints = false
        NSLayoutConstraint.activate([
            hostingController.view.leadingAnchor.constraint(equalTo: view.leadingAnchor),
            hostingController.view.trailingAnchor.constraint(equalTo: view.trailingAnchor),
            hostingController.view.topAnchor.constraint(equalTo: view.topAnchor),
            hostingController.view.bottomAnchor.constraint(equalTo: view.bottomAnchor)
        ])
        hostingController.didMove(toParent: self)
    }
    
    func updateContent(_ newContent: Content) {
        hostingController.rootView = newContent
    }
    
    func updateOrientation(_ orientation: UIDeviceOrientation) {
        print("DEBUG: [RotatingViewController] Updating orientation to: \(orientation.rawValue)")
        var transform: CGAffineTransform = .identity
        
        switch orientation {
        case .landscapeLeft:
            transform = CGAffineTransform(rotationAngle: invertRotation ? CGFloat.pi / 2 : -CGFloat.pi / 2)
        case .landscapeRight:
            transform = CGAffineTransform(rotationAngle: invertRotation ? -CGFloat.pi / 2 : CGFloat.pi / 2)
        case .portraitUpsideDown:
            transform = CGAffineTransform(rotationAngle: CGFloat.pi)
        default:
            transform = .identity
        }
        
        UIView.animate(withDuration: 0.3) {
            self.hostingController.view.transform = transform
        }
    }
}

================
File: camera/Features/Camera/Extensions/AVFoundationExtensions.swift
================
import AVFoundation
import CoreMedia

// MARK: - Double Extensions

extension Double {
    func clamped(to range: ClosedRange<Double>) -> Double {
        return min(max(self, range.lowerBound), range.upperBound)
    }
}

// MARK: - CMTime Extensions

extension CMTime {
    var displayString: String {
        let totalSeconds = CMTimeGetSeconds(self)
        let hours = Int(totalSeconds / 3600)
        let minutes = Int(totalSeconds.truncatingRemainder(dividingBy: 3600) / 60)
        let seconds = Int(totalSeconds.truncatingRemainder(dividingBy: 60))
        
        if hours > 0 {
            return String(format: "%d:%02d:%02d", hours, minutes, seconds)
        } else {
            return String(format: "%02d:%02d", minutes, seconds)
        }
    }
}

// MARK: - CGSize Extensions

extension CGSize {
    var aspectRatio: CGFloat {
        return width / height
    }
    
    func scaledToFit(in containerSize: CGSize) -> CGSize {
        let scale = min(containerSize.width / width, containerSize.height / height)
        return CGSize(width: width * scale, height: height * scale)
    }
}

================
File: camera/Features/Camera/Models/CameraError.swift
================
import Foundation

enum CameraError: Error, Identifiable {
    case cameraUnavailable
    case setupFailed
    case configurationFailed
    case recordingFailed
    case savingFailed
    case whiteBalanceError
    case unauthorized
    case sessionFailedToStart
    case deviceUnavailable
    case invalidDeviceInput
    case custom(message: String)
    
    var id: String { description }
    
    var description: String {
        switch self {
        case .cameraUnavailable:
            return "Camera device not available"
        case .setupFailed:
            return "Failed to setup camera"
        case .configurationFailed:
            return "Failed to configure camera settings"
        case .recordingFailed:
            return "Failed to record video"
        case .savingFailed:
            return "Failed to save video to photo library"
        case .whiteBalanceError:
            return "Failed to adjust white balance settings"
        case .unauthorized:
            return "Camera access denied. Please allow camera access in Settings."
        case .sessionFailedToStart:
            return "Failed to start camera session"
        case .deviceUnavailable:
            return "Requested camera device is not available"
        case .invalidDeviceInput:
            return "Cannot add camera device input to session"
        case .custom(let message):
            return message
        }
    }
}

================
File: camera/Features/Camera/Models/CameraLens.swift
================
import AVFoundation

enum CameraLens: String, CaseIterable {
    case ultraWide = "0.5"
    case wide = "1"
    case x2 = "2"
    case telephoto = "5"
    
    var deviceType: AVCaptureDevice.DeviceType {
        switch self {
        case .ultraWide: return .builtInUltraWideCamera
        case .wide: return .builtInWideAngleCamera
        case .x2: return .builtInWideAngleCamera // Uses digital zoom on wide lens
        case .telephoto: return .builtInTelephotoCamera
        }
    }
    
    var zoomFactor: CGFloat {
        switch self {
        case .ultraWide: return 0.5
        case .wide: return 1.0
        case .x2: return 2.0
        case .telephoto: return 5.0
        }
    }
    
    var systemImageName: String {
        switch self {
        case .ultraWide: return "camera.lens.ultra.wide"
        case .wide: return "camera.lens.wide"
        case .x2: return "camera.lens.wide"
        case .telephoto: return "camera.lens.telephoto"
        }
    }
    
    static func availableLenses() -> [CameraLens] {
        var lenses = CameraLens.allCases.filter { lens in
            // For 2x, we only need the wide angle camera
            if lens == .x2 {
                return AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) != nil
            }
            return AVCaptureDevice.default(lens.deviceType, for: .video, position: .back) != nil
        }
        
        // Sort lenses by zoom factor
        lenses.sort { $0.zoomFactor < $1.zoomFactor }
        return lenses
    }
}

================
File: camera/Features/Camera/Models/ShutterAngle.swift
================
import Foundation

enum ShutterAngle: Double, CaseIterable {
    case angle_360 = 360.0  // 1/24
    case angle_345_6 = 345.6  // 1/25
    case angle_288 = 288.0  // 1/30
    case angle_262_2 = 262.2  // 1/33
    case angle_180 = 180.0  // 1/48
    case angle_172_8 = 172.8  // 1/50
    case angle_144 = 144.0  // 1/60
    case angle_90 = 90.0   // 1/96
    case angle_86_4 = 86.4  // 1/100
    case angle_72 = 72.0   // 1/120
    case angle_69_1 = 69.1  // 1/125
    case angle_34_6 = 34.6  // 1/250
    case angle_17_3 = 17.3  // 1/500
    case angle_8_6 = 8.6   // 1/1000
    case angle_4_3 = 4.3   // 1/2000
    case angle_2_2 = 2.2   // 1/4000
    case angle_1_1 = 1.1   // 1/8000
    
    var shutterSpeed: String {
        switch self {
        case .angle_360: return "1/24"
        case .angle_345_6: return "1/25"
        case .angle_288: return "1/30"
        case .angle_262_2: return "1/33"
        case .angle_180: return "1/48"
        case .angle_172_8: return "1/50"
        case .angle_144: return "1/60"
        case .angle_90: return "1/96"
        case .angle_86_4: return "1/100"
        case .angle_72: return "1/120"
        case .angle_69_1: return "1/125"
        case .angle_34_6: return "1/250"
        case .angle_17_3: return "1/500"
        case .angle_8_6: return "1/1000"
        case .angle_4_3: return "1/2000"
        case .angle_2_2: return "1/4000"
        case .angle_1_1: return "1/8000"
        }
    }
}

================
File: camera/Features/Camera/Services/CameraDeviceService.swift
================
import AVFoundation
import os.log
import UIKit

protocol CameraDeviceServiceDelegate: AnyObject {
    func didUpdateCurrentLens(_ lens: CameraLens)
    func didUpdateZoomFactor(_ factor: CGFloat)
    func didEncounterError(_ error: CameraError)
}

class CameraDeviceService {
    private let logger = Logger(subsystem: "com.camera", category: "CameraDeviceService")
    private weak var delegate: CameraDeviceServiceDelegate?
    private var session: AVCaptureSession
    private var videoDeviceInput: AVCaptureDeviceInput?
    private var device: AVCaptureDevice?
    private var lastZoomFactor: CGFloat = 1.0
    private let cameraQueue = DispatchQueue(label: "com.camera.device-service")
    
    init(session: AVCaptureSession, delegate: CameraDeviceServiceDelegate) {
        self.session = session
        self.delegate = delegate
    }
    
    func setDevice(_ device: AVCaptureDevice) {
        self.device = device
        logger.info("üì∏ Set initial device: \(device.localizedName)")
    }
    
    func setVideoDeviceInput(_ input: AVCaptureDeviceInput) {
        self.videoDeviceInput = input
        logger.info("üì∏ Set initial video input: \(input.device.localizedName)")
    }
    
    private func configureSession(for newDevice: AVCaptureDevice, lens: CameraLens) throws {
        // Remove existing inputs and outputs
        session.inputs.forEach { session.removeInput($0) }
        
        // Add new input
        let newInput = try AVCaptureDeviceInput(device: newDevice)
        guard session.canAddInput(newInput) else {
            logger.error("‚ùå Cannot add input for \(newDevice.localizedName)")
            throw CameraError.invalidDeviceInput
        }
        
        // Check if format supports Apple Log before switching
        if let currentDevice = device,
           currentDevice.activeColorSpace == .appleLog {
            // Find a format that supports Apple Log for the new device
            let formats = newDevice.formats.filter { format in
                let desc = format.formatDescription
                let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
                let hasAppleLog = format.supportedColorSpaces.contains(.appleLog)
                let resolution = dimensions.width >= 1920 && dimensions.height >= 1080
                return hasAppleLog && resolution
            }
            
            if formats.isEmpty {
                logger.warning("‚ö†Ô∏è \(lens.rawValue)√ó lens does not support Apple Log, reverting to sRGB")
                
                // Additional logging to help diagnose this issue
                if lens == .ultraWide {
                    logger.warning("üì∏ Ultra-wide (0.5√ó) lens doesn't support Apple Log on this device")
                    
                    // Log all supported color spaces for this lens
                    let supportedColorSpaces = newDevice.formats.flatMap { $0.supportedColorSpaces }
                    let uniqueColorSpaces = Array(Set(supportedColorSpaces.map { $0.rawValue }))
                    logger.info("üì∏ Ultra-wide lens supported color spaces: \(uniqueColorSpaces)")
                }
                
                try newDevice.lockForConfiguration()
                newDevice.activeColorSpace = .sRGB
                newDevice.unlockForConfiguration()
            } else if let appleLogFormat = formats.first {
                try newDevice.lockForConfiguration()
                newDevice.activeFormat = appleLogFormat
                newDevice.activeColorSpace = .appleLog
                newDevice.unlockForConfiguration()
                logger.info("‚úÖ Maintained Apple Log settings for \(lens.rawValue)√ó lens")
            } else {
                logger.warning("‚ö†Ô∏è New lens does not support Apple Log, reverting to sRGB")
            }
        }
        
        session.addInput(newInput)
        videoDeviceInput = newInput
        device = newDevice
        
        // Configure the new device
        try newDevice.lockForConfiguration()
        if newDevice.isExposureModeSupported(.continuousAutoExposure) {
            newDevice.exposureMode = .continuousAutoExposure
        }
        if newDevice.isFocusModeSupported(.continuousAutoFocus) {
            newDevice.focusMode = .continuousAutoFocus
        }
        newDevice.unlockForConfiguration()
        
        logger.info("‚úÖ Successfully configured session for \(newDevice.localizedName)")
    }
    
    func switchToLens(_ lens: CameraLens) {
        cameraQueue.async { [weak self] in
            guard let self = self else { return }
            
            // For 2x zoom, use digital zoom on the wide angle camera
            if lens == .x2 {
                if let currentDevice = self.device,
                   currentDevice.deviceType == .builtInWideAngleCamera {
                    self.setDigitalZoom(to: lens.zoomFactor, on: currentDevice)
                } else {
                    self.switchToPhysicalLens(.wide, thenSetZoomTo: lens.zoomFactor)
                }
                return
            }
            
            // For all other lenses, try to switch physical device
            self.switchToPhysicalLens(lens, thenSetZoomTo: 1.0)
        }
    }
    
    private func switchToPhysicalLens(_ lens: CameraLens, thenSetZoomTo zoomFactor: CGFloat) {
        logger.info("üîÑ Attempting to switch to \(lens.rawValue)√ó lens")
        
        // Get discovery session for all possible back cameras
        let discoverySession = AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInWideAngleCamera, .builtInUltraWideCamera, .builtInTelephotoCamera],
            mediaType: .video,
            position: .back
        )
        
        logger.info("üì∏ Available devices: \(discoverySession.devices.map { "\($0.localizedName) (\($0.deviceType))" })")
        
        // Find the device we want
        guard let newDevice = discoverySession.devices.first(where: { $0.deviceType == lens.deviceType }) else {
            logger.error("‚ùå Device not available for \(lens.rawValue)√ó lens")
            DispatchQueue.main.async {
                self.delegate?.didEncounterError(.deviceUnavailable)
            }
            return
        }
        
        // Check if we're already on this device
        if let currentDevice = device, currentDevice == newDevice {
            setDigitalZoom(to: zoomFactor, on: currentDevice)
            return
        }
        
        // Store current orientation settings to preserve during lens switch
        let currentInterfaceOrientation = UIApplication.shared.windows.first?.windowScene?.interfaceOrientation ?? .portrait
        var currentVideoAngle: CGFloat = 90 // Default to portrait
        
        // Get current video orientation from any existing connection
        if let videoConnection = session.outputs.first?.connection(with: .video) {
            currentVideoAngle = videoConnection.videoRotationAngle
        }
        
        // Configure session with new device
        let wasRunning = session.isRunning
        if wasRunning {
            session.stopRunning()
        }
        
        session.beginConfiguration()
        
        do {
            try configureSession(for: newDevice, lens: lens)
            
            // Immediately set orientation for all video connections BEFORE committing configuration
            // This ensures we never display frames with incorrect orientation
            for output in session.outputs {
                if let connection = output.connection(with: .video),
                   connection.isVideoRotationAngleSupported(currentVideoAngle) {
                    connection.videoRotationAngle = currentVideoAngle
                }
            }
            
            session.commitConfiguration()
            
            if wasRunning {
                session.startRunning()
            }
            
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                self.delegate?.didUpdateCurrentLens(lens)
                self.delegate?.didUpdateZoomFactor(zoomFactor)
            }
            
            logger.info("‚úÖ Successfully switched to \(lens.rawValue)√ó lens")
            
        } catch {
            logger.error("‚ùå Failed to switch lens: \(error.localizedDescription)")
            session.commitConfiguration()
            
            // Try to recover by returning to wide angle
            if lens != .wide {
                logger.info("üîÑ Attempting to recover by switching to wide angle")
                switchToLens(.wide)
            } else {
                // If we can't even switch to wide angle, notify delegate of error
                DispatchQueue.main.async { [weak self] in
                    self?.delegate?.didEncounterError(.configurationFailed)
                }
            }
            
            if wasRunning {
                session.startRunning()
            }
        }
    }
    
    private func setDigitalZoom(to factor: CGFloat, on device: AVCaptureDevice) {
        logger.info("üì∏ Setting digital zoom to \(factor)√ó")
        
        do {
            try device.lockForConfiguration()
            
            let zoomFactor = factor.clamped(to: device.minAvailableVideoZoomFactor...device.maxAvailableVideoZoomFactor)
            device.ramp(toVideoZoomFactor: zoomFactor, withRate: 20.0)
            
            device.unlockForConfiguration()
            
            DispatchQueue.main.async { [weak self] in
                self?.delegate?.didUpdateZoomFactor(factor)
            }
            
            logger.info("‚úÖ Set digital zoom to \(factor)√ó")
            
        } catch {
            logger.error("‚ùå Failed to set zoom: \(error.localizedDescription)")
            DispatchQueue.main.async { [weak self] in
                self?.delegate?.didEncounterError(.configurationFailed)
            }
        }
    }
    
    func setZoomFactor(_ factor: CGFloat, currentLens: CameraLens, availableLenses: [CameraLens]) {
        guard let currentDevice = self.device else {
            logger.error("No camera device available")
            return
        }
        
        // Find the appropriate lens based on the zoom factor
        let targetLens = availableLenses
            .sorted { abs($0.zoomFactor - factor) < abs($1.zoomFactor - factor) }
            .first ?? .wide
        
        // If we need to switch lenses
        if targetLens != currentLens && abs(targetLens.zoomFactor - factor) < 0.5 {
            switchToLens(targetLens)
            return
        }
        
        do {
            try currentDevice.lockForConfiguration()
            
            // Calculate zoom factor relative to the current lens
            let baseZoom = currentLens.zoomFactor
            let relativeZoom = factor / baseZoom
            let zoomFactor = min(max(relativeZoom, currentDevice.minAvailableVideoZoomFactor),
                               currentDevice.maxAvailableVideoZoomFactor)
            
            // Apply zoom smoothly
            currentDevice.ramp(toVideoZoomFactor: zoomFactor, withRate: 20.0)
            
            self.delegate?.didUpdateZoomFactor(factor)
            self.lastZoomFactor = zoomFactor
            
            currentDevice.unlockForConfiguration()
        } catch {
            logger.error("Failed to set zoom: \(error.localizedDescription)")
            self.delegate?.didEncounterError(.configurationFailed)
        }
    }
    
    func optimizeVideoSettings() {
        guard let device = self.device else {
            logger.error("No camera device available")
            return
        }
        
        do {
            try device.lockForConfiguration()
            
            if device.activeFormat.isVideoStabilizationModeSupported(.cinematic) {
                if let connection = self.session.outputs.first?.connection(with: .video),
                   connection.isVideoStabilizationSupported {
                    connection.preferredVideoStabilizationMode = .cinematic
                }
            }
            
            if device.isFocusModeSupported(.continuousAutoFocus) {
                device.focusMode = .continuousAutoFocus
            }
            
            device.unlockForConfiguration()
        } catch {
            logger.error("Error optimizing video settings: \(error.localizedDescription)")
        }
    }
    
    // Helper method to update connections after switching lenses
    func updateVideoOrientation(for connection: AVCaptureConnection, orientation: UIInterfaceOrientation) {
        // Log the current state
        logger.info("üì± Orientation Update Request:")
        logger.info("- Interface Orientation: \(orientation.rawValue)")
        logger.info("- Device Orientation: \(UIDevice.current.orientation.rawValue)")
        logger.info("- Current Connection Angle: \(connection.videoRotationAngle)¬∞")
        logger.info("- Recording Lock State: \(self.isRecordingOrientationLocked)")
        
        guard !self.isRecordingOrientationLocked else {
            logger.info("Orientation update skipped: Recording in progress.")
            return
        }
        
        let newAngle: CGFloat
        let deviceOrientation = UIDevice.current.orientation
        
        // First check device orientation for more accurate rotation
        if deviceOrientation.isValidInterfaceOrientation {
            switch deviceOrientation {
            case .portrait:
                newAngle = 90  // Portrait mode: rotate 90¬∞ clockwise
                logger.info("Setting portrait orientation from device (90¬∞)")
            case .landscapeLeft:  // USB port on right
                newAngle = 0   // No rotation needed
                logger.info("Setting landscape left from device - USB right (0¬∞)")
            case .landscapeRight:  // USB port on left
                newAngle = 180 // Rotate 180¬∞
                logger.info("Setting landscape right from device - USB left (180¬∞)")
            case .portraitUpsideDown:
                newAngle = 270
                logger.info("Setting portrait upside down from device (270¬∞)")
            default:
                // Fallback to interface orientation
                switch orientation {
                case .portrait:
                    newAngle = 90
                    logger.info("Setting portrait orientation from interface (90¬∞)")
                case .landscapeLeft:
                    newAngle = 0
                    logger.info("Setting landscape left from interface - USB right (0¬∞)")
                case .landscapeRight:
                    newAngle = 180
                    logger.info("Setting landscape right from interface - USB left (180¬∞)")
                case .portraitUpsideDown:
                    newAngle = 270
                    logger.info("Setting portrait upside down from interface (270¬∞)")
                default:
                    logger.warning("Unknown orientation, defaulting to portrait (90¬∞)")
                    newAngle = 90
                }
            }
        } else {
            // Fallback to interface orientation if device orientation is not valid
            switch orientation {
            case .portrait:
                newAngle = 90
                logger.info("Setting portrait orientation from interface (90¬∞)")
            case .landscapeLeft:
                newAngle = 0
                logger.info("Setting landscape left from interface - USB right (0¬∞)")
            case .landscapeRight:
                newAngle = 180
                logger.info("Setting landscape right from interface - USB left (180¬∞)")
            case .portraitUpsideDown:
                newAngle = 270
                logger.info("Setting portrait upside down from interface (270¬∞)")
            default:
                logger.warning("Unknown orientation, defaulting to portrait (90¬∞)")
                newAngle = 90
            }
        }
        
        // Check if the new angle is supported
        guard connection.isVideoRotationAngleSupported(newAngle) else {
            logger.warning("Rotation angle \(newAngle)¬∞ not supported for connection.")
            return
        }
        
        // Only update if the angle is actually different
        if connection.videoRotationAngle != newAngle {
            connection.videoRotationAngle = newAngle
            logger.info("Updated video connection rotation angle to \(newAngle)¬∞")
        }
    }
    
    // Flag to track orientation locking during recording
    private var isRecordingOrientationLocked = false
    
    func lockOrientationForRecording(_ locked: Bool) {
        self.isRecordingOrientationLocked = locked
        logger.info("Orientation updates \(locked ? "locked" : "unlocked") for recording.")
        if locked {
            logger.info("üì± Locking orientation state:")
            logger.info("- Interface Orientation: \(UIApplication.shared.windows.first?.windowScene?.interfaceOrientation.rawValue ?? -1)")
            logger.info("- Device Orientation: \(UIDevice.current.orientation.rawValue)")
            if let connection = session.outputs.first?.connection(with: .video) {
                logger.info("- Current Connection Angle: \(connection.videoRotationAngle)¬∞")
            }
        }
    }
}

================
File: camera/Features/Camera/Services/CameraSetupService.swift
================
import AVFoundation
import Foundation
import os.log

protocol CameraSetupServiceDelegate: AnyObject {
    func didUpdateSessionStatus(_ status: CameraViewModel.Status)
    func didEncounterError(_ error: CameraError)
    func didInitializeCamera(device: AVCaptureDevice)
    func didStartRunning(_ isRunning: Bool)
}

class CameraSetupService {
    private let logger = Logger(subsystem: "com.camera", category: "CameraSetupService")
    private var session: AVCaptureSession
    private weak var delegate: CameraSetupServiceDelegate?
    private var videoDeviceInput: AVCaptureDeviceInput?
    
    init(session: AVCaptureSession, delegate: CameraSetupServiceDelegate) {
        self.session = session
        self.delegate = delegate
    }
    
    func setupSession() throws {
        logger.info("Setting up camera session")
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        session.beginConfiguration()
        
        // Start with wide angle camera
        guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                    for: .video,
                                                    position: .back) else {
            logger.error("No camera device available")
            delegate?.didEncounterError(.cameraUnavailable)
            delegate?.didUpdateSessionStatus(.failed)
            session.commitConfiguration()
            return
        }
        
        logger.info("Found camera device: \(videoDevice.localizedName)")
        
        do {
            let input = try AVCaptureDeviceInput(device: videoDevice)
            videoDeviceInput = input
            
            if session.canAddInput(input) {
                session.addInput(input)
                logger.info("Added video input to session")
            } else {
                logger.error("Failed to add video input to session")
            }
            
            if let audioDevice = AVCaptureDevice.default(for: .audio),
               let audioInput = try? AVCaptureDeviceInput(device: audioDevice),
               session.canAddInput(audioInput) {
                session.addInput(audioInput)
                logger.info("Added audio input to session")
            }
            
            delegate?.didInitializeCamera(device: videoDevice)
            
        } catch {
            logger.error("Error setting up camera: \(error.localizedDescription)")
            delegate?.didEncounterError(.setupFailed)
            session.commitConfiguration()
            return
        }
        
        session.commitConfiguration()
        
        if session.canSetSessionPreset(.hd4K3840x2160) {
            session.sessionPreset = .hd4K3840x2160
            logger.info("Using 4K preset")
        } else if session.canSetSessionPreset(.hd1920x1080) {
            session.sessionPreset = .hd1920x1080
            logger.info("Using 1080p preset")
        }
        
        // Request camera permissions if needed
        checkCameraPermissionsAndStart()
    }
    
    private func checkCameraPermissionsAndStart() {
        let cameraAuthorizationStatus = AVCaptureDevice.authorizationStatus(for: .video)
        
        switch cameraAuthorizationStatus {
        case .authorized:
            logger.info("Camera access already authorized")
            startCameraSession()
            
        case .notDetermined:
            logger.info("Requesting camera authorization...")
            AVCaptureDevice.requestAccess(for: .video) { [weak self] granted in
                guard let self = self else { return }
                
                if granted {
                    self.logger.info("Camera access granted")
                    self.startCameraSession()
                } else {
                    self.logger.error("Camera access denied")
                    DispatchQueue.main.async {
                        self.delegate?.didEncounterError(.unauthorized)
                        self.delegate?.didUpdateSessionStatus(.unauthorized)
                    }
                }
            }
            
        case .denied, .restricted:
            logger.error("Camera access denied or restricted")
            DispatchQueue.main.async {
                self.delegate?.didEncounterError(.unauthorized)
                self.delegate?.didUpdateSessionStatus(.unauthorized)
            }
            
        @unknown default:
            logger.warning("Unknown camera authorization status")
            startCameraSession()
        }
    }
    
    private func startCameraSession() {
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }
            
            self.logger.info("Starting camera session...")
            if !self.session.isRunning {
                self.session.startRunning()
                
                DispatchQueue.main.async {
                    let isRunning = self.session.isRunning
                    self.delegate?.didStartRunning(isRunning)
                    self.delegate?.didUpdateSessionStatus(isRunning ? .running : .failed)
                    self.logger.info("Camera session running: \(isRunning)")
                }
            } else {
                self.logger.warning("Camera session already running")
                
                DispatchQueue.main.async {
                    self.delegate?.didStartRunning(true)
                    self.delegate?.didUpdateSessionStatus(.running)
                }
            }
        }
    }
    
    func stopSession() {
        if session.isRunning {
            session.stopRunning()
            delegate?.didStartRunning(false)
        }
    }
}

================
File: camera/Features/Camera/Services/ExposureService.swift
================
import AVFoundation
import os.log
import CoreMedia

protocol ExposureServiceDelegate: AnyObject {
    func didUpdateWhiteBalance(_ temperature: Float)
    func didUpdateISO(_ iso: Float)
    func didUpdateShutterSpeed(_ speed: CMTime)
    func didEncounterError(_ error: CameraError)
}

class ExposureService {
    private let logger = Logger(subsystem: "com.camera", category: "ExposureService")
    private weak var delegate: ExposureServiceDelegate?
    
    private var device: AVCaptureDevice?
    private var isAutoExposureEnabled = true
    
    init(delegate: ExposureServiceDelegate) {
        self.delegate = delegate
    }
    
    func setDevice(_ device: AVCaptureDevice) {
        self.device = device
    }
    
    func updateWhiteBalance(_ temperature: Float) {
        guard let device = device else { 
            logger.error("No camera device available")
            return 
        }
        
        do {
            try device.lockForConfiguration()
            let tnt = AVCaptureDevice.WhiteBalanceTemperatureAndTintValues(temperature: temperature, tint: 0.0)
            var gains = device.deviceWhiteBalanceGains(for: tnt)
            let maxGain = device.maxWhiteBalanceGain
            
            gains.redGain   = min(max(1.0, gains.redGain), maxGain)
            gains.greenGain = min(max(1.0, gains.greenGain), maxGain)
            gains.blueGain  = min(max(1.0, gains.blueGain), maxGain)
            
            device.setWhiteBalanceModeLocked(with: gains) { _ in }
            device.unlockForConfiguration()
            
            delegate?.didUpdateWhiteBalance(temperature)
        } catch {
            logger.error("White balance error: \(error.localizedDescription)")
            delegate?.didEncounterError(.whiteBalanceError)
        }
    }
    
    func updateISO(_ iso: Float) {
        guard let device = device else { 
            logger.error("No camera device available")
            return 
        }
        
        // Get the current device's supported ISO range
        let minISO = device.activeFormat.minISO
        let maxISO = device.activeFormat.maxISO
        
        logger.debug("ISO update requested to \(iso). Device supports range: \(minISO) to \(maxISO)")
        
        // Ensure the ISO value is within the supported range
        let clampedISO = min(max(minISO, iso), maxISO)
        
        // Log if clamping occurred
        if clampedISO != iso {
            logger.debug("Clamped ISO from \(iso) to \(clampedISO) to stay within device limits")
        }
        
        do {
            try device.lockForConfiguration()
            
            // Set exposure mode to custom
            if device.isExposureModeSupported(.custom) {
                device.exposureMode = .custom
                device.setExposureModeCustom(duration: device.exposureDuration, iso: clampedISO) { _ in }
            }
            
            device.unlockForConfiguration()
            
            // Update the delegate with the actual value used
            delegate?.didUpdateISO(clampedISO)
            logger.debug("Successfully set ISO to \(clampedISO)")
        } catch {
            logger.error("ISO update error: \(error.localizedDescription)")
            delegate?.didEncounterError(.configurationFailed)
        }
    }
    
    func updateShutterSpeed(_ speed: CMTime) {
        guard let device = device else { 
            logger.error("No camera device available")
            return 
        }
        
        do {
            try device.lockForConfiguration()
            
            // Get the current device's supported ISO range
            let minISO = device.activeFormat.minISO
            let maxISO = device.activeFormat.maxISO
            
            // Get current ISO value
            let currentISO = device.iso
            
            // Ensure the ISO value is within the supported range
            let clampedISO = min(max(minISO, currentISO), maxISO)
            
            // If ISO is 0 or outside valid range, use min ISO as fallback
            let safeISO = clampedISO <= 0 ? minISO : clampedISO
            
            if device.isExposureModeSupported(.custom) {
                device.exposureMode = .custom
                device.setExposureModeCustom(duration: speed, iso: safeISO) { _ in }
            }
            
            device.unlockForConfiguration()
            
            delegate?.didUpdateShutterSpeed(speed)
            
            // If we had to correct ISO, update that too
            if safeISO != currentISO {
                delegate?.didUpdateISO(safeISO)
            }
        } catch {
            logger.error("Shutter speed error: \(error.localizedDescription)")
            delegate?.didEncounterError(.configurationFailed)
        }
    }
    
    func updateShutterAngle(_ angle: Double, frameRate: Double) {
        let clampedAngle = min(max(angle, 1.1), 360.0)
        let duration = (clampedAngle / 360.0) * (1.0 / frameRate)
        let time = CMTimeMakeWithSeconds(duration, preferredTimescale: 1000000)
        updateShutterSpeed(time)
    }
    
    func setAutoExposureEnabled(_ enabled: Bool) {
        isAutoExposureEnabled = enabled
        updateExposureMode()
    }
    
    private func updateExposureMode() {
        guard let device = device else { 
            logger.error("No camera device available")
            return 
        }
        
        do {
            try device.lockForConfiguration()
            
            if isAutoExposureEnabled {
                if device.isExposureModeSupported(.continuousAutoExposure) {
                    device.exposureMode = .continuousAutoExposure
                    logger.info("Auto exposure enabled")
                }
            } else {
                if device.isExposureModeSupported(.custom) {
                    device.exposureMode = .custom
                    
                    // Double check ISO range limits
                    let minISO = device.activeFormat.minISO
                    let maxISO = device.activeFormat.maxISO
                    let currentISO = device.iso
                    let clampedISO = min(max(minISO, currentISO), maxISO)
                    
                    device.setExposureModeCustom(duration: device.exposureDuration,
                                                 iso: clampedISO) { _ in }
                    logger.info("Manual exposure enabled with ISO \(clampedISO)")
                    
                    // If we had to adjust the ISO, update the delegate
                    if clampedISO != currentISO {
                        delegate?.didUpdateISO(clampedISO)
                    }
                }
            }
            
            device.unlockForConfiguration()
        } catch {
            logger.error("Error setting exposure mode: \(error.localizedDescription)")
            delegate?.didEncounterError(.configurationFailed)
        }
    }
    
    func updateTint(_ tintValue: Double, currentWhiteBalance: Float) {
        guard let device = device else { 
            logger.error("No camera device available")
            return 
        }
        
        let tintRange = (-150.0...150.0)
        let clampedTint = min(max(tintValue, tintRange.lowerBound), tintRange.upperBound)
        
        do {
            try device.lockForConfiguration()
            if device.isWhiteBalanceModeSupported(.locked) {
                device.whiteBalanceMode = .locked
                
                let currentGains = device.deviceWhiteBalanceGains
                var newGains = currentGains
                let tintScale = clampedTint / 150.0
                
                if tintScale > 0 {
                    newGains.greenGain = currentGains.greenGain * (1.0 + Float(tintScale))
                } else {
                    let magentaScale = 1.0 + Float(abs(tintScale))
                    newGains.redGain = currentGains.redGain * magentaScale
                    newGains.blueGain = currentGains.blueGain * magentaScale
                }
                
                let maxGain = device.maxWhiteBalanceGain
                newGains.redGain = min(max(1.0, newGains.redGain), maxGain)
                newGains.greenGain = min(max(1.0, newGains.greenGain), maxGain)
                newGains.blueGain = min(max(1.0, newGains.blueGain), maxGain)
                
                device.setWhiteBalanceModeLocked(with: newGains) { _ in }
            }
            device.unlockForConfiguration()
        } catch {
            logger.error("Error setting tint: \(error.localizedDescription)")
            delegate?.didEncounterError(.whiteBalanceError)
        }
    }
}

================
File: camera/Features/Camera/Services/RecordingService.swift
================
import AVFoundation
import Photos
import os.log
import UIKit
import CoreMedia
import CoreImage

protocol RecordingServiceDelegate: AnyObject {
    func didStartRecording()
    func didStopRecording()
    func didFinishSavingVideo(thumbnail: UIImage?)
    func didUpdateProcessingState(_ isProcessing: Bool)
    func didEncounterError(_ error: CameraError)
}

class RecordingService: NSObject {
    private let logger = Logger(subsystem: "com.camera", category: "RecordingService")
    private weak var delegate: RecordingServiceDelegate?
    private var session: AVCaptureSession
    private var device: AVCaptureDevice?
    private var lutManager: LUTManager?
    private var isAppleLogEnabled = false
    private var isBakeInLUTEnabled = true // Default to true to maintain backward compatibility
    
    // Recording properties
    private var assetWriter: AVAssetWriter?
    private var assetWriterInput: AVAssetWriterInput?
    private var assetWriterPixelBufferAdaptor: AVAssetWriterInputPixelBufferAdaptor?
    private var videoDataOutput: AVCaptureVideoDataOutput?
    private var audioDataOutput: AVCaptureAudioDataOutput?
    private var currentRecordingURL: URL?
    private var recordingStartTime: CMTime?
    private var recordingOrientation: CGFloat?
    private var isRecording = false
    private var selectedFrameRate: Double = 30.0
    private var selectedResolution: CameraViewModel.Resolution = .uhd
    private var selectedCodec: CameraViewModel.VideoCodec = .hevc
    private var processingQueue: DispatchQueue
    private var ciContext = CIContext()
    
    // Statistics for debugging
    private var videoFrameCount = 0
    private var audioFrameCount = 0
    private var successfulVideoFrames = 0
    private var failedVideoFrames = 0
    
    init(session: AVCaptureSession, delegate: RecordingServiceDelegate) {
        self.session = session
        self.delegate = delegate
        self.processingQueue = DispatchQueue(
            label: "com.camera.recording",
            qos: .userInitiated,
            attributes: [],
            autoreleaseFrequency: .workItem
        )
        super.init()
    }
    
    func setDevice(_ device: AVCaptureDevice) {
        self.device = device
    }
    
    func setLUTManager(_ lutManager: LUTManager) {
        self.lutManager = lutManager
    }
    
    func setAppleLogEnabled(_ enabled: Bool) {
        self.isAppleLogEnabled = enabled
    }
    
    func setBakeInLUTEnabled(_ enabled: Bool) {
        self.isBakeInLUTEnabled = enabled
        logger.info("Bake in LUT setting changed to: \(enabled)")
    }
    
    func setVideoConfiguration(frameRate: Double, resolution: CameraViewModel.Resolution, codec: CameraViewModel.VideoCodec) {
        self.selectedFrameRate = frameRate
        self.selectedResolution = resolution
        self.selectedCodec = codec
    }
    
    func setupVideoDataOutput() {
        if videoDataOutput == nil {
            videoDataOutput = AVCaptureVideoDataOutput()
            videoDataOutput?.setSampleBufferDelegate(self, queue: processingQueue)
            if session.canAddOutput(videoDataOutput!) {
                session.addOutput(videoDataOutput!)
                logger.info("Added video data output to session")
            } else {
                logger.error("Failed to add video data output to session")
            }
        }
    }
    
    func setupAudioDataOutput() {
        if audioDataOutput == nil {
            audioDataOutput = AVCaptureAudioDataOutput()
            audioDataOutput?.setSampleBufferDelegate(self, queue: processingQueue)
            if session.canAddOutput(audioDataOutput!) {
                session.addOutput(audioDataOutput!)
                logger.info("Added audio data output to session")
            } else {
                logger.error("Failed to add audio data output to session")
            }
        }
    }
    
    func startRecording(orientation: CGFloat) async {
        guard !isRecording else { return }
        
        // Enhanced orientation logging
        let deviceOrientation = UIDevice.current.orientation
        let interfaceOrientation = UIApplication.shared.windows.first?.windowScene?.interfaceOrientation
        
        logger.info("üì± Starting recording with:")
        logger.info("- Device orientation: \(deviceOrientation.rawValue)")
        logger.info("- Interface orientation: \(interfaceOrientation?.rawValue ?? -1)")
        logger.info("- Requested orientation angle: \(orientation)¬∞")
        
        // Determine the correct orientation angle
        let recordingAngle: CGFloat
        if deviceOrientation.isValidInterfaceOrientation {
            // Use device orientation if valid
            recordingAngle = deviceOrientation.videoRotationAngleValue
            logger.info("Using device orientation angle: \(recordingAngle)¬∞")
        } else if let interfaceOrientation = interfaceOrientation {
            // Fallback to interface orientation
            switch interfaceOrientation {
            case .portrait:
                recordingAngle = 90
            case .landscapeLeft:
                recordingAngle = 0
            case .landscapeRight:
                recordingAngle = 180
            case .portraitUpsideDown:
                recordingAngle = 270
            @unknown default:
                recordingAngle = 90
            }
            logger.info("Using interface orientation angle: \(recordingAngle)¬∞")
        } else {
            // Default to portrait if no valid orientation
            recordingAngle = 90
            logger.info("Using default portrait orientation (90¬∞)")
        }
        
        // Store the orientation when starting recording
        recordingOrientation = recordingAngle
        logger.info("Stored recording orientation: \(recordingAngle)¬∞")
        
        // Reset counters when starting a new recording
        videoFrameCount = 0
        audioFrameCount = 0
        successfulVideoFrames = 0
        failedVideoFrames = 0
        
        do {
            // Create temporary URL for recording
            let tempDir = FileManager.default.temporaryDirectory
            let fileName = "recording_\(Date().timeIntervalSince1970).mov"
            let tempURL = tempDir.appendingPathComponent(fileName)
            currentRecordingURL = tempURL
            
            logger.info("Creating asset writer at \(tempURL.path)")
            
            // Create asset writer
            assetWriter = try AVAssetWriter(url: tempURL, fileType: .mov)
            
            // Get dimensions from current format
            guard let device = device else {
                logger.error("Camera device is nil in startRecording")
                throw CameraError.configurationFailed
            }
            
            // Get active format
            let format = device.activeFormat
            
            // Now safely get dimensions
            guard let dimensions = format.dimensions else {
                logger.error("Could not get dimensions from active format: \(format)")
                throw CameraError.configurationFailed
            }
            
            // Set dimensions based on the native format dimensions
            let videoWidth = dimensions.width
            let videoHeight = dimensions.height
            
            // Configure video settings based on current configuration
            var videoSettings: [String: Any] = [
                AVVideoWidthKey: videoWidth,
                AVVideoHeightKey: videoHeight
            ]
            
            if selectedCodec == .proRes {
                videoSettings[AVVideoCodecKey] = AVVideoCodecType.proRes422HQ
            } else {
                videoSettings[AVVideoCodecKey] = AVVideoCodecType.hevc
                
                // Create a single dictionary for all compression properties
                let compressionProperties: [String: Any] = [
                    AVVideoAverageBitRateKey: selectedCodec.bitrate,
                    AVVideoExpectedSourceFrameRateKey: NSNumber(value: selectedFrameRate),
                    AVVideoMaxKeyFrameIntervalKey: Int(selectedFrameRate), // One keyframe per second
                    AVVideoMaxKeyFrameIntervalDurationKey: 1.0, // Force keyframe every second
                    AVVideoAllowFrameReorderingKey: false,
                    AVVideoProfileLevelKey: "HEVC_Main42210_AutoLevel",
                    AVVideoColorPrimariesKey: isAppleLogEnabled ? "ITU_R_2020" : "ITU_R_709_2",
                    AVVideoYCbCrMatrixKey: isAppleLogEnabled ? "ITU_R_2020" : "ITU_R_709_2",
                    "AllowOpenGOP": false,
                    "EncoderID": "com.apple.videotoolbox.videoencoder.hevc.422v2"
                ]
                
                videoSettings[AVVideoCompressionPropertiesKey] = compressionProperties
            }
            
            // Create video input with better buffer handling
            assetWriterInput = AVAssetWriterInput(mediaType: .video, outputSettings: videoSettings)
            assetWriterInput?.expectsMediaDataInRealTime = true
            
            // Apply the correct transform based on current orientation
            assetWriterInput?.transform = getVideoTransform(for: recordingAngle)
            
            logger.info("Created asset writer input with settings: \(videoSettings)")
            
            // Configure audio settings
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatLinearPCM,
                AVSampleRateKey: 48000,
                AVNumberOfChannelsKey: 2,
                AVLinearPCMBitDepthKey: 16,
                AVLinearPCMIsFloatKey: false,
                AVLinearPCMIsBigEndianKey: false,
                AVLinearPCMIsNonInterleaved: false
            ]
            
            // Create audio input
            let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioInput.expectsMediaDataInRealTime = true
            
            // Create pixel buffer adaptor with appropriate format
            let sourcePixelBufferAttributes: [String: Any] = [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA,
                kCVPixelBufferWidthKey as String: dimensions.width,
                kCVPixelBufferHeightKey as String: dimensions.height,
                kCVPixelBufferIOSurfacePropertiesKey as String: [:],
                kCVPixelBufferMetalCompatibilityKey as String: true
            ]
            
            assetWriterPixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(
                assetWriterInput: assetWriterInput!,
                sourcePixelBufferAttributes: sourcePixelBufferAttributes
            )
            
            // Add inputs to writer
            if assetWriter!.canAdd(assetWriterInput!) {
                assetWriter!.add(assetWriterInput!)
                logger.info("Added video input to asset writer")
            } else {
                logger.error("Failed to add video input to asset writer")
            }
            
            if assetWriter!.canAdd(audioInput) {
                assetWriter!.add(audioInput)
                logger.info("Added audio input to asset writer")
            } else {
                logger.error("Failed to add audio input to asset writer")
            }
            
            // Ensure video and audio outputs are configured
            setupVideoDataOutput()
            setupAudioDataOutput()
            
            // Start writing
            recordingStartTime = CMTime(seconds: CACurrentMediaTime(), preferredTimescale: 1000000)
            assetWriter!.startWriting()
            assetWriter!.startSession(atSourceTime: self.recordingStartTime!)
            
            logger.info("Started asset writer session at time: \(self.recordingStartTime!.seconds)")
            
            isRecording = true
            delegate?.didStartRecording()
            
            logger.info("Started recording to: \(tempURL.path)")
            logger.info("Recording settings - Resolution: \(videoWidth)x\(videoHeight), Codec: \(self.selectedCodec == .proRes ? "ProRes 422 HQ" : "HEVC"), Frame Rate: \(self.selectedFrameRate)")
            
        } catch {
            delegate?.didEncounterError(.recordingFailed)
            logger.error("Failed to start recording: \(error.localizedDescription)")
        }
    }
    
    func stopRecording() async {
        guard isRecording else { return }
        
        // Clear stored recording orientation
        recordingOrientation = nil
        logger.info("Cleared recording orientation")
        
        logger.info("Finalizing video with \(self.videoFrameCount) frames (\(self.successfulVideoFrames) successful, \(self.failedVideoFrames) failed)")
        
        await MainActor.run {
            delegate?.didUpdateProcessingState(true)
        }
        
        // Mark all inputs as finished
        assetWriterInput?.markAsFinished()
        logger.info("Marked asset writer inputs as finished")
        
        // Wait for asset writer to finish
        if let assetWriter = assetWriter {
            logger.info("Waiting for asset writer to finish writing...")
            await assetWriter.finishWriting()
            logger.info("Asset writer finished with status: \(assetWriter.status.rawValue)")
            
            if let error = assetWriter.error {
                logger.error("Asset writer error: \(error.localizedDescription)")
            }
        }
        
        // Clean up recording resources
        if let videoDataOutput = videoDataOutput {
            session.removeOutput(videoDataOutput)
            self.videoDataOutput = nil
            logger.info("Removed video data output from session")
        }
        
        if let audioDataOutput = audioDataOutput {
            session.removeOutput(audioDataOutput)
            self.audioDataOutput = nil
            logger.info("Removed audio data output from session")
        }
        
        // Reset recording state
        isRecording = false
        delegate?.didStopRecording()
        recordingStartTime = nil
        
        // Save to photo library if we have a valid recording
        if let outputURL = currentRecordingURL {
            logger.info("Saving video to photo library: \(outputURL.path)")
            
            // Generate thumbnail before saving
            let thumbnail = await generateThumbnail(from: outputURL)
            
            // Save the video
            await saveToPhotoLibrary(outputURL, thumbnail: thumbnail)
        }
        
        // Clean up
        assetWriter = nil
        assetWriterInput = nil
        assetWriterPixelBufferAdaptor = nil
        currentRecordingURL = nil
        
        await MainActor.run {
            delegate?.didUpdateProcessingState(false)
        }
        
        logger.info("Recording session completed")
    }
    
    private func saveToPhotoLibrary(_ outputURL: URL, thumbnail: UIImage?) async {
        do {
            let status = await PHPhotoLibrary.requestAuthorization(for: .readWrite)
            guard status == .authorized else {
                await MainActor.run {
                    delegate?.didEncounterError(.savingFailed)
                    logger.error("Photo library access denied")
                }
                return
            }
            
            try await PHPhotoLibrary.shared().performChanges {
                let options = PHAssetResourceCreationOptions()
                let creationRequest = PHAssetCreationRequest.forAsset()
                creationRequest.addResource(with: .video, fileURL: outputURL, options: options)
            }
            
            await MainActor.run {
                logger.info("Video saved to photo library")
                delegate?.didFinishSavingVideo(thumbnail: thumbnail)
            }
        } catch {
            await MainActor.run {
                logger.error("Error saving video: \(error.localizedDescription)")
                delegate?.didEncounterError(.savingFailed)
            }
        }
    }
    
    private func generateThumbnail(from videoURL: URL) async -> UIImage? {
        let asset = AVURLAsset(url: videoURL)
        let imageGenerator = AVAssetImageGenerator(asset: asset)
        imageGenerator.appliesPreferredTrackTransform = true
        
        // Get thumbnail from first frame
        let time = CMTime(seconds: 0, preferredTimescale: 600)
        
        do {
            let cgImage = try await imageGenerator.image(at: time).image
            return UIImage(cgImage: cgImage)
        } catch {
            logger.error("Error generating thumbnail: \(error.localizedDescription)")
            return nil
        }
    }
    
    private func getVideoTransform(for rotationAngle: CGFloat) -> CGAffineTransform {
        // Convert rotation angle to radians
        let rotationInRadians = rotationAngle * .pi / 180.0
        
        // Handle specific orientations
        let transform: CGAffineTransform
        switch rotationAngle {
        case 90.0:  // Portrait mode
            transform = CGAffineTransform(rotationAngle: .pi / 2)  // 90¬∞ clockwise
            logger.info("üì± Video Transform: Portrait mode (90¬∞) -> œÄ/2 radians")
        case 180.0:  // Landscape with USB on left
            transform = CGAffineTransform(rotationAngle: .pi)  // 180¬∞
            logger.info("üì± Video Transform: Landscape USB left (180¬∞) -> œÄ radians")
        case 0.0:  // Landscape with USB on right
            transform = .identity  // No rotation
            logger.info("üì± Video Transform: Landscape USB right (0¬∞) -> identity")
        case 270.0:  // Portrait upside down
            transform = CGAffineTransform(rotationAngle: -.pi / 2)  // 270¬∞
            logger.info("üì± Video Transform: Portrait upside down (270¬∞) -> -œÄ/2 radians")
        default:
            // For any other angles, use the standard calculation
            transform = CGAffineTransform(rotationAngle: rotationInRadians)
            logger.info("üì± Video Transform: Custom angle (\(rotationAngle)¬∞) -> \(rotationInRadians) radians")
        }
        
        // Log the actual transform values for debugging
        logger.info("üîÑ Transform Details - a: \(transform.a), b: \(transform.b), c: \(transform.c), d: \(transform.d)")
        return transform
    }
    
    // Process image with LUT filter if available
    func applyLUT(to image: CIImage) -> CIImage? {
        guard let lutFilter = lutManager?.currentLUTFilter else {
            return image
        }
        
        lutFilter.setValue(image, forKey: kCIInputImageKey)
        return lutFilter.outputImage
    }
    
    // Helper method for creating pixel buffers from CIImage
    private func createPixelBuffer(from ciImage: CIImage, with template: CVPixelBuffer) -> CVPixelBuffer? {
        var newPixelBuffer: CVPixelBuffer?
        CVPixelBufferCreate(kCFAllocatorDefault,
                           CVPixelBufferGetWidth(template),
                           CVPixelBufferGetHeight(template),
                           CVPixelBufferGetPixelFormatType(template),
                           [kCVPixelBufferIOSurfacePropertiesKey as String: [:]] as CFDictionary,
                           &newPixelBuffer)
        
        guard let outputBuffer = newPixelBuffer else {
            logger.warning("Failed to create pixel buffer from CI image")
            return nil
        }
        
        ciContext.render(ciImage, to: outputBuffer)
        return outputBuffer
    }
    
    // Helper method for creating sample buffers
    private func createSampleBuffer(
        from pixelBuffer: CVPixelBuffer,
        formatDescription: CMFormatDescription,
        timing: UnsafeMutablePointer<CMSampleTimingInfo>
    ) -> CMSampleBuffer? {
        var sampleBuffer: CMSampleBuffer?
        let status = CMSampleBufferCreateForImageBuffer(
            allocator: kCFAllocatorDefault,
            imageBuffer: pixelBuffer,
            dataReady: true,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: formatDescription,
            sampleTiming: timing,
            sampleBufferOut: &sampleBuffer
        )
        
        if status != noErr {
            logger.warning("Failed to create sample buffer: \(status)")
            return nil
        }
        
        return sampleBuffer
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate & AVCaptureAudioDataOutputSampleBufferDelegate

extension RecordingService: AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard isRecording,
              let assetWriter = assetWriter,
              assetWriter.status == .writing else {
            return
        }
        
        // Handle video data
        if output == videoDataOutput,
           let assetWriterInput = assetWriterInput,
           assetWriterInput.isReadyForMoreMediaData {
            
            videoFrameCount += 1
            
            // Log every 30 frames to avoid flooding
            let shouldLog = videoFrameCount % 30 == 0
            if shouldLog {
                logger.debug("Processing video frame #\(self.videoFrameCount), writer status: \(assetWriter.status.rawValue)")
            }
            
            if let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) {
                if isBakeInLUTEnabled, let lutManager = lutManager, lutManager.currentLUTFilter != nil {
                    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
                    if let processedImage = applyLUT(to: ciImage),
                       let processedPixelBuffer = createPixelBuffer(from: processedImage, with: pixelBuffer) {
                        
                        // Use original timing information
                        var timing = CMSampleTimingInfo()
                        CMSampleBufferGetSampleTimingInfo(sampleBuffer, at: 0, timingInfoOut: &timing)
                        
                        // Create format description for processed buffer
                        var info: CMFormatDescription?
                        let status = CMVideoFormatDescriptionCreateForImageBuffer(
                            allocator: kCFAllocatorDefault,
                            imageBuffer: processedPixelBuffer,
                            formatDescriptionOut: &info
                        )
                        
                        if status == noErr, let info = info,
                           let newSampleBuffer = createSampleBuffer(
                            from: processedPixelBuffer,
                            formatDescription: info,
                            timing: &timing
                           ) {
                            assetWriterInput.append(newSampleBuffer)
                            successfulVideoFrames += 1
                            if shouldLog {
                                logger.debug("Successfully appended processed frame #\(self.successfulVideoFrames)")
                            }
                        } else {
                            failedVideoFrames += 1
                            logger.warning("Failed to create format description for processed frame #\(self.videoFrameCount)")
                        }
                    }
                } else {
                    // No LUT processing needed - use original sample buffer directly
                    assetWriterInput.append(sampleBuffer)
                    successfulVideoFrames += 1
                    if shouldLog {
                        logger.debug("Successfully appended original frame #\(self.successfulVideoFrames)")
                    }
                }
            }
        }
        
        // Handle audio data
        if output == audioDataOutput,
           let audioInput = assetWriter.inputs.first(where: { $0.mediaType == .audio }),
           audioInput.isReadyForMoreMediaData {
            audioFrameCount += 1
            audioInput.append(sampleBuffer)
            if audioFrameCount % 100 == 0 {
                logger.debug("Processed audio frame #\(self.audioFrameCount)")
            }
        }
    }
}

================
File: camera/Features/Camera/Services/VideoFormatService.swift
================
import AVFoundation
import os.log
import CoreMedia

protocol VideoFormatServiceDelegate: AnyObject {
    func didEncounterError(_ error: CameraError)
    func didUpdateFrameRate(_ frameRate: Double)
}

class VideoFormatService {
    private let logger = Logger(subsystem: "com.camera", category: "VideoFormatService")
    private weak var delegate: VideoFormatServiceDelegate?
    private var session: AVCaptureSession
    private var device: AVCaptureDevice?
    
    private var isAppleLogEnabled = false
    
    init(session: AVCaptureSession, delegate: VideoFormatServiceDelegate) {
        self.session = session
        self.delegate = delegate
    }
    
    func setDevice(_ device: AVCaptureDevice) {
        self.device = device
    }
    
    func setAppleLogEnabled(_ enabled: Bool) {
        self.isAppleLogEnabled = enabled
    }
    
    func updateCameraFormat(for resolution: CameraViewModel.Resolution) async throws {
        guard let device = device else { 
            logger.error("No camera device available")
            throw CameraError.configurationFailed 
        }
        
        logger.info("Updating camera format to \(resolution.rawValue)")
        
        let wasRunning = session.isRunning
        if wasRunning {
            session.stopRunning()
        }
        
        do {
            try device.lockForConfiguration()
            defer { device.unlockForConfiguration() }
            
            // Find all formats that match our resolution
            let matchingFormats = device.formats.filter { format in
                let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
                return dimensions.width == resolution.dimensions.width &&
                       dimensions.height == resolution.dimensions.height
            }
            
            logger.info("Found \(matchingFormats.count) matching formats")
            
            // Find the best format that supports current frame rate
            let frameRate = device.activeVideoMinFrameDuration.timescale > 0 ?
                Double(device.activeVideoMinFrameDuration.timescale) / Double(device.activeVideoMinFrameDuration.value) :
                30.0
            
            let bestFormat = matchingFormats.first { format in
                format.videoSupportedFrameRateRanges.contains { range in
                    range.minFrameRate...range.maxFrameRate ~= frameRate
                }
            } ?? matchingFormats.first
            
            guard let selectedFormat = bestFormat else {
                logger.error("No compatible format found for resolution \(resolution.rawValue)")
                if wasRunning {
                    session.startRunning()
                }
                throw CameraError.configurationFailed
            }
            
            // Begin configuration
            session.beginConfiguration()
            
            // Set the format
            device.activeFormat = selectedFormat
            
            // Set the frame duration
            let duration = CMTime(value: 1, timescale: CMTimeScale(frameRate))
            device.activeVideoMinFrameDuration = duration
            device.activeVideoMaxFrameDuration = duration
            
            // If Apple Log is enabled, set the color space
            if isAppleLogEnabled && selectedFormat.supportedColorSpaces.contains(.appleLog) {
                device.activeColorSpace = .appleLog
                logger.info("Applied Apple Log color space")
            } else {
                device.activeColorSpace = .sRGB
                logger.info("Applied sRGB color space")
            }
            
            // Commit the configuration
            session.commitConfiguration()
            
            // Restore session state if it was running
            if wasRunning {
                session.startRunning()
            }
            
            logger.info("Camera format updated successfully")
            
        } catch {
            logger.error("Error updating camera format: \(error.localizedDescription)")
            session.commitConfiguration()
            
            if wasRunning {
                session.startRunning()
            }
            
            throw error
        }
    }
    
    func updateFrameRate(_ fps: Double) throws {
        guard let device = device else { 
            logger.error("No camera device available")
            throw CameraError.configurationFailed 
        }
        
        do {
            guard let compatibleFormat = findCompatibleFormat(for: fps) else {
                logger.error("No compatible format found for \(fps) fps")
                throw CameraError.configurationFailed(message: "This device doesn't support \(fps) fps recording")
            }
            
            try device.lockForConfiguration()
            
            if device.activeFormat != compatibleFormat {
                logger.info("Switching to compatible format for \(fps) fps")
                device.activeFormat = compatibleFormat
            }
            
            let frameDuration: CMTime
            switch fps {
            case 23.976:
                frameDuration = CMTime(value: 1001, timescale: 24000)
            case 29.97:
                frameDuration = CMTime(value: 1001, timescale: 30000)
            case 24:
                frameDuration = CMTime(value: 1, timescale: 24)
            case 25:
                frameDuration = CMTime(value: 1, timescale: 25)
            case 30:
                frameDuration = CMTime(value: 1, timescale: 30)
            default:
                frameDuration = CMTimeMake(value: 1, timescale: Int32(fps))
            }
            
            device.activeVideoMinFrameDuration = frameDuration
            device.activeVideoMaxFrameDuration = frameDuration
            
            device.unlockForConfiguration()
            
            delegate?.didUpdateFrameRate(fps)
            logger.info("Frame rate updated to \(fps) fps")
            
        } catch {
            logger.error("Frame rate error: \(error.localizedDescription)")
            delegate?.didEncounterError(.configurationFailed(message: "Failed to set \(fps) fps: \(error.localizedDescription)"))
            throw error
        }
    }
    
    private func findCompatibleFormat(for fps: Double) -> AVCaptureDevice.Format? {
        guard let device = device else { return nil }
        
        let targetFps = fps
        let tolerance = 0.01
        
        let formats = device.formats.filter { format in
            let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
            let isHighRes = dimensions.width >= 1920
            let supportsFrameRate = format.videoSupportedFrameRateRanges.contains { range in
                if abs(targetFps - 23.976) < 0.001 {
                    return range.minFrameRate <= (targetFps - tolerance) &&
                           (targetFps + tolerance) <= range.maxFrameRate
                } else {
                    return range.minFrameRate <= targetFps && targetFps <= range.maxFrameRate
                }
            }
            
            if isAppleLogEnabled {
                return isHighRes && supportsFrameRate && format.supportedColorSpaces.contains(.appleLog)
            }
            return isHighRes && supportsFrameRate
        }
        
        return formats.first
    }
    
    func configureAppleLog() async throws {
        logger.info("Configuring Apple Log")
        
        guard let device = device else {
            logger.error("No camera device available")
            throw CameraError.configurationFailed
        }
        
        // Check if the current device is ultra-wide, which often doesn't support Apple Log
        if device.deviceType == .builtInUltraWideCamera {
            logger.warning("‚ö†Ô∏è Attempting to enable Apple Log on ultra-wide (0.5√ó) lens which may not be supported")
        }
        
        do {
            session.stopRunning()
            
            try await Task.sleep(for: .milliseconds(100))
            session.beginConfiguration()
            
            do {
                try device.lockForConfiguration()
            } catch {
                throw CameraError.configurationFailed
            }
            
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                session.startRunning()
            }
            
            // Find a format that supports Apple Log
            let formats = device.formats.filter { format in
                let desc = format.formatDescription
                let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
                let hasAppleLog = format.supportedColorSpaces.contains(.appleLog)
                let resolution = CameraViewModel.Resolution.uhd.dimensions
                let matchesResolution = dimensions.width >= resolution.width &&
                                      dimensions.height >= resolution.height
                return hasAppleLog && matchesResolution
            }
            
            guard !formats.isEmpty else {
                logger.error("No suitable Apple Log format found for \(device.localizedName) (\(device.deviceType))")
                
                // Additional logging for debugging ultrawide lens issues
                if device.deviceType == .builtInUltraWideCamera {
                    logger.error("‚ùå Ultra-wide (0.5√ó) lens does not support Apple Log on this device model")
                    
                    // Throw a specific error for the ultrawide lens
                    throw CameraError.configurationFailed(message: "Apple Log is not supported on the 0.5√ó ultra-wide lens")
                }
                
                throw CameraError.configurationFailed
            }
            
            guard let selectedFormat = formats.first else {
                logger.error("No suitable Apple Log format found")
                throw CameraError.configurationFailed
            }
            
            logger.info("Found suitable Apple Log format")
            
            // Set the format first
            device.activeFormat = selectedFormat
            
            // Verify the format supports Apple Log
            guard selectedFormat.supportedColorSpaces.contains(.appleLog) else {
                logger.error("Selected format does not support Apple Log")
                throw CameraError.configurationFailed
            }
            
            // Get current frame rate
            let frameRate = device.activeVideoMinFrameDuration.timescale > 0 ?
                Double(device.activeVideoMinFrameDuration.timescale) / Double(device.activeVideoMinFrameDuration.value) :
                30.0
            
            // Set frame duration
            let duration = CMTimeMake(value: 1000, timescale: Int32(frameRate * 1000))
            device.activeVideoMinFrameDuration = duration
            device.activeVideoMaxFrameDuration = duration
            
            // Configure HDR if supported
            if selectedFormat.isVideoHDRSupported {
                device.automaticallyAdjustsVideoHDREnabled = false
                device.isVideoHDREnabled = true
                logger.info("Enabled HDR support")
            }
            
            // Set color space
            device.activeColorSpace = .appleLog
            logger.info("Set color space to Apple Log")
            
            logger.info("Successfully configured Apple Log format")
            
        } catch {
            logger.error("Error configuring Apple Log: \(error.localizedDescription)")
            throw error
        }
    }
    
    func resetAppleLog() async throws {
        logger.info("Resetting Apple Log")
        
        guard let device = device else {
            logger.error("No camera device available")
            throw CameraError.configurationFailed
        }
        
        do {
            session.stopRunning()
            
            try await Task.sleep(for: .milliseconds(100))
            session.beginConfiguration()
            
            do {
                try device.lockForConfiguration()
            } catch {
                throw CameraError.configurationFailed
            }
            
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                session.startRunning()
            }
            
            // Find a format that matches our resolution
            let dims = CameraViewModel.Resolution.uhd.dimensions
            let formats = device.formats.filter { format in
                let desc = format.formatDescription
                let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
                return dimensions.width >= dims.width && dimensions.height >= dims.height
            }
            
            guard let selectedFormat = formats.first else {
                logger.error("No suitable format found")
                throw CameraError.configurationFailed
            }
            
            // Set the format
            device.activeFormat = selectedFormat
            
            // Get current frame rate
            let frameRate = device.activeVideoMinFrameDuration.timescale > 0 ?
                Double(device.activeVideoMinFrameDuration.timescale) / Double(device.activeVideoMinFrameDuration.value) :
                30.0
            
            // Set frame duration
            let duration = CMTimeMake(value: 1000, timescale: Int32(frameRate * 1000))
            device.activeVideoMinFrameDuration = duration
            device.activeVideoMaxFrameDuration = duration
            
            // Reset HDR settings
            if selectedFormat.isVideoHDRSupported {
                device.automaticallyAdjustsVideoHDREnabled = true
                logger.info("Reset HDR settings")
            }
            
            // Reset color space
            device.activeColorSpace = .sRGB
            logger.info("Reset color space to sRGB")
            
            logger.info("Successfully reset Apple Log format")
            
        } catch {
            logger.error("Error resetting Apple Log: \(error.localizedDescription)")
            throw error
        }
    }
}

================
File: camera/Features/Camera/Services/VideoOutputDelegate.swift
================
import AVFoundation
import CoreImage

class VideoOutputDelegate: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    
    let lutManager: LUTManager
    let viewModel: CameraViewModel
    let context: CIContext
    
    // Counter to limit debug logging
    private static var frameCounter = 0
    
    init(lutManager: LUTManager, viewModel: CameraViewModel, context: CIContext) {
        self.lutManager = lutManager
        self.viewModel = viewModel
        self.context = context
        super.init()
        
        print("VideoOutputDelegate initialized")
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        VideoOutputDelegate.frameCounter += 1
        
        // Only print debug logging every 60 frames to avoid console spam
        let isLoggingFrame = VideoOutputDelegate.frameCounter % 60 == 0
        // After 120 frames, only log important events
        let isEstablishedStream = VideoOutputDelegate.frameCounter > 120
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            if isLoggingFrame && !isEstablishedStream {
                print("Could not get pixel buffer from sample buffer")
            }
            return
        }
        
        // Create CIImage from the pixel buffer
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // If Apple Log is enabled, handle LOG processing
        if viewModel.isAppleLogEnabled {
            if isLoggingFrame && !isEstablishedStream {
                print("Processing LOG image")
            }
        }
        
        // Apply LUT if available
        if lutManager.currentLUTFilter != nil {
            if isLoggingFrame && !isEstablishedStream {
                print("Attempting to apply LUT filter")
            }
            
            if let processedImage = lutManager.applyLUT(to: ciImage) {
                ciImage = processedImage
                
                if isLoggingFrame && !isEstablishedStream {
                    print("‚úÖ LUT applied successfully")
                }
            } else if isLoggingFrame {
                print("‚ùå LUT application failed")
            }
        }
        
        // Render the processed image back to the pixel buffer
        context.render(ciImage, to: pixelBuffer)
        
        // Process the frame in the viewModel - just check if processing succeeded
        if viewModel.processVideoFrame(sampleBuffer) != nil {
            if isLoggingFrame && !isEstablishedStream {
                print("Frame processed successfully")
            }
        }
        
        // Shutter angle debug removed to reduce console spam
    }
}

================
File: camera/Features/Camera/Services/VolumeButtonHandler.swift
================
import SwiftUI
import AVKit

@available(iOS 17.2, *)
@MainActor
class VolumeButtonHandler {
    private weak var viewModel: CameraViewModel?
    private var eventInteraction: AVCaptureEventInteraction?
    private var isProcessingEvent = false
    private let debounceInterval: TimeInterval = 1.0
    
    init(viewModel: CameraViewModel) {
        self.viewModel = viewModel
        setupVolumeButtonInteraction()
    }
    
    private func setupVolumeButtonInteraction() {
        // Primary handler (volume down button)
        let primaryHandler = { [weak self] (event: AVCaptureEvent) in
            guard let self = self,
                  let viewModel = self.viewModel,
                  !self.isProcessingEvent else { return }
            
            switch event.phase {
            case .began:
                Task { @MainActor in
                    guard !viewModel.isProcessingRecording else { return }
                    self.isProcessingEvent = true
                    
                    if !viewModel.isRecording {
                        await viewModel.startRecording()
                    } else {
                        await viewModel.stopRecording()
                    }
                    
                    // Add delay before allowing next event
                    try? await Task.sleep(for: .seconds(self.debounceInterval))
                    self.isProcessingEvent = false
                }
            default:
                break
            }
        }
        
        // Secondary handler (volume up button)
        let secondaryHandler = { [weak self] (event: AVCaptureEvent) in
            guard let self = self,
                  let viewModel = self.viewModel,
                  !self.isProcessingEvent else { return }
            
            switch event.phase {
            case .began:
                Task { @MainActor in
                    guard !viewModel.isProcessingRecording else { return }
                    self.isProcessingEvent = true
                    
                    if !viewModel.isRecording {
                        await viewModel.startRecording()
                    } else {
                        await viewModel.stopRecording()
                    }
                    
                    // Add delay before allowing next event
                    try? await Task.sleep(for: .seconds(self.debounceInterval))
                    self.isProcessingEvent = false
                }
            default:
                break
            }
        }
        
        // Create and configure the interaction
        eventInteraction = AVCaptureEventInteraction(primary: primaryHandler, secondary: secondaryHandler)
        eventInteraction?.isEnabled = true
    }
    
    func attachToView(_ view: UIView) {
        guard let eventInteraction = eventInteraction else { return }
        view.addInteraction(eventInteraction)
        print("‚úÖ Volume button interaction attached to view")
    }
    
    func detachFromView(_ view: UIView) {
        guard let eventInteraction = eventInteraction else { return }
        view.removeInteraction(eventInteraction)
        print("üîÑ Volume button interaction detached from view")
    }
}

================
File: camera/Features/Camera/Utilities/DocumentPicker.swift
================
import SwiftUI
import UniformTypeIdentifiers

struct DocumentPicker: UIViewControllerRepresentable {
    let types: [UTType]
    let onPick: (URL) -> Void
    
    func makeUIViewController(context: Context) -> UIDocumentPickerViewController {
        print("üìÑ DocumentPicker: Creating document picker for types: \(types.map { $0.identifier })")
        let picker = UIDocumentPickerViewController(forOpeningContentTypes: types)
        picker.delegate = context.coordinator
        picker.allowsMultipleSelection = false
        picker.shouldShowFileExtensions = true
        print("üìÑ DocumentPicker: Document picker created successfully")
        return picker
    }
    
    func updateUIViewController(_ uiViewController: UIDocumentPickerViewController, context: Context) {}
    
    func makeCoordinator() -> Coordinator {
        print("üìÑ DocumentPicker: Creating coordinator")
        return Coordinator(onPick: onPick)
    }
    
    class Coordinator: NSObject, UIDocumentPickerDelegate {
        let onPick: (URL) -> Void
        
        init(onPick: @escaping (URL) -> Void) {
            self.onPick = onPick
            super.init()
            print("üìÑ DocumentPicker: Coordinator initialized")
        }
        
        func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
            guard let url = urls.first else {
                print("‚ùå DocumentPicker: No document was selected")
                return
            }
            
            print("‚úÖ DocumentPicker: Document selected at URL: \(url.path)")
            
            if !url.startAccessingSecurityScopedResource() {
                print("‚ùå DocumentPicker: Failed to access security scoped resource at \(url.path)")
                return
            }
            
            defer {
                url.stopAccessingSecurityScopedResource()
                print("üìÑ DocumentPicker: Stopped accessing security scoped resource")
            }
            
            guard FileManager.default.fileExists(atPath: url.path) else {
                print("‚ùå DocumentPicker: File does not exist at path: \(url.path)")
                return
            }
            
            do {
                let tempURL = FileManager.default.temporaryDirectory.appendingPathComponent(url.lastPathComponent)
                
                if FileManager.default.fileExists(atPath: tempURL.path) {
                    try FileManager.default.removeItem(at: tempURL)
                    print("üìÑ DocumentPicker: Removed existing file at temp location")
                }
                
                try FileManager.default.copyItem(at: url, to: tempURL)
                print("‚úÖ DocumentPicker: Successfully copied file to: \(tempURL.path)")
                
                let fileAttributes = try FileManager.default.attributesOfItem(atPath: tempURL.path)
                if let fileSize = fileAttributes[.size] as? NSNumber {
                    print("üìÑ DocumentPicker: File size: \(fileSize.intValue) bytes")
                }
                
                DispatchQueue.main.async {
                    self.onPick(tempURL)
                }
            } catch {
                print("‚ùå DocumentPicker: Error handling selected file: \(error.localizedDescription)")
            }
        }
        
        func documentPickerWasCancelled(_ controller: UIDocumentPickerViewController) {
            print("üìÑ DocumentPicker: Document selection was cancelled")
        }
    }
}

================
File: camera/Features/Camera/Views/CameraPreviewImplementation.swift
================
// File: camera/Features/Camera/CameraPreviewImplementation.swift

import SwiftUI
import AVFoundation

/// A SwiftUI view that presents a live camera preview using an AVCaptureVideoPreviewLayer.
/// This implementation locks the preview to a fixed portrait orientation
/// so that the preview does not rotate even if the device rotates.
struct CameraPreview: UIViewRepresentable {
    private let source: PreviewSource

    init(source: PreviewSource) {
        self.source = source
    }

    func makeUIView(context: Context) -> PreviewView {
        // Lock the orientation to portrait
        CameraOrientationLock.lockToPortrait()
        let preview = PreviewView()
        source.connect(to: preview)
        return preview
    }

    func updateUIView(_ uiView: PreviewView, context: Context) {
        // No updates required.
    }
    
    static func dismantleUIView(_ uiView: PreviewView, coordinator: ()) {
        // Maintain portrait orientation when view is dismantled
        CameraOrientationLock.lockToPortrait()
    }

    /// A UIView whose backing layer is AVCaptureVideoPreviewLayer.
    /// It sets the session and forces a fixed rotation.
    class PreviewView: UIView, PreviewTarget {
        override class var layerClass: AnyClass {
            AVCaptureVideoPreviewLayer.self
        }
        
        var previewLayer: AVCaptureVideoPreviewLayer {
            layer as! AVCaptureVideoPreviewLayer
        }
        
        init() {
            super.init(frame: .zero)
            backgroundColor = .black
            // Do not register for orientation notifications to keep a fixed preview.
        }
        
        required init?(coder: NSCoder) {
            fatalError("init(coder:) has not been implemented")
        }
        
        func setSession(_ session: AVCaptureSession) {
            previewLayer.session = session
            previewLayer.videoGravity = .resizeAspectFill
            
            if let connection = previewLayer.connection {
                // Force a fixed rotation to portrait (90 degrees)
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                    print("DEBUG: Set videoRotationAngle to 90 (portrait fixed)")
                } else {
                    print("DEBUG: videoRotationAngle 90 not supported")
                }
                print("DEBUG: Current videoRotationAngle: \(connection.videoRotationAngle)")
            } else {
                print("DEBUG: No connection available on previewLayer")
            }
        }
    }
}

/// A protocol to allow connecting an AVCaptureSession to a preview view.
@preconcurrency
protocol PreviewSource: Sendable {
    func connect(to target: PreviewTarget)
}

/// A protocol that defines a preview target which can accept an AVCaptureSession.
protocol PreviewTarget {
    func setSession(_ session: AVCaptureSession)
}

/// The default implementation for connecting a session.
struct DefaultPreviewSource: PreviewSource {
    private let session: AVCaptureSession
    
    init(session: AVCaptureSession) {
        self.session = session
    }
    
    func connect(to target: PreviewTarget) {
        target.setSession(session)
    }
    
    func makeUIView() -> UIView {
        let view = UIView()
        let previewLayer = AVCaptureVideoPreviewLayer()
        
        // Configure preview layer
        previewLayer.session = session
        previewLayer.videoGravity = .resizeAspectFill
        
        // Force portrait orientation
        if let connection = previewLayer.connection, connection.isVideoRotationAngleSupported(90) {
            connection.videoRotationAngle = 90
            print("DEBUG: DefaultPreviewSource set rotation to 90¬∞")
        }
        
        // Add preview layer to view
        previewLayer.frame = view.bounds
        view.layer.addSublayer(previewLayer)
        
        return view
    }
    
    func updateUIView(_ uiView: UIView) {
        guard let previewLayer = uiView.layer.sublayers?.first as? AVCaptureVideoPreviewLayer else {
            return
        }
        
        // Force portrait orientation
        if let connection = previewLayer.connection, connection.isVideoRotationAngleSupported(90) {
            connection.videoRotationAngle = 90
        }
        
        // Update frame to match view
        previewLayer.frame = uiView.bounds
    }
}

================
File: camera/Features/Camera/Views/CameraPreviewView.swift
================
import SwiftUI
import AVFoundation
import CoreImage
import AVKit

struct CameraPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    @ObservedObject var lutManager: LUTManager
    let viewModel: CameraViewModel
    
    func makeUIView(context: Context) -> RotationLockedContainer {
        print("DEBUG: Creating CameraPreviewView")
        
        // Create container with explicit bounds
        let screen = UIScreen.main.bounds
        let container = RotationLockedContainer(contentView: CustomPreviewView(frame: screen, 
                                                                              session: session, 
                                                                              lutManager: lutManager,
                                                                              viewModel: viewModel))
        
        // Store reference to this container in the view model for LUT processing
        viewModel.owningView = container
        
        // Force layout
        container.setNeedsLayout()
        container.layoutIfNeeded()
        
        return container
    }
    
    func updateUIView(_ uiView: RotationLockedContainer, context: Context) {
        print("DEBUG: updateUIView - Container frame: \(uiView.frame)")
        if let preview = uiView.viewWithTag(100) as? CustomPreviewView {
            print("DEBUG: updateUIView - Preview frame: \(preview.frame)")
            // Update LUT if needed - just pass the reference, don't modify
            preview.updateLUT(lutManager.currentLUTFilter)
        }
    }
    
    func makeCoordinator() -> Coordinator {
        Coordinator(parent: self)
    }
    
    class Coordinator: NSObject {
        let parent: CameraPreviewView
        
        init(parent: CameraPreviewView) {
            self.parent = parent
            super.init()
        }
    }
    
    // MARK: - Custom Views
    
    // A container view that actively resists rotation changes
    class RotationLockedContainer: UIView {
        private let contentView: UIView
        private var borderLayer: CALayer?
        private let cornerRadius: CGFloat = 20.0
        private let borderWidth: CGFloat = 4.0
        private var volumeButtonHandler: VolumeButtonHandler?
        
        init(contentView: UIView) {
            self.contentView = contentView
            super.init(frame: .zero)
            setupView()
            setupBorderLayer()
            setupVolumeButtonHandler()
            
            // Observe recording state changes
            NotificationCenter.default.addObserver(self,
                                                 selector: #selector(handleRecordingStateChange),
                                                 name: NSNotification.Name("RecordingStateChanged"),
                                                 object: nil)
        }
        
        required init?(coder: NSCoder) {
            fatalError("init(coder:) has not been implemented")
        }
        
        private func setupView() {
            // Set background to black
            backgroundColor = .black
            
            // Add content view to fill the container but with space only for border
            addSubview(contentView)
            contentView.translatesAutoresizingMaskIntoConstraints = false
            
            // Create constraints with slightly lower priority
            let topConstraint = contentView.topAnchor.constraint(equalTo: topAnchor, constant: borderWidth)
            let bottomConstraint = contentView.bottomAnchor.constraint(equalTo: bottomAnchor, constant: -borderWidth)
            let leadingConstraint = contentView.leadingAnchor.constraint(equalTo: leadingAnchor, constant: borderWidth)
            let trailingConstraint = contentView.trailingAnchor.constraint(equalTo: trailingAnchor, constant: -borderWidth)
            
            // Set priority to just below required
            topConstraint.priority = .required - 1
            bottomConstraint.priority = .required - 1
            leadingConstraint.priority = .required - 1
            trailingConstraint.priority = .required - 1
            
            // Activate the constraints
            NSLayoutConstraint.activate([
                topConstraint,
                bottomConstraint,
                leadingConstraint,
                trailingConstraint
            ])
            
            // Disable safe area insets
            contentView.insetsLayoutMarginsFromSafeArea = false
            
            // Set black background for all parent views
            setBlackBackgroundForParentViews()
        }
        
        private func setupBorderLayer() {
            let border = CALayer()
            border.borderWidth = borderWidth
            border.borderColor = UIColor.clear.cgColor
            border.cornerRadius = cornerRadius  // Use the same corner radius as the preview
            layer.addSublayer(border)
            borderLayer = border
        }
        
        private func setupVolumeButtonHandler() {
            if #available(iOS 17.2, *) {
                if let previewView = contentView as? CustomPreviewView {
                    volumeButtonHandler = VolumeButtonHandler(viewModel: previewView.viewModel)
                    volumeButtonHandler?.attachToView(self)
                    print("‚úÖ Volume button handler initialized and attached")
                }
            } else {
                print("‚ö†Ô∏è Volume button recording requires iOS 17.2 or later")
            }
        }
        
        deinit {
            if #available(iOS 17.2, *) {
                // Store a weak reference to volumeButtonHandler before deinit
                let handler = volumeButtonHandler
                let view = self
                Task { @MainActor in
                    handler?.detachFromView(view)
                }
                // Clear the reference
                volumeButtonHandler = nil
            }
        }
        
        @objc private func handleRecordingStateChange() {
            if let viewModel = (contentView as? CustomPreviewView)?.viewModel {
                if viewModel.isRecording {
                    animateBorderIn()
                } else {
                    animateBorderOut()
                }
            }
        }
        
        private func animateBorderIn() {
            let animation = CABasicAnimation(keyPath: "borderColor")
            animation.fromValue = UIColor.clear.cgColor
            animation.toValue = UIColor.red.cgColor
            animation.duration = 0.3
            animation.fillMode = .forwards
            animation.isRemovedOnCompletion = false
            borderLayer?.add(animation, forKey: "borderColorAnimation")
        }
        
        private func animateBorderOut() {
            let animation = CABasicAnimation(keyPath: "borderColor")
            animation.fromValue = UIColor.red.cgColor
            animation.toValue = UIColor.clear.cgColor
            animation.duration = 0.3
            animation.fillMode = .forwards
            animation.isRemovedOnCompletion = false
            borderLayer?.add(animation, forKey: "borderColorAnimation")
        }
        
        override func layoutSubviews() {
            super.layoutSubviews()
            
            // Update border frame to match view bounds
            CATransaction.begin()
            CATransaction.setDisableActions(true)
            borderLayer?.frame = bounds
            CATransaction.commit()
            
            // Keep background color black during layout changes
            backgroundColor = .black
            
            // Check for changes in safe area insets
            if safeAreaInsets != .zero {
                // Force content to fill entire view with just border width
                // contentView.frame = bounds.inset(by: UIEdgeInsets(top: borderWidth,
                //                                                 left: borderWidth,
                //                                                 bottom: borderWidth,
                //                                                 right: borderWidth))
            }
            
            // Set black background for parent views
            setBlackBackgroundForParentViews()
        }
        
        private func setBlackBackgroundForParentViews() {
            // Recursively set black background color on all parent views
            var currentView: UIView? = self
            while let view = currentView {
                view.backgroundColor = .black
                
                // Also set any CALayer backgrounds to black
                view.layer.backgroundColor = UIColor.black.cgColor
                
                currentView = view.superview
            }
            
            print("DEBUG: RotationLockedContainer set black background for all parent views")
        }
        
        // Make safe area insets zero to prevent any white bars
        override var safeAreaInsets: UIEdgeInsets {
            return .zero
        }
        
        override func safeAreaInsetsDidChange() {
            super.safeAreaInsetsDidChange()
            // Force black background when safe area changes
            setBlackBackgroundForParentViews()
        }
    }
    
    // Custom preview view that handles LUT processing
    class CustomPreviewView: UIView, AVCaptureVideoDataOutputSampleBufferDelegate {
        private let previewLayer: AVCaptureVideoPreviewLayer
        private var dataOutput: AVCaptureVideoDataOutput?
        private let session: AVCaptureSession
        private var lutManager: LUTManager
        var viewModel: CameraViewModel
        private var ciContext = CIContext(options: [.useSoftwareRenderer: false])
        private let processingQueue = DispatchQueue(label: "com.camera.lutprocessing", qos: .userInitiated)
        private var currentLUTFilter: CIFilter?
        private let cornerRadius: CGFloat = 20.0
        
        init(frame: CGRect, session: AVCaptureSession, lutManager: LUTManager, viewModel: CameraViewModel) {
            self.session = session
            self.lutManager = lutManager
            self.viewModel = viewModel
            self.previewLayer = AVCaptureVideoPreviewLayer()
            super.init(frame: frame)
            setupView()
        }
        
        required init?(coder: NSCoder) {
            fatalError("init(coder:) has not been implemented")
        }
        
        private func setupView() {
            // Set up preview layer
            previewLayer.session = session
            previewLayer.videoGravity = .resizeAspectFill
            previewLayer.cornerRadius = cornerRadius
            previewLayer.masksToBounds = true
            layer.addSublayer(previewLayer)
            
            // Tag this view for easy lookup
            tag = 100
            
            // Ensure preview layer fills the entire view
            previewLayer.frame = bounds
            
            // Disable autoresizing mask to use constraints
            translatesAutoresizingMaskIntoConstraints = false
            
            // Set background color
            backgroundColor = .black
            
            // Force portrait orientation for the preview layer
            if let connection = previewLayer.connection {
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                }
            }
            
            // Initial frame update
            updateFrameSize()
        }
        
        override func layoutSubviews() {
            super.layoutSubviews()
            
            // Ensure preview layer and LUT overlay fill the entire view
            CATransaction.begin()
            CATransaction.setDisableActions(true)
            
            // Update preview layer frame
            previewLayer.frame = bounds
            
            // Update LUT overlay if it exists
            if let overlay = previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                overlay.frame = bounds
                overlay.cornerRadius = cornerRadius
            }
            
            CATransaction.commit()
            
            // Force portrait orientation
            if let connection = previewLayer.connection {
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                }
            }
        }
        
        // MARK: - Frame Management
        
        func updateFrameSize() {
            // Use animation to prevent abrupt changes
            CATransaction.begin()
            CATransaction.setDisableActions(true)
            
            // Calculate the frame that covers the entire screen
            let screenBounds = UIScreen.main.bounds
            let frame = CGRect(x: 0, y: 0, width: screenBounds.width, height: screenBounds.height)
            
            // Update view frame
            self.frame = frame
            
            // Update preview layer frame
            previewLayer.frame = bounds
            
            // Ensure corners stay rounded
            previewLayer.cornerRadius = cornerRadius
            
            // Force portrait orientation
            if let connection = previewLayer.connection {
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                }
            }
            
            // Update LUT overlay layer if it exists
            if let overlay = previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                overlay.frame = bounds
                overlay.cornerRadius = cornerRadius
            }
            
            CATransaction.commit()
        }
        
        // Method to ensure LUT overlay has correct orientation
        func updateLUTOverlayOrientation() {
            // Force portrait orientation for preview layer
            if let connection = previewLayer.connection {
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                }
            }
            
            // Ensure data output connection has correct orientation
            if let dataOutput = dataOutput, let connection = dataOutput.connection(with: .video) {
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                }
            }
            
            // Update overlay to match preview layer orientation
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                if let overlay = self.previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                    // No need to modify overlay properties, just ensure underlying connections are correct
                    print("DEBUG: Ensured LUT overlay orientation is correct (90¬∞)")
                }
            }
        }
        
        func updateLUT(_ filter: CIFilter?) {
            // Skip if same filter (reference equality)
            if (filter === currentLUTFilter) {
                return
            }
            
            // If filter state changed (nil vs non-nil), update processing
            if (filter != nil) != (currentLUTFilter != nil) {
                if filter != nil {
                    // New filter added when none existed
                    setupDataOutput()
                    
                    // Ensure proper orientation immediately when adding a filter
                    DispatchQueue.main.async { [weak self] in
                        guard let self = self else { return }
                        
                        if let connection = self.previewLayer.connection,
                           connection.isVideoRotationAngleSupported(90) {
                            // Force orientation update when LUT is applied
                            connection.videoRotationAngle = 90
                        }
                    }
                } else {
                    // Filter removed - first clean up any existing overlay
                    DispatchQueue.main.async { [weak self] in
                        guard let self = self else { return }
                        
                        // Remove any existing LUT overlay layer before removing the data output
                        CATransaction.begin()
                        CATransaction.setDisableActions(true)
                        
                        if let overlay = self.previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                            overlay.removeFromSuperlayer()
                            print("DEBUG: Removed LUT overlay layer during filter update")
                        }
                        
                        CATransaction.commit()
                        
                        // Now remove the data output on the session queue to prevent freezing
                        DispatchQueue.global(qos: .userInitiated).async {
                            self.removeDataOutput()
                        }
                    }
                }
            }
            
            // Update our local reference only - don't modify published properties
            currentLUTFilter = filter
            
            // DON'T update viewModel.lutManager here - this is called during view updates
            // and would trigger the SwiftUI warning
            
            if filter != nil {
                print("DEBUG: CustomPreviewView updated with LUT filter")
            } else {
                print("DEBUG: CustomPreviewView cleared LUT filter")
            }
        }
        
        private func setupDataOutput() {
            // Remove any existing outputs
            if let existingOutput = dataOutput {
                session.removeOutput(existingOutput)
            }
            
            session.beginConfiguration()
            
            // Create new video data output
            let output = AVCaptureVideoDataOutput()
            output.setSampleBufferDelegate(self, queue: processingQueue)
            output.alwaysDiscardsLateVideoFrames = true
            
            // Set video settings for compatibility with CoreImage
            let settings: [String: Any] = [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
            ]
            output.videoSettings = settings
            
            if session.canAddOutput(output) {
                session.addOutput(output)
                dataOutput = output
                
                // Ensure proper orientation - force portrait
                if let connection = output.connection(with: .video) {
                    if connection.isVideoRotationAngleSupported(90) {
                        connection.videoRotationAngle = 90
                    }
                }
                
                print("DEBUG: Added video data output for LUT processing")
            }
            
            session.commitConfiguration()
        }
        
        private func removeDataOutput() {
            if let output = dataOutput {
                session.beginConfiguration()
                session.removeOutput(output)
                session.commitConfiguration()
                dataOutput = nil
                
                // Clear any existing LUT overlay to prevent freezing
                DispatchQueue.main.async { [weak self] in
                    guard let self = self else { return }
                    
                    CATransaction.begin()
                    CATransaction.setDisableActions(true)
                    
                    // Remove the LUT overlay layer if it exists
                    if let overlay = self.previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                        overlay.removeFromSuperlayer()
                        print("DEBUG: Removed LUT overlay layer")
                    }
                    
                    CATransaction.commit()
                }
                
                print("DEBUG: Removed video data output for LUT processing")
            }
        }
        
        // MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
        
        func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
            guard let currentLUTFilter = currentLUTFilter,
                  let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
                return
            }
            
            // Create CIImage from the pixel buffer
            let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            
            // Apply LUT filter
            currentLUTFilter.setValue(ciImage, forKey: kCIInputImageKey)
            
            guard let outputImage = currentLUTFilter.outputImage,
                  let cgImage = ciContext.createCGImage(outputImage, from: outputImage.extent) else {
                return
            }
            
            // Create or update overlay on main thread
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                
                CATransaction.begin()
                CATransaction.setDisableActions(true)
                
                // Get the screen bounds
                let screenBounds = UIScreen.main.bounds
                
                // Update existing overlay or create a new one
                if let overlay = self.previewLayer.sublayers?.first(where: { $0.name == "LUTOverlayLayer" }) {
                    overlay.contents = cgImage
                    overlay.frame = screenBounds
                    overlay.cornerRadius = self.cornerRadius
                } else {
                    let overlayLayer = CALayer()
                    overlayLayer.name = "LUTOverlayLayer"
                    overlayLayer.frame = screenBounds
                    overlayLayer.contentsGravity = .resizeAspectFill
                    overlayLayer.contents = cgImage
                    overlayLayer.cornerRadius = self.cornerRadius
                    overlayLayer.masksToBounds = true
                    self.previewLayer.addSublayer(overlayLayer)
                    
                    // Ensure the orientation is correct when overlay is first added
                    if let connection = self.previewLayer.connection,
                       connection.isVideoRotationAngleSupported(90) {
                        // Force orientation update to the preview layer right after adding overlay
                        connection.videoRotationAngle = 90
                    }
                    
                    print("DEBUG: Added LUT overlay layer with frame: \(screenBounds)")
                }
                
                CATransaction.commit()
            }
        }
        
        // Completely prevent any transform-based animations
        override func action(for layer: CALayer, forKey event: String) -> CAAction? {
            if event == "transform" || event == "position" || event == "bounds" {
                return NSNull()
            }
            return super.action(for: layer, forKey: event)
        }
    }
}

================
File: camera/Features/Camera/Views/CameraView.swift
================
import SwiftUI
import CoreData
import CoreMedia
import UIKit

struct CameraView: View {
    @StateObject private var viewModel = CameraViewModel()
    @StateObject private var lutManager = LUTManager()
    @StateObject private var orientationViewModel = DeviceOrientationViewModel()
    @State private var isShowingSettings = false
    @State private var isShowingDocumentPicker = false
    @State private var showLUTPreview = true
    @State private var isShowingVideoLibrary = false
    @State private var statusBarHidden = true
    @State private var isDebugEnabled = false
    
    // Initialize with proper handling of StateObjects
    init() {
        // We CANNOT access @StateObject properties here as they won't be initialized yet
        // Only setup notifications that don't depend on StateObjects
        setupOrientationNotifications()
    }
    
    private func setupOrientationNotifications() {
        // Register for app state changes to re-enforce orientation when app becomes active
        NotificationCenter.default.addObserver(
            forName: UIApplication.didBecomeActiveNotification,
            object: nil,
            queue: .main
        ) { _ in
            print("DEBUG: App became active - re-enforcing camera orientation")
            // We CANNOT reference viewModel directly here - it causes the error
            // Instead, we'll handle this in onAppear or through a dedicated @State property
        }
    }
    
    var body: some View {
        GeometryReader { geometry in
            ZStack {
                // Background
                Color.black
                    .edgesIgnoringSafeArea(.all)
                
                // Camera preview with LUT
                cameraPreview
                    .edgesIgnoringSafeArea(.all)
                
                // Function buttons overlay
                FunctionButtonsView()
                    .zIndex(100)
                    .allowsHitTesting(true)
                    .ignoresSafeArea()
                
                // Lens selection with zoom slider
                VStack {
                    Spacer()
                        .frame(height: geometry.safeAreaInsets.top + geometry.size.height * 0.75)
                    
                    if !viewModel.availableLenses.isEmpty {
                        ZoomSliderView(viewModel: viewModel, availableLenses: viewModel.availableLenses)
                            .padding(.bottom, 20)
                    }
                    
                    Spacer()
                }
                .zIndex(99)
                
                // Bottom controls container
                VStack {
                    Spacer()
                    ZStack {
                        // Center record button
                        recordButton
                            .frame(width: 75, height: 75)
                        
                        // Position library button on the left and settings button on the right
                        HStack {
                            videoLibraryButton
                                .frame(width: 60, height: 60)
                            Spacer()
                            settingsButton
                                .frame(width: 60, height: 60)
                        }
                        .padding(.horizontal, 67.5) // Half the record button width (75/2) + button width (60)
                    }
                    .padding(.bottom, 30) // Approximately 1cm from USB-C port
                }
                .ignoresSafeArea()
                .zIndex(101)
            }
            .onAppear {
                print("DEBUG: CameraView appeared, size: \(geometry.size), safeArea: \(geometry.safeAreaInsets)")
                startSession()
            }
            .onDisappear {
                stopSession()
            }
            .onReceive(NotificationCenter.default.publisher(for: UIApplication.willResignActiveNotification)) { _ in
                // When app is moved to background
                stopSession()
            }
            .onReceive(NotificationCenter.default.publisher(for: UIApplication.didBecomeActiveNotification)) { _ in
                // When app returns to foreground
                startSession()
            }
            .onReceive(NotificationCenter.default.publisher(for: UIDevice.orientationDidChangeNotification)) { _ in
                // Get the current device orientation
                let deviceOrientation = UIDevice.current.orientation
                
                // Only update when the device orientation is a valid interface orientation
                if deviceOrientation.isValidInterfaceOrientation {
                    print("DEBUG: Device orientation changed to: \(deviceOrientation.rawValue)")
                    // Convert device orientation to interface orientation
                    let interfaceOrientation: UIInterfaceOrientation
                    switch deviceOrientation {
                    case .portrait:
                        interfaceOrientation = .portrait
                    case .portraitUpsideDown:
                        interfaceOrientation = .portraitUpsideDown
                    case .landscapeLeft:
                        interfaceOrientation = .landscapeRight // Note: these are flipped
                    case .landscapeRight:
                        interfaceOrientation = .landscapeLeft  // Note: these are flipped
                    default:
                        interfaceOrientation = .portrait
                    }
                    viewModel.updateOrientation(interfaceOrientation)
                }
            }
            .onChange(of: lutManager.currentLUTFilter) { oldValue, newValue in
                // When LUT changes, update preview indicator
                if newValue != nil {
                    print("DEBUG: LUT filter updated to: \(lutManager.currentLUTName)")
                    // Automatically turn on preview when a new LUT is loaded
                    showLUTPreview = true
                } else {
                    print("DEBUG: LUT filter removed")
                }
            }
            .onChange(of: viewModel.currentLens) { oldValue, newValue in
                // When lens changes, ensure LUT overlay maintains correct orientation
                if showLUTPreview && lutManager.currentLUTFilter != nil {
                    // Access the preview view and update its LUT overlay orientation
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        if let container = viewModel.owningView,
                           let preview = container.viewWithTag(100) as? CameraPreviewView.CustomPreviewView {
                            preview.updateLUTOverlayOrientation()
                            print("DEBUG: Updated LUT overlay orientation after lens change")
                        }
                    }
                }
            }
            .alert(item: $viewModel.error) { error in
                Alert(
                    title: Text("Error"),
                    message: Text(error.description),
                    dismissButton: .default(Text("OK"))
                )
            }
            .sheet(isPresented: $isShowingSettings) {
                SettingsView(
                    lutManager: lutManager,
                    viewModel: viewModel,
                    isDebugEnabled: $isDebugEnabled
                )
            }
            .sheet(isPresented: $isShowingDocumentPicker) {
                DocumentPicker(types: LUTManager.supportedTypes) { url in
                    DispatchQueue.main.async {
                        handleLUTImport(url: url)
                        isShowingDocumentPicker = false
                    }
                }
            }
            .statusBar(hidden: statusBarHidden)
        }
    }
    
    private var cameraPreview: some View {
        GeometryReader { geometry in
            Group {
                if viewModel.isSessionRunning {
                    // Camera is running - show camera preview
                    CameraPreviewView(
                        session: viewModel.session,
                        lutManager: lutManager,
                        viewModel: viewModel
                    )
                    .ignoresSafeArea()
                    .frame(
                        width: geometry.size.width * 0.9,
                        height: geometry.size.height * 0.75 * 0.9
                    )
                    .padding(.top, geometry.safeAreaInsets.top + 60)
                    .clipped()
                    .frame(maxWidth: .infinity)
                    .overlay(alignment: .topLeading) {
                        if isDebugEnabled {
                            debugOverlay
                                .padding(.top, geometry.safeAreaInsets.top + 70)
                                .padding(.leading, 20)
                        }
                    }
                } else {
                    // Show loading or error state
                    VStack {
                        Text("Starting camera...")
                            .font(.headline)
                            .foregroundColor(.white)
                        
                        if viewModel.status == .failed, let error = viewModel.error {
                            Text("Error: \(error.description)")
                                .font(.subheadline)
                                .foregroundColor(.red)
                                .padding()
                        }
                    }
                    .frame(maxWidth: .infinity, maxHeight: .infinity)
                    .background(Color.black)
                }
            }
        }
    }
    
    private var debugOverlay: some View {
        VStack(alignment: .leading, spacing: 2) {
            Text("Resolution: \(viewModel.selectedResolution.rawValue)")
            Text("FPS: \(String(format: "%.2f", viewModel.selectedFrameRate))")
            Text("Codec: \(viewModel.selectedCodec.rawValue)")
            Text("Color: \(viewModel.isAppleLogEnabled ? "Apple Log" : "Rec.709")")
        }
        .font(.system(size: 10, weight: .medium, design: .monospaced))
        .foregroundColor(.white)
        .padding(6)
        .background(Color.black.opacity(0.5))
        .cornerRadius(6)
    }
    
    private var videoLibraryButton: some View {
        RotatingView(orientationViewModel: orientationViewModel) {
            Button(action: {
                print("DEBUG: [LibraryButton] Button tapped")
                print("DEBUG: [LibraryButton] Current orientation: \(orientationViewModel.orientation.rawValue)")
                AppDelegate.isVideoLibraryPresented = true
                isShowingVideoLibrary = true
            }) {
                ZStack {
                    // Thumbnail background
                    RoundedRectangle(cornerRadius: 12)
                        .fill(Color.black.opacity(0.6))
                        .frame(width: 60, height: 60)
                    
                    // Placeholder or actual thumbnail
                    if let thumbnailImage = viewModel.lastRecordedVideoThumbnail {
                        Image(uiImage: thumbnailImage)
                            .resizable()
                            .aspectRatio(contentMode: .fill)
                            .frame(width: 54, height: 54)
                            .clipShape(RoundedRectangle(cornerRadius: 10))
                    } else {
                        Image(systemName: "film")
                            .font(.system(size: 24))
                            .foregroundColor(.white)
                    }
                }
            }
            .buttonStyle(PlainButtonStyle())
        }
        .frame(width: 60, height: 60)
        .onChange(of: orientationViewModel.orientation) { oldValue, newValue in
            print("DEBUG: [LibraryButton] Orientation changed: \(oldValue.rawValue) -> \(newValue.rawValue)")
        }
        .fullScreenCover(isPresented: $isShowingVideoLibrary, onDismiss: {
            print("DEBUG: [ORIENTATION-DEBUG] fullScreenCover onDismiss - setting AppDelegate.isVideoLibraryPresented = false")
            AppDelegate.isVideoLibraryPresented = false
            
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                print("DEBUG: [ORIENTATION-DEBUG] Current orientation before reset: \(UIDevice.current.orientation.rawValue)")
                let orientations: UIInterfaceOrientationMask = [.portrait]
                let geometryPreferences = UIWindowScene.GeometryPreferences.iOS(interfaceOrientations: orientations)
                
                print("DEBUG: Returning to portrait after video library")
                windowScene.requestGeometryUpdate(geometryPreferences) { error in
                    print("DEBUG: Portrait return result: \(error.localizedDescription)")
                    print("DEBUG: [ORIENTATION-DEBUG] Device orientation after portrait reset: \(UIDevice.current.orientation.rawValue)")
                }
                
                for window in windowScene.windows {
                    window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                    print("DEBUG: [ORIENTATION-DEBUG] Updated orientation on root controller")
                }
                
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                    print("DEBUG: [ORIENTATION-DEBUG] Device orientation 0.5s after reset: \(UIDevice.current.orientation.rawValue)")
                }
            }
        }) {
            VideoLibraryView()
        }
    }
    
    private var settingsButton: some View {
        RotatingView(orientationViewModel: orientationViewModel) {
            Button(action: {
                isShowingSettings = true
            }) {
                ZStack {
                    RoundedRectangle(cornerRadius: 12)
                        .fill(Color.black.opacity(0.6))
                        .frame(width: 60, height: 60)
                    
                    Image(systemName: "gearshape.fill")
                        .font(.system(size: 24))
                        .foregroundColor(.white)
                }
            }
            .simultaneousGesture(
                LongPressGesture(minimumDuration: 3.0)
                    .onEnded { _ in
                        withAnimation {
                            isDebugEnabled.toggle()
                        }
                    }
            )
        }
    }
    
    private var recordButton: some View {
        Button(action: {
            withAnimation(.easeInOut(duration: 0.3)) {
                _ = Task { @MainActor in
                    if viewModel.isRecording {
                        await viewModel.stopRecording()
                    } else {
                        await viewModel.startRecording()
                    }
                }
            }
        }) {
            ZStack {
                // White border circle
                Circle()
                    .strokeBorder(Color.white, lineWidth: 4)
                    .frame(width: 75, height: 75)
                
                // Red recording indicator
                Group {
                    if viewModel.isRecording {
                        // Square when recording
                        RoundedRectangle(cornerRadius: 8)
                            .fill(Color.red)
                            .frame(width: 30, height: 30)
                    } else {
                        // Circle when not recording
                        Circle()
                            .fill(Color.red)
                            .frame(width: 54, height: 54)
                    }
                }
                .animation(.easeInOut(duration: 0.3), value: viewModel.isRecording)
            }
            .opacity(viewModel.isProcessingRecording ? 0.5 : 1.0)
        }
        .buttonStyle(ScaleButtonStyle()) // Custom button style for press animation
        .disabled(viewModel.isProcessingRecording)
    }
    
    // Custom button style for scale animation on press
    private struct ScaleButtonStyle: ButtonStyle {
        func makeBody(configuration: Configuration) -> some View {
            configuration.label
                .scaleEffect(configuration.isPressed ? 0.95 : 1.0)
                .animation(.easeInOut(duration: 0.2), value: configuration.isPressed)
        }
    }
    
    private func handleLUTImport(url: URL) {
        // Import LUT file
        let fileSize = (try? url.resourceValues(forKeys: [.fileSizeKey]))?.fileSize ?? 0
        print("LUT file size: \(fileSize) bytes")
        
        lutManager.importLUT(from: url) { success in
            if success {
                print("DEBUG: LUT import successful, enabling preview")
                
                // Enable the LUT in the viewModel for real-time preview
                if let lutFilter = self.lutManager.currentLUTFilter {
                    self.viewModel.lutManager.currentLUTFilter = lutFilter
                    self.showLUTPreview = true
                    print("DEBUG: LUT filter set in viewModel for preview")
                }
            } else {
                print("DEBUG: LUT import failed")
            }
        }
    }
    
    private func startSession() {
        // Start the camera session when the view appears
        if !viewModel.session.isRunning {
            DispatchQueue.global(qos: .userInitiated).async {
                viewModel.session.startRunning()
                DispatchQueue.main.async {
                    viewModel.isSessionRunning = viewModel.session.isRunning
                    viewModel.status = viewModel.session.isRunning ? .running : .failed
                    viewModel.error = viewModel.session.isRunning ? nil : CameraError.sessionFailedToStart
                    print("DEBUG: Camera session running: \(viewModel.isSessionRunning)")
                }
            }
        }
        
        // Double enforce orientation lock on view appearance
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
            viewModel.updateOrientation(windowScene.interfaceOrientation)
        }
        
        // Setup notification for when app becomes active
        NotificationCenter.default.addObserver(
            forName: UIApplication.didBecomeActiveNotification,
            object: nil,
            queue: .main
        ) { _ in
            print("DEBUG: App became active - re-enforcing camera orientation")
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                viewModel.updateOrientation(windowScene.interfaceOrientation)
            }
        }
        
        // Share the lutManager between views
        viewModel.lutManager = lutManager
        
        // Enable LUT preview by default
        showLUTPreview = true
        
        // Enable device orientation notifications
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
    }
    
    private func stopSession() {
        // Remove notification observer when the view disappears
        NotificationCenter.default.removeObserver(self, name: UIApplication.didBecomeActiveNotification, object: nil)
        
        // Stop the camera session when the view disappears
        if viewModel.session.isRunning {
            DispatchQueue.global(qos: .userInitiated).async {
                viewModel.session.stopRunning()
                DispatchQueue.main.async {
                    viewModel.isSessionRunning = false
                }
            }
        }
        
        // Disable device orientation notifications
        UIDevice.current.endGeneratingDeviceOrientationNotifications()
    }
}

// Add preview at the bottom of the file
#Preview("Camera View") {
    CameraView()
        .preferredColorScheme(.dark)
        .environment(\.colorScheme, .dark)
}

================
File: camera/Features/Camera/Views/ContentView.swift
================
import SwiftUI
import AVFoundation

struct ContentView: View {
    @StateObject private var viewModel = CameraViewModel()
    @StateObject private var lutManager = LUTManager()
    @State private var showTestOverlay = false // Toggle for testing
    
    var body: some View {
        ZStack {
            Color.black.edgesIgnoringSafeArea(.all)  // Add black background
            
            CameraPreviewView(session: viewModel.session, 
                            lutManager: lutManager,
                            viewModel: viewModel)
                .edgesIgnoringSafeArea(.all)  // Use newer modifier
                .frame(maxWidth: .infinity, maxHeight: .infinity)
            
            // Main content or test overlay
            if showTestOverlay {
                TestDynamicIslandOverlayView()
                    .edgesIgnoringSafeArea(.all)
            } else {
                VStack {
                    // Controls content remains the same
                }
                .frame(maxWidth: .infinity, maxHeight: .infinity)
            }
            
            // Button to toggle test view (bottom right corner)
            VStack {
                Spacer()
                HStack {
                    Spacer()
                    Button(action: {
                        showTestOverlay.toggle()
                    }) {
                        Image(systemName: "ladybug.fill")
                            .font(.system(size: 24))
                            .padding()
                            .background(Color.black.opacity(0.6))
                            .clipShape(Circle())
                    }
                    .padding()
                }
            }
        }
        .background(Color.black) // Extra safety black background
        .disableSafeArea() // Use our custom modifier to disable safe area insets
        .statusBar(hidden: true)
        .preferredColorScheme(.dark) // Force dark mode
        .onAppear {
            // Force dark mode at window level when view appears
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                windowScene.windows.forEach { window in
                    window.overrideUserInterfaceStyle = .dark
                    
                    // Disable safe area insets for the key window
                    if #available(iOS 13.0, *) {
                        window.rootViewController?.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
                    }
                }
            }
        }
    }
}

================
File: camera/Features/Camera/Views/FunctionButtonsView.swift
================
import SwiftUI
import UIKit

struct FunctionButtonsView: View {
    @State private var topSafeAreaHeight: CGFloat = 44 // Default value
    @StateObject private var orientationViewModel = DeviceOrientationViewModel()
    
    var body: some View {
        GeometryReader { geometry in
            HStack(spacing: 0) { // Set HStack spacing to 0 to have full control
                // Left side buttons
                HStack {
                    RotatingView(orientationViewModel: orientationViewModel, invertRotation: true) {
                        Button("F1") {
                            print("F1 tapped")
                        }
                        .buttonStyle(FunctionButtonStyle())
                    }
                    .frame(width: 40, height: 30)
                    .padding(.trailing, 16) // Space between F1 and F2
                    
                    RotatingView(orientationViewModel: orientationViewModel, invertRotation: true) {
                        Button("F2") {
                            print("F2 tapped")
                        }
                        .buttonStyle(FunctionButtonStyle())
                    }
                    .frame(width: 40, height: 30)
                }
                .padding(.leading, geometry.size.width * 0.15) // Keep F2 in current position
                
                Spacer()
                    .frame(minWidth: geometry.size.width * 0.3) // Ensure minimum space for Dynamic Island
                
                // Right side buttons
                HStack {
                    RotatingView(orientationViewModel: orientationViewModel, invertRotation: true) {
                        Button("F3") {
                            print("F3 tapped")
                        }
                        .buttonStyle(FunctionButtonStyle())
                    }
                    .frame(width: 40, height: 30)
                    .padding(.trailing, 16) // Space between F3 and F4
                    
                    RotatingView(orientationViewModel: orientationViewModel, invertRotation: true) {
                        Button("F4") {
                            print("F4 tapped")
                        }
                        .buttonStyle(FunctionButtonStyle())
                    }
                    .frame(width: 40, height: 30)
                }
                .padding(.trailing, geometry.size.width * 0.15) // Keep F3 in current position
            }
            .frame(width: geometry.size.width, height: 44)
            .background(Color.black.opacity(0.01))
            .position(x: geometry.size.width / 2, y: 28)
        }
        .ignoresSafeArea()
    }
}

// A container that uses UIViewRepresentable to completely bypass safe areas
struct FunctionButtonsContainer<Content: View>: UIViewRepresentable {
    let content: Content
    
    init(@ViewBuilder content: () -> Content) {
        self.content = content()
    }
    
    func makeUIView(context: Context) -> UIView {
        let hostingController = UIHostingController(rootView: content)
        hostingController.view.backgroundColor = .clear
        
        // Disable safe area insets
        hostingController.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
        
        // Set up the container view
        let containerView = UIView()
        containerView.backgroundColor = .clear
        
        // Add hosting view as a child view
        containerView.addSubview(hostingController.view)
        
        // Set up constraints to position at the absolute top
        hostingController.view.translatesAutoresizingMaskIntoConstraints = false
        NSLayoutConstraint.activate([
            hostingController.view.topAnchor.constraint(equalTo: containerView.topAnchor),
            hostingController.view.leadingAnchor.constraint(equalTo: containerView.leadingAnchor),
            hostingController.view.trailingAnchor.constraint(equalTo: containerView.trailingAnchor),
            hostingController.view.bottomAnchor.constraint(equalTo: containerView.bottomAnchor)
        ])
        
        // Store the hosting controller in the context
        context.coordinator.hostingController = hostingController
        
        return containerView
    }
    
    func updateUIView(_ uiView: UIView, context: Context) {
        // Update the hosting controller's rootView if needed
        context.coordinator.hostingController?.rootView = content
        
        // Make sure it still has negative safe area insets
        context.coordinator.hostingController?.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
    }
    
    func makeCoordinator() -> Coordinator {
        Coordinator()
    }
    
    class Coordinator {
        var hostingController: UIHostingController<Content>?
    }
}

struct FunctionButtonStyle: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .font(.system(size: 12, weight: .semibold))
            .padding(.vertical, 3)
            .padding(.horizontal, 8)
            .background(Color.gray.opacity(0.3))
            .cornerRadius(5)
            .foregroundColor(.white)
            .scaleEffect(configuration.isPressed ? 0.95 : 1)
            .opacity(configuration.isPressed ? 0.9 : 1)
    }
}

================
File: camera/Features/Camera/Views/LensSelectionView.swift
================
import SwiftUI

struct LensSelectionView: View {
    @ObservedObject var viewModel: CameraViewModel
    let availableLenses: [CameraLens]
    
    var body: some View {
        HStack(spacing: 25) {
            ForEach(availableLenses, id: \.self) { lens in
                Button(action: {
                    viewModel.switchToLens(lens)
                }) {
                    Text(lens.rawValue + "√ó")
                        .font(.system(size: 16, weight: .medium))
                        .foregroundColor(viewModel.currentLens == lens ? .yellow : .white)
                        .frame(width: 32, height: 32)
                        .background(
                            Circle()
                                .fill(Color.black.opacity(0.35))
                                .overlay(
                                    Circle()
                                        .strokeBorder(
                                            viewModel.currentLens == lens ? Color.yellow : Color.white.opacity(0.5),
                                            lineWidth: 1
                                        )
                                )
                        )
                }
            }
        }
        .padding(.horizontal)
        .background(Color.black.opacity(0.25))
    }
}

================
File: camera/Features/Camera/Views/SettingsView.swift
================
import SwiftUI
import AVFoundation
import UniformTypeIdentifiers

struct SettingsView: View {
    @Environment(\.dismiss) private var dismiss
    @ObservedObject var lutManager: LUTManager
    @ObservedObject var viewModel: CameraViewModel
    @StateObject private var settingsModel = SettingsModel()
    @Binding var isDebugEnabled: Bool
    @State private var isShowingLUTDocumentPicker = false
    
    var body: some View {
        NavigationView {
            List {
                Section {
                    // Resolution
                    Picker("Resolution", selection: $viewModel.selectedResolution) {
                        ForEach(CameraViewModel.Resolution.allCases, id: \.self) { resolution in
                            Text(resolution.rawValue).tag(resolution)
                        }
                    }
                    
                    // Color Space
                    Picker("Color Space", selection: selectedColorSpace) {
                        ForEach(colorSpaceOptions, id: \.self) { colorSpace in
                            Text(colorSpace).tag(colorSpace)
                        }
                    }
                    
                    // Codec
                    Picker("Codec", selection: $viewModel.selectedCodec) {
                        ForEach(CameraViewModel.VideoCodec.allCases, id: \.self) { codec in
                            Text(codec.rawValue).tag(codec)
                        }
                    }
                    
                    // Frame Rate
                    Picker("Frame Rate", selection: $viewModel.selectedFrameRate) {
                        ForEach(viewModel.availableFrameRates, id: \.self) { fps in
                            Text(fps == 29.97 ? "29.97" : String(format: "%.2f", fps))
                                .tag(fps)
                        }
                    }
                } header: {
                    Text("Camera üé•")
                }
                
                // LUT Settings Section
                Section {
                    Button(action: {
                        isShowingLUTDocumentPicker = true
                    }) {
                        HStack {
                            Image(systemName: "plus.circle.fill")
                                .foregroundColor(.blue)
                            Text("Import LUT")
                        }
                    }
                    
                    if lutManager.currentLUTFilter != nil {
                        HStack {
                            Text("Current LUT")
                            Spacer()
                            Text(lutManager.currentLUTName)
                                .foregroundColor(.secondary)
                        }
                        
                        Toggle(isOn: $settingsModel.isBakeInLUTEnabled) {
                            HStack {
                                Text("Bake in LUT")
                                if settingsModel.isBakeInLUTEnabled {
                                    Image(systemName: "checkmark.square.fill")
                                        .foregroundColor(.blue)
                                }
                            }
                        }
                        .tint(.blue)
                        
                        Button(action: {
                            lutManager.clearLUT()
                        }) {
                            HStack {
                                Image(systemName: "xmark.circle.fill")
                                    .foregroundColor(.red)
                                Text("Remove Current LUT")
                            }
                        }
                    }
                    
                    if let recentLUTs = lutManager.recentLUTs, !recentLUTs.isEmpty {
                        ForEach(Array(recentLUTs.keys), id: \.self) { name in
                            if let url = recentLUTs[name] {
                                Button(action: {
                                    lutManager.loadLUT(from: url)
                                }) {
                                    HStack {
                                        Image(systemName: "photo.fill")
                                            .foregroundColor(.blue)
                                        Text(name)
                                    }
                                }
                            }
                        }
                    }
                } header: {
                    Text("Color LUTs üé®")
                } footer: {
                    if lutManager.currentLUTFilter != nil {
                        Text("When 'Bake in LUT' is enabled, the LUT color profile will be permanently applied to your recorded video. When disabled, the preview will still show the LUT effect, but the original camera footage will be recorded.")
                    }
                }
                
                // Flashlight Settings
                FlashlightSettingsView(settingsModel: settingsModel)
                
                Section {
                    Toggle(isOn: $isDebugEnabled) {
                        HStack {
                            Text("Show Debug Info")
                            if isDebugEnabled {
                                Image(systemName: "info.circle.fill")
                                    .foregroundColor(.blue)
                            }
                        }
                    }
                } header: {
                    Text("Display")
                }
                
                Section {
                    Text("Storage settings will go here")
                } header: {
                    Text("Storage")
                }
            }
            .navigationTitle("Settings")
            .navigationBarTitleDisplayMode(.inline)
            .toolbar {
                ToolbarItem(placement: .navigationBarTrailing) {
                    Button("Done") {
                        dismiss()
                    }
                }
            }
            .sheet(isPresented: $isShowingLUTDocumentPicker) {
                DocumentPicker(types: LUTManager.supportedTypes) { url in
                    lutManager.importLUT(from: url) { success in
                        if success {
                            print("LUT imported successfully")
                        }
                    }
                }
            }
        }
    }
    
    // Color space options
    private let colorSpaceOptions = [
        "Rec.709",
        "Apple Log"
    ]
    
    // Binding for color space that updates Apple Log
    private var selectedColorSpace: Binding<String> {
        Binding(
            get: { viewModel.isAppleLogEnabled ? "Apple Log" : "Rec.709" },
            set: { newValue in
                viewModel.isAppleLogEnabled = (newValue == "Apple Log")
            }
        )
    }
}

#Preview {
    SettingsView(
        lutManager: LUTManager(),
        viewModel: CameraViewModel(),
        isDebugEnabled: .constant(false)
    )
}

================
File: camera/Features/Camera/Views/TestDynamicIslandOverlayView.swift
================
import SwiftUI

/// A test view for positioning content in the Dynamic Island area
struct TestDynamicIslandOverlayView: View {
    @State private var topInset: CGFloat = 0
    @State private var viewFrame: CGRect = .zero
    
    var body: some View {
        GeometryReader { geometry in
            let _ = print("DEBUG: GeometryReader size: \(geometry.size)")
            
            ZStack(alignment: .top) {
                // Background to see the view bounds
                Color.black.opacity(0.5)
                    .edgesIgnoringSafeArea(.all)
                
                // Function buttons container
                VStack(spacing: 0) {
                    // Function buttons
                    HStack {
                        // Left side buttons
                        HStack(spacing: 12) {
                            Button("F1") {
                                print("F1 tapped")
                            }
                            .buttonStyle(TestFunctionButtonStyle())
                            
                            Button("F2") {
                                print("F2 tapped")
                            }
                            .buttonStyle(TestFunctionButtonStyle())
                        }
                        
                        Spacer()
                        
                        // Right side buttons
                        HStack(spacing: 12) {
                            Button("F3") {
                                print("F3 tapped")
                            }
                            .buttonStyle(TestFunctionButtonStyle())
                            
                            Button("F4") {
                                print("F4 tapped")
                            }
                            .buttonStyle(TestFunctionButtonStyle())
                        }
                    }
                    .padding(.horizontal, 16)
                    .padding(.top, 2)
                    .background(GeometryReader { buttonGeometry in
                        Color.clear
                            .preference(key: ViewFrameKey.self, value: buttonGeometry.frame(in: .global))
                    })
                    
                    Spacer() // Push content to top
                }
            }
            .frame(width: geometry.size.width, height: geometry.size.height)
        }
        .ignoresSafeArea()
        .onPreferenceChange(ViewFrameKey.self) { frame in
            viewFrame = frame
            print("DEBUG: Button container frame: \(frame)")
        }
        .onAppear {
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene,
               let window = windowScene.windows.first {
                topInset = window.safeAreaInsets.top
                print("DEBUG: TestDynamicIslandOverlayView onAppear - Top safe area inset: \(topInset)")
                print("DEBUG: TestDynamicIslandOverlayView onAppear - Window frame: \(window.frame)")
                print("DEBUG: TestDynamicIslandOverlayView onAppear - Safe area insets: \(window.safeAreaInsets)")
            }
        }
    }
}

/// Custom button style for test function buttons
struct TestFunctionButtonStyle: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .font(.system(size: 12, weight: .semibold))
            .padding(.vertical, 3)
            .padding(.horizontal, 8)
            .background(Color.gray.opacity(0.3))
            .cornerRadius(5)
            .foregroundColor(.white)
            .scaleEffect(configuration.isPressed ? 0.95 : 1)
            .opacity(configuration.isPressed ? 0.9 : 1)
    }
}

// Preference key to track view frame
struct ViewFrameKey: PreferenceKey {
    static var defaultValue: CGRect = .zero
    
    static func reduce(value: inout CGRect, nextValue: () -> CGRect) {
        value = nextValue()
    }
}

/// Preview provider for TestDynamicIslandOverlayView
struct TestDynamicIslandOverlayView_Previews: PreviewProvider {
    static var previews: some View {
        TestDynamicIslandOverlayView()
    }
}

================
File: camera/Features/Camera/Views/ZoomSliderView.swift
================
import SwiftUI
import Foundation

struct ZoomSliderView: View {
    @ObservedObject var viewModel: CameraViewModel
    let availableLenses: [CameraLens]
    @State private var isDragging = false
    @State private var dragVelocity: CGFloat = 0
    @State private var lastDragLocation: CGPoint?
    @State private var lastDragTime: Date?
    
    private let minZoom: CGFloat = 1.0
    private let maxZoom: CGFloat = 10.0
    private let sliderWidth: CGFloat = 200
    
    var body: some View {
        VStack(spacing: 12) {
            // Zoom slider
            GeometryReader { geometry in
                HStack {
                    Spacer()
                    ZStack(alignment: .leading) {
                        // Background track
                        Rectangle()
                            .fill(Color.white.opacity(0.3))
                            .frame(height: 2)
                        
                        // Zoom indicator
                        Rectangle()
                            .fill(Color.yellow)
                            .frame(width: 2, height: 12)
                            .offset(x: normalizedPosition * sliderWidth)
                        
                        // Lens position indicators
                        ForEach(availableLenses, id: \.self) { lens in
                            Rectangle()
                                .fill(Color.white.opacity(0.5))
                                .frame(width: 1, height: 8)
                                .offset(x: normalizedZoomPosition(for: lens) * sliderWidth)
                        }
                    }
                    .frame(width: sliderWidth)
                    .gesture(
                        DragGesture(minimumDistance: 0)
                            .onChanged { value in
                                isDragging = true
                                
                                // Calculate velocity
                                if let lastLocation = lastDragLocation,
                                   let lastTime = lastDragTime {
                                    let deltaX = value.location.x - lastLocation.x
                                    let deltaTime = Date().timeIntervalSince(lastTime)
                                    dragVelocity = CGFloat(deltaX / CGFloat(deltaTime))
                                }
                                
                                lastDragLocation = value.location
                                lastDragTime = Date()
                                
                                // Apply zoom with velocity sensitivity
                                let normalizedX = value.location.x / sliderWidth
                                let velocityFactor = min(abs(dragVelocity) / 1000, 2.0)
                                let zoomDelta = (normalizedX - normalizedPosition) * velocityFactor
                                let newZoom = calculateZoomFactor(for: normalizedPosition + zoomDelta)
                                
                                // Handle lens transitions
                                switch viewModel.currentLens {
                                case .wide:
                                    if newZoom >= 2.0 {
                                        viewModel.switchToLens(.x2)
                                    } else {
                                        viewModel.setZoomFactor(newZoom)
                                    }
                                case .x2:
                                    if newZoom <= 1.0 {
                                        viewModel.switchToLens(.wide)
                                    } else if newZoom >= 5.0 && availableLenses.contains(.telephoto) {
                                        viewModel.switchToLens(.telephoto)
                                    } else {
                                        viewModel.setZoomFactor(newZoom)
                                    }
                                case .telephoto:
                                    if newZoom <= 2.0 {
                                        viewModel.switchToLens(.x2)
                                    } else {
                                        viewModel.setZoomFactor(min(newZoom, 10.0))
                                    }
                                default:
                                    viewModel.setZoomFactor(newZoom)
                                }
                            }
                            .onEnded { _ in
                                isDragging = false
                                lastDragLocation = nil
                                lastDragTime = nil
                                dragVelocity = 0
                            }
                    )
                    Spacer()
                }
            }
            .frame(height: 20)
            
            // Current zoom display
            Text(String(format: "%.1f√ó", viewModel.currentZoomFactor))
                .font(.system(size: 12, weight: .medium))
                .foregroundColor(.yellow)
                .opacity(isDragging ? 1 : 0)
            
            // Lens buttons
            HStack(spacing: 14) {
                ForEach(availableLenses, id: \.self) { lens in
                    Button(action: {
                        viewModel.switchToLens(lens)
                    }) {
                        Text(lens.rawValue + "√ó")
                            .font(.system(size: viewModel.currentLens == lens ? 17 : 15, weight: .medium))
                            .foregroundColor(viewModel.currentLens == lens ? .yellow : .white)
                            .frame(width: viewModel.currentLens == lens ? 42 : 36, height: viewModel.currentLens == lens ? 42 : 36)
                            .background(
                                Circle()
                                    .fill(Color.black.opacity(0.65))
                            )
                    }
                    .transition(.scale)
                    .animation(.spring(response: 0.3), value: viewModel.currentLens == lens)
                }
            }
        }
    }
    
    private var normalizedPosition: CGFloat {
        (viewModel.currentZoomFactor - minZoom) / (maxZoom - minZoom)
    }
    
    private func normalizedZoomPosition(for lens: CameraLens) -> CGFloat {
        (lens.zoomFactor - minZoom) / (maxZoom - minZoom)
    }
    
    private func calculateZoomFactor(for normalized: CGFloat) -> CGFloat {
        let clamped = normalized.clamped(to: 0...1)
        return minZoom + (clamped * (maxZoom - minZoom))
    }
}

extension Comparable {
    func clamped(to range: ClosedRange<Self>) -> Self {
        return min(max(self, range.lowerBound), range.upperBound)
    }
}

================
File: camera/Features/Camera/CameraViewModel.swift
================
import AVFoundation
import SwiftUI
import Photos
import VideoToolbox
import CoreVideo
import os.log
import CoreImage
import CoreMedia

extension CFString {
    var string: String {
        self as String
    }
}

extension AVCaptureDevice.Format {
    var dimensions: CMVideoDimensions? {
        CMVideoFormatDescriptionGetDimensions(formatDescription)
    }
}

class CameraViewModel: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate {
    // Add property to track the view containing the camera preview
    weak var owningView: UIView?
    
    // Flashlight manager
    private let flashlightManager = FlashlightManager()
    private var settingsObserver: NSObjectProtocol?
    
    enum Status {
        case unknown
        case running
        case failed
        case unauthorized
    }
    @Published var status: Status = .unknown
    
    enum CaptureMode {
        case photo
        case video
    }
    @Published var captureMode: CaptureMode = .video
    
    private let logger = Logger(subsystem: "com.camera", category: "CameraViewModel")
    
    @Published var isSessionRunning = false
    @Published var error: CameraError?
    @Published var whiteBalance: Float = 5000 // Kelvin
    @Published var iso: Float = 100
    @Published var shutterSpeed: CMTime = CMTimeMake(value: 1, timescale: 60)
    @Published var isRecording = false {
        didSet {
            NotificationCenter.default.post(name: NSNotification.Name("RecordingStateChanged"), object: nil)
            
            // Handle flashlight state based on recording state
            let settings = SettingsModel()
            if isRecording && settings.isFlashlightEnabled {
                Task {
                    await flashlightManager.performStartupSequence()
                }
            } else {
                flashlightManager.cleanup()
            }
        }
    }
    @Published var recordingFinished = false
    @Published var isSettingsPresented = false
    @Published var isProcessingRecording = false
    
    // Add thumbnail property
    @Published var lastRecordedVideoThumbnail: UIImage?
    
    // Storage for temporarily disabling LUT preview without losing the filter
    var tempLUTFilter: CIFilter? {
        didSet {
            if tempLUTFilter != nil {
                print("DEBUG: CameraViewModel stored LUT filter temporarily")
            } else if oldValue != nil {
                print("DEBUG: CameraViewModel cleared temporary LUT filter")
            }
        }
    }
    
    @Published var isAppleLogEnabled = true { // Set Apple Log enabled by default
        didSet {
            print("\n=== Apple Log Toggle ===")
            print("üîÑ Status: \(status)")
            print("üìπ Capture Mode: \(captureMode)")
            print("‚úÖ Attempting to set Apple Log to: \(isAppleLogEnabled)")
            
            guard status == .running, captureMode == .video else {
                print("‚ùå Cannot configure Apple Log - Status or mode incorrect")
                print("Required: status == .running (is: \(status))")
                print("Required: captureMode == .video (is: \(captureMode))")
                return
            }
            
            // If enabling Apple Log and current lens is ultraWide, auto-switch to wide if available
            if isAppleLogEnabled && currentLens == .ultraWide && availableLenses.contains(.wide) {
                print("üîÑ Auto-switching from ultra-wide to wide lens for Apple Log support")
                switchToLens(.wide)
                // The Apple Log will be configured after the lens switch completes
                return
            }
            
            // Check if current lens is ultra-wide
            if currentLens == .ultraWide {
                print("‚ö†Ô∏è Warning: Apple Log may not be supported on ultra-wide (0.5√ó) lens")
            }
            
            Task {
                do {
                    if isAppleLogEnabled {
                        print("üé• Configuring Apple Log...")
                        try await videoFormatService.configureAppleLog()
                    } else {
                        print("‚Ü©Ô∏è Resetting Apple Log...")
                        try await videoFormatService.resetAppleLog()
                    }
                    
                    // Update recording service with new Apple Log setting
                    recordingService.setAppleLogEnabled(isAppleLogEnabled)
                    videoFormatService.setAppleLogEnabled(isAppleLogEnabled)
                } catch {
                    print("‚ùå Error setting Apple Log: \(error)")
                    
                    // Check if this is the specific ultra-wide lens error
                    if let cameraError = error as? CameraError,
                       case .custom(let message) = cameraError,
                       message.contains("ultra-wide lens") {
                        // Reset the Apple Log state only in the UI, since it can't actually be enabled
                        DispatchQueue.main.async {
                            self.isAppleLogEnabled = false
                            
                            // Offer to switch to a supported lens
                            if availableLenses.contains(.wide) {
                                self.showAlert(
                                    title: "Apple Log Not Supported",
                                    message: "Apple Log is not supported on the 0.5√ó ultra-wide lens. Would you like to switch to the 1√ó lens?",
                                    primaryButton: .default("Switch to 1√ó") {
                                        self.switchToLens(.wide)
                                        // Re-enable Apple Log after switching lens
                                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                                            self.isAppleLogEnabled = true
                                        }
                                    },
                                    secondaryButton: .cancel()
                                )
                            } else {
                                self.showAlert(
                                    title: "Apple Log Not Supported", 
                                    message: "Apple Log is not supported on the 0.5√ó ultra-wide lens. Please switch to a different lens."
                                )
                            }
                        }
                    } else {
                        await MainActor.run {
                            self.error = .configurationFailed
                        }
                        logger.error("Failed to configure Apple Log: \(error.localizedDescription)")
                    }
                }
            }
            print("=== End Apple Log Toggle ===\n")
        }
    }
    
    @Published private(set) var isAppleLogSupported = false
    
    let session = AVCaptureSession()
    private var device: AVCaptureDevice?
    
    // Video recording properties
    private var assetWriter: AVAssetWriter?
    private var assetWriterInput: AVAssetWriterInput?
    private var assetWriterPixelBufferAdaptor: AVAssetWriterInputPixelBufferAdaptor?
    private var videoDataOutput: AVCaptureVideoDataOutput?
    private var audioDataOutput: AVCaptureAudioDataOutput?
    private var currentRecordingURL: URL?
    private var recordingStartTime: CMTime?
    
    private var defaultFormat: AVCaptureDevice.Format?
    
    var minISO: Float {
        device?.activeFormat.minISO ?? 50
    }
    var maxISO: Float {
        device?.activeFormat.maxISO ?? 1600
    }
    
    @Published var selectedFrameRate: Double = 30.0
    let availableFrameRates: [Double] = [23.976, 24.0, 25.0, 29.97, 30.0]
    
    private var orientationObserver: NSObjectProtocol?
    @Published private(set) var currentInterfaceOrientation: UIInterfaceOrientation = .portrait
    
    private let processingQueue = DispatchQueue(
        label: "com.camera.processing",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem
    )
    
    private var lastFrameTimestamp: CFAbsoluteTime = 0
    private var lastFrameTime: CMTime?
    private var frameCount: Int = 0
    private var frameRateAccumulator: Double = 0
    private var frameRateUpdateInterval: Int = 30
    
    private var supportedFrameRateRange: AVFrameRateRange? {
        device?.activeFormat.videoSupportedFrameRateRanges.first
    }
    
    // Resolution settings
    enum Resolution: String, CaseIterable {
        case uhd = "4K (3840x2160)"
        case hd = "HD (1920x1080)"
        case sd = "720p (1280x720)"
        
        var dimensions: CMVideoDimensions {
            switch self {
            case .uhd: return CMVideoDimensions(width: 3840, height: 2160)
            case .hd: return CMVideoDimensions(width: 1920, height: 1080)
            case .sd: return CMVideoDimensions(width: 1280, height: 720)
            }
        }
    }
    
    @Published var selectedResolution: Resolution = .uhd {
        didSet {
            Task {
                do {
                    try await videoFormatService.updateCameraFormat(for: selectedResolution)
                } catch {
                    print("Error updating camera format: \(error)")
                    self.error = .configurationFailed
                }
            }
        }
    }
    
    // Codec settings
    enum VideoCodec: String, CaseIterable {
        case hevc = "HEVC (H.265)"
        case proRes = "Apple ProRes"
        
        var avCodecKey: AVVideoCodecType {
            switch self {
            case .hevc: return .hevc
            case .proRes: return .proRes422HQ
            }
        }
        
        var bitrate: Int {
            switch self {
            case .hevc: return 50_000_000 // Increased to 50 Mbps for 4:2:2
            case .proRes: return 0 // ProRes doesn't use bitrate control
            }
        }
    }
    
    @Published var selectedCodec: VideoCodec = .hevc { // Set HEVC as default codec
        didSet {
            updateVideoConfiguration()
        }
    }
    
    private var videoConfiguration: [String: Any] = [
        AVVideoCodecKey: AVVideoCodecType.proRes422,
        AVVideoCompressionPropertiesKey: [
            AVVideoAverageBitRateKey: 50_000_000,
            AVVideoMaxKeyFrameIntervalKey: 1,
            AVVideoAllowFrameReorderingKey: false,
            AVVideoExpectedSourceFrameRateKey: 30
        ]
    ]
    
    private struct FrameRates {
        static let ntsc23_976 = CMTime(value: 1001, timescale: 24000)
        static let ntsc29_97 = CMTime(value: 1001, timescale: 30000)
        static let film24 = CMTime(value: 1, timescale: 24)
        static let pal25 = CMTime(value: 1, timescale: 25)
        static let ntsc30 = CMTime(value: 1, timescale: 30)
    }
    
    @Published var currentTint: Double = 0.0 // Range: -150 to +150
    private let tintRange = (-150.0...150.0)
    
    private var videoDeviceInput: AVCaptureDeviceInput?
    
    @Published var isAutoExposureEnabled: Bool = true {
        didSet {
            exposureService.setAutoExposureEnabled(isAutoExposureEnabled)
        }
    }
    
    @Published var lutManager = LUTManager()
    private var ciContext = CIContext()
    
    // Add flag to lock orientation updates during recording
    private var recordingOrientationLocked = false
    
    // Save the original rotation values to restore them after recording
    private var originalRotationValues: [AVCaptureConnection: CGFloat] = [:]
    
    @Published var currentLens: CameraLens = .wide
    @Published var availableLenses: [CameraLens] = []
    
    @Published var currentZoomFactor: CGFloat = 1.0
    private var lastZoomFactor: CGFloat = 1.0
    
    // HEVC Hardware Encoding Properties
    private var compressionSession: VTCompressionSession?
    private let encoderQueue = DispatchQueue(label: "com.camera.encoder", qos: .userInteractive)
    
    // Add constants that are missing from VideoToolbox
    private enum VTConstants {
        static let hardwareAcceleratorOnly = "EnableHardwareAcceleratedVideoEncoder" as CFString
        static let priority = "Priority" as CFString
        static let priorityRealtimePreview = "RealtimePreview" as CFString
        
        // Color space constants for HEVC
        static let primariesITUR709 = "ITU_R_709_2"
        static let primariesBT2020 = "ITU_R_2020"
        static let yCbCrMatrix2020 = "ITU_R_2020"
        static let yCbCrMatrixITUR709 = "ITU_R_709_2"
        
        // HEVC Profile constants
        static let hevcMain422_10Profile = "HEVC_Main42210_AutoLevel"
    }
    
    private var encoderSpecification: [CFString: Any] {
        [
            kVTVideoEncoderSpecification_EnableHardwareAcceleratedVideoEncoder: true,
            kVTVideoEncoderSpecification_RequireHardwareAcceleratedVideoEncoder: true,
            kVTVideoEncoderSpecification_EnableLowLatencyRateControl: true
        ]
    }
    
    // Store recording orientation
    private var recordingOrientation: CGFloat?
    
    // Service Instances
    private var cameraSetupService: CameraSetupService!
    private var exposureService: ExposureService!
    private var recordingService: RecordingService!
    private var cameraDeviceService: CameraDeviceService!
    private var videoFormatService: VideoFormatService!
    
    override init() {
        super.init()
        print("\n=== Camera Initialization ===")
        
        // Initialize services
        setupServices()
        
        // Add observer for flashlight settings changes
        settingsObserver = NotificationCenter.default.addObserver(
            forName: .flashlightSettingChanged,
            object: nil,
            queue: .main
        ) { [weak self] _ in
            guard let self = self else { return }
            let settings = SettingsModel()
            if self.isRecording && settings.isFlashlightEnabled {
                self.flashlightManager.isEnabled = true
                self.flashlightManager.intensity = settings.flashlightIntensity
            } else {
                self.flashlightManager.isEnabled = false
            }
        }
        
        // Add observer for bake in LUT setting changes
        NotificationCenter.default.addObserver(
            forName: .bakeInLUTSettingChanged,
            object: nil,
            queue: .main
        ) { [weak self] _ in
            guard let self = self else { return }
            let settings = SettingsModel()
            self.recordingService.setBakeInLUTEnabled(settings.isBakeInLUTEnabled)
        }
        
        do {
            try cameraSetupService.setupSession()
            
            if let device = device {
                print("üìä Device Capabilities:")
                print("- Name: \(device.localizedName)")
                print("- Model ID: \(device.modelID)")
                
                isAppleLogSupported = device.formats.contains { format in
                    format.supportedColorSpaces.contains(.appleLog)
                }
                print("\n‚úÖ Apple Log Support: \(isAppleLogSupported)")
            }
            print("=== End Initialization ===\n")
            
            if let device = device {
                defaultFormat = device.activeFormat
            }
        } catch {
            self.error = .setupFailed
            print("Failed to setup session: \(error)")
        }
        
        // Add orientation change observer
        orientationObserver = NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { [weak self] _ in
                guard let self = self,
                      let videoConnection = self.session.outputs.first?.connection(with: .video) else { return }

                self.cameraDeviceService.updateVideoOrientation(for: videoConnection, orientation: self.currentInterfaceOrientation)
        }
        
        // Set initial shutter angle
        updateShutterAngle(180.0)
        
        print("üì± LUT Loading: No default LUTs will be loaded")
    }
    
    deinit {
        if let observer = orientationObserver {
            NotificationCenter.default.removeObserver(observer)
        }
        
        if let observer = settingsObserver {
            NotificationCenter.default.removeObserver(observer)
        }
        
        flashlightManager.cleanup()
    }
    
    private func setupServices() {
        // Initialize services with self as delegate
        cameraSetupService = CameraSetupService(session: session, delegate: self)
        exposureService = ExposureService(delegate: self)
        recordingService = RecordingService(session: session, delegate: self)
        cameraDeviceService = CameraDeviceService(session: session, delegate: self)
        videoFormatService = VideoFormatService(session: session, delegate: self)
    }
    
    func updateWhiteBalance(_ temperature: Float) {
        exposureService.updateWhiteBalance(temperature)
    }
    
    func updateISO(_ iso: Float) {
        exposureService.updateISO(iso)
    }
    
    func updateShutterSpeed(_ speed: CMTime) {
        exposureService.updateShutterSpeed(speed)
    }
    
    func updateShutterAngle(_ angle: Double) {
        exposureService.updateShutterAngle(angle, frameRate: selectedFrameRate)
    }
    
    func updateFrameRate(_ fps: Double) {
        do {
            try videoFormatService.updateFrameRate(fps)
        } catch {
            print("‚ùå Frame rate error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    func updateTint(_ newValue: Double) {
        currentTint = newValue.clamped(to: tintRange)
        exposureService.updateTint(currentTint, currentWhiteBalance: whiteBalance)
    }
    
    func updateOrientation(_ orientation: UIInterfaceOrientation) {
        self.currentInterfaceOrientation = orientation
        
        if let videoConnection = session.outputs.first?.connection(with: .video) {
            cameraDeviceService.updateVideoOrientation(for: videoConnection, orientation: orientation)
        }
        
        print("DEBUG: UI Interface orientation updated to: \(orientation.rawValue)")
    }
    
    func switchToLens(_ lens: CameraLens) {
        cameraDeviceService.switchToLens(lens)
        
        // Update orientation for all video connections after lens switch
        // This ensures LUT overlay orientation remains correct
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { [weak self] in
            guard let self = self else { return }
            
            // Update all video connections to maintain proper orientation
            for output in self.session.outputs {
                if let connection = output.connection(with: .video) {
                    self.cameraDeviceService.updateVideoOrientation(for: connection, orientation: self.currentInterfaceOrientation)
                }
            }
        }
    }
    
    func setZoomFactor(_ factor: CGFloat) {
        cameraDeviceService.setZoomFactor(factor, currentLens: currentLens, availableLenses: availableLenses)
    }
    
    @MainActor
    func startRecording() async {
        guard !isRecording, status == .running, let device = self.device else { return }
        
        // Get current settings
        let settings = SettingsModel()
        
        // Update configuration for recording
        recordingService.setDevice(device)
        recordingService.setLUTManager(lutManager)
        recordingService.setAppleLogEnabled(isAppleLogEnabled)
        recordingService.setBakeInLUTEnabled(settings.isBakeInLUTEnabled)
        recordingService.setVideoConfiguration(
            frameRate: selectedFrameRate,
            resolution: selectedResolution,
            codec: selectedCodec
        )
        
        // Lock orientation during recording
        cameraDeviceService.lockOrientationForRecording(true)
        
        // Get current orientation for recording
        let recordingOrientation = session.outputs.first?.connection(with: .video)?.videoRotationAngle ?? 0
        
        // Start recording
        await recordingService.startRecording(orientation: recordingOrientation)
        
        isRecording = true
    }
    
    @MainActor
    func stopRecording() async {
        guard isRecording else { return }
        
        // Stop recording
        await recordingService.stopRecording()
        
        // Unlock orientation after recording
        cameraDeviceService.lockOrientationForRecording(false)
        
        isRecording = false
    }
    
    private func updateVideoConfiguration() {
        // Update recording service with new codec
        recordingService.setVideoConfiguration(
            frameRate: selectedFrameRate,
            resolution: selectedResolution,
            codec: selectedCodec
        )
        
        print("\n=== Updating Video Configuration ===")
        print("üé¨ Selected Codec: \(selectedCodec.rawValue)")
        print("üé® Apple Log Enabled: \(isAppleLogEnabled)")
        
        if selectedCodec == .proRes {
            print("‚úÖ Configured for ProRes recording")
        } else {
            print("‚úÖ Configured for HEVC recording")
            print("üìä Bitrate: \(selectedCodec.bitrate / 1_000_000) Mbps")
        }
        
        print("=== End Video Configuration ===\n")
    }
    
    // MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
    
    // Track frame counts for logging
    private var videoFrameCount = 0
    private var audioFrameCount = 0
    private var successfulVideoFrames = 0
    private var failedVideoFrames = 0
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let assetWriter = assetWriter,
              assetWriter.status == .writing else {
            return
        }
        
        // Handle video data
        if output == videoDataOutput,
           let assetWriterInput = assetWriterInput,
           assetWriterInput.isReadyForMoreMediaData {
            
            videoFrameCount += 1
            
            // Log every 30 frames to avoid flooding
            let shouldLog = videoFrameCount % 30 == 0
            if shouldLog {
                print("üìΩÔ∏è Processing video frame #\(videoFrameCount), writer status: \(assetWriter.status.rawValue)")
            }
            
            if let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) {
                if let lutFilter = tempLUTFilter ?? lutManager.currentLUTFilter {
                    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
                    if let processedImage = applyLUT(to: ciImage, using: lutFilter),
                       let processedPixelBuffer = createPixelBuffer(from: processedImage, with: pixelBuffer) {
                        
                        // Use original timing information
                        var timing = CMSampleTimingInfo()
                        CMSampleBufferGetSampleTimingInfo(sampleBuffer, at: 0, timingInfoOut: &timing)
                        
                        // Create format description for processed buffer
                        var info: CMFormatDescription?
                        let status = CMVideoFormatDescriptionCreateForImageBuffer(
                            allocator: kCFAllocatorDefault,
                            imageBuffer: processedPixelBuffer,
                            formatDescriptionOut: &info
                        )
                        
                        if status == noErr, let info = info,
                           let newSampleBuffer = createSampleBuffer(
                            from: processedPixelBuffer,
                            formatDescription: info,
                            timing: &timing
                           ) {
                            assetWriterInput.append(newSampleBuffer)
                            successfulVideoFrames += 1
                            if shouldLog {
                                print("‚úÖ Successfully appended processed frame #\(successfulVideoFrames)")
                            }
                        } else {
                            failedVideoFrames += 1
                            print("‚ö†Ô∏è Failed to create format description for processed frame #\(videoFrameCount), status: \(status)")
                        }
                    }
                } else {
                    // No LUT processing needed - use original sample buffer directly
                    assetWriterInput.append(sampleBuffer)
                    successfulVideoFrames += 1
                    if shouldLog {
                        print("‚úÖ Successfully appended original frame #\(successfulVideoFrames)")
                    }
                }
            }
        }
        
        // Handle audio data
        if output == audioDataOutput,
           let audioInput = assetWriter.inputs.first(where: { $0.mediaType == .audio }),
           audioInput.isReadyForMoreMediaData {
            audioFrameCount += 1
            audioInput.append(sampleBuffer)
            if audioFrameCount % 100 == 0 {
                print("üéµ Processed audio frame #\(audioFrameCount)")
            }
        }
    }
    
    private func createPixelBuffer(from ciImage: CIImage, with template: CVPixelBuffer) -> CVPixelBuffer? {
        var newPixelBuffer: CVPixelBuffer?
        CVPixelBufferCreate(kCFAllocatorDefault,
                           CVPixelBufferGetWidth(template),
                           CVPixelBufferGetHeight(template),
                           CVPixelBufferGetPixelFormatType(template),
                           [kCVPixelBufferIOSurfacePropertiesKey as String: [:]] as CFDictionary,
                           &newPixelBuffer)
        
        guard let outputBuffer = newPixelBuffer else { 
            print("‚ö†Ô∏è Failed to create pixel buffer from CI image")
            return nil 
        }
        
        ciContext.render(ciImage, to: outputBuffer)
        return outputBuffer
    }

    // Add helper property for tracking keyframes
    private var lastKeyFrameTime: CMTime?

    // Add helper method for creating sample buffers
    private func createSampleBuffer(
        from pixelBuffer: CVPixelBuffer,
        formatDescription: CMFormatDescription,
        timing: UnsafeMutablePointer<CMSampleTimingInfo>
    ) -> CMSampleBuffer? {
        var sampleBuffer: CMSampleBuffer?
        let status = CMSampleBufferCreateForImageBuffer(
            allocator: kCFAllocatorDefault,
            imageBuffer: pixelBuffer,
            dataReady: true,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: formatDescription,
            sampleTiming: timing,
            sampleBufferOut: &sampleBuffer
        )
        
        if status != noErr {
            print("‚ö†Ô∏è Failed to create sample buffer: \(status)")
            return nil
        }
        
        return sampleBuffer
    }

    private func getVideoTransform(for orientation: UIInterfaceOrientation) -> CGAffineTransform {
        switch orientation {
        case .portrait:
            return CGAffineTransform(rotationAngle: .pi/2) // 90 degrees clockwise
        case .portraitUpsideDown:
            return CGAffineTransform(rotationAngle: -.pi/2) // 90 degrees counterclockwise
        case .landscapeLeft: // USB on right
            return CGAffineTransform(rotationAngle: .pi) // 180 degrees (was .identity)
        case .landscapeRight: // USB on left
            return .identity // No rotation (was .pi)
        default:
            return .identity
        }
    }

    // MARK: - LUT Processing

    private func applyLUT(to image: CIImage, using filter: CIFilter) -> CIImage? {
        filter.setValue(image, forKey: kCIInputImageKey)
        return filter.outputImage
    }

    // MARK: - Error Handling

    // Common error handler for all delegate protocols
    func didEncounterError(_ error: CameraError) {
        DispatchQueue.main.async {
            self.error = error
        }
    }

    // MARK: - Video Frame Processing
    
    func processVideoFrame(_ sampleBuffer: CMSampleBuffer) -> CVPixelBuffer? {
        // Process the frame if needed, or handle any frame-level logic
        // For now, just returning the pixel buffer from the sample buffer
        return CMSampleBufferGetImageBuffer(sampleBuffer)
    }
}

// MARK: - VideoFormatServiceDelegate

extension CameraViewModel: VideoFormatServiceDelegate {
    func didUpdateFrameRate(_ frameRate: Double) {
        DispatchQueue.main.async {
            self.selectedFrameRate = frameRate
        }
    }
}

extension CameraError {
    static func configurationFailed(message: String = "Camera configuration failed") -> CameraError {
        return .custom(message: message)
    }
}

// MARK: - CameraSetupServiceDelegate

extension CameraViewModel: CameraSetupServiceDelegate {
    func didUpdateSessionStatus(_ status: Status) {
        DispatchQueue.main.async {
            self.status = status
        }
    }
    
    func didInitializeCamera(device: AVCaptureDevice) {
        self.device = device
        exposureService.setDevice(device)
        recordingService.setDevice(device)
        cameraDeviceService.setDevice(device)
        videoFormatService.setDevice(device)
        
        // Check Apple Log support for initial device
        isAppleLogSupported = device.formats.contains { format in
            format.supportedColorSpaces.contains(.appleLog)
        }
        
        // Initialize Apple Log if supported
        if isAppleLogSupported && isAppleLogEnabled {
            Task {
                do {
                    try await videoFormatService.configureAppleLog()
                } catch {
                    print("Failed to configure initial Apple Log: \(error)")
                }
            }
        }
        
        // Initialize available lenses
        availableLenses = CameraLens.availableLenses()
        
        // Check Apple Log support for all available lenses
        checkAppleLogSupportForAllLenses()
    }
    
    private func checkAppleLogSupportForAllLenses() {
        // Get discovery session for all possible back cameras
        let discoverySession = AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInWideAngleCamera, .builtInUltraWideCamera, .builtInTelephotoCamera],
            mediaType: .video,
            position: .back
        )
        
        print("\n=== Apple Log Support Check ===")
        
        for lens in availableLenses {
            // Find the device for this lens
            if let device = discoverySession.devices.first(where: { $0.deviceType == lens.deviceType }) {
                // Check if any formats for this device support Apple Log
                let supportsAppleLog = device.formats.contains { format in
                    format.supportedColorSpaces.contains(.appleLog)
                }
                
                print("üì∏ \(lens.rawValue)√ó lens (\(device.localizedName)): Apple Log \(supportsAppleLog ? "‚úÖ SUPPORTED" : "‚ùå NOT SUPPORTED")")
                
                // For unsupported lenses, log all available color spaces
                if !supportsAppleLog {
                    let supportedColorSpaces = device.formats.flatMap { $0.supportedColorSpaces }
                    let uniqueColorSpaces = Array(Set(supportedColorSpaces.map { $0.rawValue }))
                    print("   Available color spaces: \(uniqueColorSpaces)")
                }
            }
        }
        
        print("=== End Apple Log Support Check ===\n")
    }
    
    func didStartRunning(_ isRunning: Bool) {
        DispatchQueue.main.async {
            self.isSessionRunning = isRunning
        }
    }
}

// MARK: - ExposureServiceDelegate

extension CameraViewModel: ExposureServiceDelegate {
    func didUpdateWhiteBalance(_ temperature: Float) {
        DispatchQueue.main.async {
            self.whiteBalance = temperature
        }
    }
    
    func didUpdateISO(_ iso: Float) {
        DispatchQueue.main.async {
            self.iso = iso
        }
    }
    
    func didUpdateShutterSpeed(_ speed: CMTime) {
        DispatchQueue.main.async {
            self.shutterSpeed = speed
        }
    }
}

// MARK: - RecordingServiceDelegate

extension CameraViewModel: RecordingServiceDelegate {
    func didStartRecording() {
        // Handled by the isRecording property
    }
    
    func didStopRecording() {
        // Handled by the isRecording property
    }
    
    func didFinishSavingVideo(thumbnail: UIImage?) {
        DispatchQueue.main.async {
            self.recordingFinished = true
            self.lastRecordedVideoThumbnail = thumbnail
        }
    }
    
    func didUpdateProcessingState(_ isProcessing: Bool) {
        DispatchQueue.main.async {
            self.isProcessingRecording = isProcessing
        }
    }
}

// MARK: - CameraDeviceServiceDelegate

extension CameraViewModel: CameraDeviceServiceDelegate {
    func didUpdateCurrentLens(_ lens: CameraLens) {
        DispatchQueue.main.async {
            self.currentLens = lens
        }
    }
    
    func didUpdateZoomFactor(_ factor: CGFloat) {
        DispatchQueue.main.async {
            self.currentZoomFactor = factor
        }
    }
}

================
File: camera/Features/Camera/CameraViewModel.swift.old
================
import AVFoundation
import SwiftUI
import Photos
import VideoToolbox
import CoreVideo
import os.log
import CoreImage
import CoreMedia

extension CFString {
    var string: String {
        self as String
    }
}

extension AVCaptureDevice.Format {
    var dimensions: CMVideoDimensions? {
        CMVideoFormatDescriptionGetDimensions(formatDescription)
    }
}

class CameraViewModel: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate {
    // Add property to track the view containing the camera preview
    weak var owningView: UIView?
    
    // Flashlight manager
    private let flashlightManager = FlashlightManager()
    private var settingsObserver: NSObjectProtocol?
    
    enum Status {
        case unknown
        case running
        case failed
        case unauthorized
    }
    @Published var status: Status = .unknown
    
    enum CaptureMode {
        case photo
        case video
    }
    @Published var captureMode: CaptureMode = .video
    
    private let logger = Logger(subsystem: "com.camera", category: "CameraViewModel")
    
    @Published var isSessionRunning = false
    @Published var error: CameraError?
    @Published var whiteBalance: Float = 5000 // Kelvin
    @Published var iso: Float = 100
    @Published var shutterSpeed: CMTime = CMTimeMake(value: 1, timescale: 60)
    @Published var isRecording = false {
        didSet {
            NotificationCenter.default.post(name: NSNotification.Name("RecordingStateChanged"), object: nil)
            
            // Handle flashlight state based on recording state
            let settings = SettingsModel()
            if isRecording && settings.isFlashlightEnabled {
                Task {
                    await flashlightManager.performStartupSequence()
                }
            } else {
                flashlightManager.cleanup()
            }
        }
    }
    @Published var recordingFinished = false
    @Published var isSettingsPresented = false
    @Published var isProcessingRecording = false
    
    // Add thumbnail property
    @Published var lastRecordedVideoThumbnail: UIImage?
    
    // Storage for temporarily disabling LUT preview without losing the filter
    var tempLUTFilter: CIFilter? {
        didSet {
            if tempLUTFilter != nil {
                print("DEBUG: CameraViewModel stored LUT filter temporarily")
            } else if oldValue != nil {
                print("DEBUG: CameraViewModel cleared temporary LUT filter")
            }
        }
    }
    
    @Published var isAppleLogEnabled = true { // Set Apple Log enabled by default
        didSet {
            print("\n=== Apple Log Toggle ===")
            print("üîÑ Status: \(status)")
            print("üìπ Capture Mode: \(captureMode)")
            print("‚úÖ Attempting to set Apple Log to: \(isAppleLogEnabled)")
            
            guard status == .running, captureMode == .video else {
                print("‚ùå Cannot configure Apple Log - Status or mode incorrect")
                print("Required: status == .running (is: \(status))")
                print("Required: captureMode == .video (is: \(captureMode))")
                return
            }
            
            Task {
                do {
                    if isAppleLogEnabled {
                        print("üé• Configuring Apple Log...")
                        try await videoFormatService.configureAppleLog()
                    } else {
                        print("‚Ü©Ô∏è Resetting Apple Log...")
                        try await videoFormatService.resetAppleLog()
                    }
                    
                    // Update recording service with new Apple Log setting
                    recordingService.setAppleLogEnabled(isAppleLogEnabled)
                    videoFormatService.setAppleLogEnabled(isAppleLogEnabled)
                } catch {
                    await MainActor.run {
                        self.error = .configurationFailed
                    }
                    logger.error("Failed to configure Apple Log: \(error.localizedDescription)")
                    print("‚ùå Apple Log configuration failed: \(error)")
                }
            }
            print("=== End Apple Log Toggle ===\n")
        }
    }
    
    @Published private(set) var isAppleLogSupported = false
    
    let session = AVCaptureSession()
    private var device: AVCaptureDevice?
    
    // Video recording properties
    private var assetWriter: AVAssetWriter?
    private var assetWriterInput: AVAssetWriterInput?
    private var assetWriterPixelBufferAdaptor: AVAssetWriterInputPixelBufferAdaptor?
    private var videoDataOutput: AVCaptureVideoDataOutput?
    private var audioDataOutput: AVCaptureAudioDataOutput?
    private var currentRecordingURL: URL?
    private var recordingStartTime: CMTime?
    
    private var defaultFormat: AVCaptureDevice.Format?
    
    var minISO: Float {
        device?.activeFormat.minISO ?? 50
    }
    var maxISO: Float {
        device?.activeFormat.maxISO ?? 1600
    }
    
    @Published var selectedFrameRate: Double = 30.0
    let availableFrameRates: [Double] = [23.976, 24.0, 25.0, 29.97, 30.0]
    
    private var orientationObserver: NSObjectProtocol?
    @Published private(set) var currentInterfaceOrientation: UIInterfaceOrientation = .portrait
    
    private let processingQueue = DispatchQueue(
        label: "com.camera.processing",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem
    )
    
    private var lastFrameTimestamp: CFAbsoluteTime = 0
    private var lastFrameTime: CMTime?
    private var frameCount: Int = 0
    private var frameRateAccumulator: Double = 0
    private var frameRateUpdateInterval: Int = 30
    
    private var supportedFrameRateRange: AVFrameRateRange? {
        device?.activeFormat.videoSupportedFrameRateRanges.first
    }
    
    // Resolution settings
    enum Resolution: String, CaseIterable {
        case uhd = "4K (3840x2160)"
        case hd = "HD (1920x1080)"
        case sd = "720p (1280x720)"
        
        var dimensions: CMVideoDimensions {
            switch self {
            case .uhd: return CMVideoDimensions(width: 3840, height: 2160)
            case .hd: return CMVideoDimensions(width: 1920, height: 1080)
            case .sd: return CMVideoDimensions(width: 1280, height: 720)
            }
        }
    }
    
    @Published var selectedResolution: Resolution = .uhd {
        didSet {
            Task {
                do {
                    try await videoFormatService.updateCameraFormat(for: selectedResolution)
                } catch {
                    print("Error updating camera format: \(error)")
                    self.error = .configurationFailed
                }
            }
        }
    }
    
    // Codec settings
    enum VideoCodec: String, CaseIterable {
        case hevc = "HEVC (H.265)"
        case proRes = "Apple ProRes"
        
        var avCodecKey: AVVideoCodecType {
            switch self {
            case .hevc: return .hevc
            case .proRes: return .proRes422HQ
            }
        }
        
        var bitrate: Int {
            switch self {
            case .hevc: return 50_000_000 // Increased to 50 Mbps for 4:2:2
            case .proRes: return 0 // ProRes doesn't use bitrate control
            }
        }
    }
    
    @Published var selectedCodec: VideoCodec = .hevc { // Set HEVC as default codec
        didSet {
            updateVideoConfiguration()
        }
    }
    
    private var videoConfiguration: [String: Any] = [
        AVVideoCodecKey: AVVideoCodecType.proRes422,
        AVVideoCompressionPropertiesKey: [
            AVVideoAverageBitRateKey: 50_000_000,
            AVVideoMaxKeyFrameIntervalKey: 1,
            AVVideoAllowFrameReorderingKey: false,
            AVVideoExpectedSourceFrameRateKey: 30
        ]
    ]
    
    private struct FrameRates {
        static let ntsc23_976 = CMTime(value: 1001, timescale: 24000)
        static let ntsc29_97 = CMTime(value: 1001, timescale: 30000)
        static let film24 = CMTime(value: 1, timescale: 24)
        static let pal25 = CMTime(value: 1, timescale: 25)
        static let ntsc30 = CMTime(value: 1, timescale: 30)
    }
    
    @Published var currentTint: Double = 0.0 // Range: -150 to +150
    private let tintRange = (-150.0...150.0)
    
    private var videoDeviceInput: AVCaptureDeviceInput?
    
    @Published var isAutoExposureEnabled: Bool = true {
        didSet {
            exposureService.setAutoExposureEnabled(isAutoExposureEnabled)
        }
    }
    
    @Published var lutManager = LUTManager()
    private var ciContext = CIContext()
    
    // Add flag to lock orientation updates during recording
    private var recordingOrientationLocked = false
    
    // Save the original rotation values to restore them after recording
    private var originalRotationValues: [AVCaptureConnection: CGFloat] = [:]
    
    @Published var currentLens: CameraLens = .wide
    @Published var availableLenses: [CameraLens] = []
    
    @Published var currentZoomFactor: CGFloat = 1.0
    private var lastZoomFactor: CGFloat = 1.0
    
    // HEVC Hardware Encoding Properties
    private var compressionSession: VTCompressionSession?
    private let encoderQueue = DispatchQueue(label: "com.camera.encoder", qos: .userInteractive)
    
    // Add constants that are missing from VideoToolbox
    private enum VTConstants {
        static let hardwareAcceleratorOnly = "EnableHardwareAcceleratedVideoEncoder" as CFString
        static let priority = "Priority" as CFString
        static let priorityRealtimePreview = "RealtimePreview" as CFString
        
        // Color space constants for HEVC
        static let primariesITUR709 = "ITU_R_709_2"
        static let primariesBT2020 = "ITU_R_2020"
        static let yCbCrMatrix2020 = "ITU_R_2020"
        static let yCbCrMatrixITUR709 = "ITU_R_709_2"
        
        // HEVC Profile constants
        static let hevcMain422_10Profile = "HEVC_Main42210_AutoLevel"
    }
    
    private var encoderSpecification: [CFString: Any] {
        [
            kVTVideoEncoderSpecification_EnableHardwareAcceleratedVideoEncoder: true,
            kVTVideoEncoderSpecification_RequireHardwareAcceleratedVideoEncoder: true,
            kVTVideoEncoderSpecification_EnableLowLatencyRateControl: true
        ]
    }
    
    // Store recording orientation
    private var recordingOrientation: CGFloat?
    
    // Service Instances
    private var cameraSetupService: CameraSetupService!
    private var exposureService: ExposureService!
    private var recordingService: RecordingService!
    private var cameraDeviceService: CameraDeviceService!
    private var videoFormatService: VideoFormatService!
    
    override init() {
        super.init()
        print("\n=== Camera Initialization ===")
        
        // Initialize services
        setupServices()
        
        // Add observer for flashlight settings changes
        settingsObserver = NotificationCenter.default.addObserver(
            forName: .flashlightSettingChanged,
            object: nil,
            queue: .main
        ) { [weak self] _ in
            guard let self = self else { return }
            let settings = SettingsModel()
            if self.isRecording && settings.isFlashlightEnabled {
                self.flashlightManager.isEnabled = true
                self.flashlightManager.intensity = settings.flashlightIntensity
            } else {
                self.flashlightManager.isEnabled = false
            }
        }
        
        do {
            try cameraSetupService.setupSession()
            
            if let device = device {
                print("üìä Device Capabilities:")
                print("- Name: \(device.localizedName)")
                print("- Model ID: \(device.modelID)")
                
                isAppleLogSupported = device.formats.contains { format in
                    format.supportedColorSpaces.contains(.appleLog)
                }
                print("\n‚úÖ Apple Log Support: \(isAppleLogSupported)")
            }
            print("=== End Initialization ===\n")
            
            if let device = device {
                defaultFormat = device.activeFormat
            }
        } catch {
            self.error = .setupFailed
            print("Failed to setup session: \(error)")
        }
        
        // Add orientation change observer
        orientationObserver = NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { [weak self] _ in
                guard let self = self,
                      let videoConnection = self.session.outputs.first?.connection(with: .video) else { return }

                self.cameraDeviceService.updateVideoOrientation(for: videoConnection, orientation: self.currentInterfaceOrientation)
        }
        
        // Set initial shutter angle
        updateShutterAngle(180.0)
        
        print("üì± LUT Loading: No default LUTs will be loaded")
    }
    
    deinit {
        if let observer = orientationObserver {
            NotificationCenter.default.removeObserver(observer)
        }
        
        if let observer = settingsObserver {
            NotificationCenter.default.removeObserver(observer)
        }
        
        flashlightManager.cleanup()
    }
    
    private func setupServices() {
        // Initialize services with self as delegate
        cameraSetupService = CameraSetupService(session: session, delegate: self)
        exposureService = ExposureService(delegate: self)
        recordingService = RecordingService(session: session, delegate: self)
        cameraDeviceService = CameraDeviceService(session: session, delegate: self)
        videoFormatService = VideoFormatService(session: session, delegate: self)
    }
    
    func updateWhiteBalance(_ temperature: Float) {
        exposureService.updateWhiteBalance(temperature)
    }
    
    func updateISO(_ iso: Float) {
        exposureService.updateISO(iso)
    }
    
    func updateShutterSpeed(_ speed: CMTime) {
        exposureService.updateShutterSpeed(speed)
    }
    
    func updateShutterAngle(_ angle: Double) {
        exposureService.updateShutterAngle(angle, frameRate: selectedFrameRate)
    }
    
    func updateFrameRate(_ fps: Double) {
        do {
            try videoFormatService.updateFrameRate(fps)
        } catch {
            print("‚ùå Frame rate error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    func updateTint(_ newValue: Double) {
        currentTint = newValue.clamped(to: tintRange)
        exposureService.updateTint(currentTint, currentWhiteBalance: whiteBalance)
    }
    
    func updateOrientation(_ orientation: UIInterfaceOrientation) {
        self.currentInterfaceOrientation = orientation
        
        if let videoConnection = session.outputs.first?.connection(with: .video) {
            cameraDeviceService.updateVideoOrientation(for: videoConnection, orientation: orientation)
        }
        
        print("DEBUG: UI Interface orientation updated to: \(orientation.rawValue)")
    }
    
    func switchToLens(_ lens: CameraLens) {
        cameraDeviceService.switchToLens(lens)
    }
    
    func setZoomFactor(_ factor: CGFloat) {
        cameraDeviceService.setZoomFactor(factor, currentLens: currentLens, availableLenses: availableLenses)
    }
    
    @MainActor
    func startRecording() async {
        guard !isRecording else { return }
        
        // Update configuration for recording
        recordingService.setDevice(device)
        recordingService.setLUTManager(lutManager)
        recordingService.setAppleLogEnabled(isAppleLogEnabled)
        recordingService.setVideoConfiguration(
            frameRate: selectedFrameRate,
            resolution: selectedResolution,
            codec: selectedCodec
        )
        
        // Lock orientation during recording
        cameraDeviceService.lockOrientationForRecording(true)
        
        // Get current orientation for recording
        let recordingOrientation = session.outputs.first?.connection(with: .video)?.videoRotationAngle ?? 0
        
        // Start recording
        await recordingService.startRecording(orientation: recordingOrientation)
        
        isRecording = true
    }
    
    @MainActor
    func stopRecording() async {
        guard isRecording else { return }
        
        // Stop recording
        await recordingService.stopRecording()
        
        // Unlock orientation after recording
        cameraDeviceService.lockOrientationForRecording(false)
        
        isRecording = false
    }
    
    private func updateVideoConfiguration() {
        // Update recording service with new codec
        recordingService.setVideoConfiguration(
            frameRate: selectedFrameRate,
            resolution: selectedResolution,
            codec: selectedCodec
        )
        
        print("\n=== Updating Video Configuration ===")
        print("üé¨ Selected Codec: \(selectedCodec.rawValue)")
        print("üé® Apple Log Enabled: \(isAppleLogEnabled)")
        
        if selectedCodec == .proRes {
            print("‚úÖ Configured for ProRes recording")
        } else {
            print("‚úÖ Configured for HEVC recording")
            print("üìä Bitrate: \(selectedCodec.bitrate / 1_000_000) Mbps")
        }
        
        print("=== End Video Configuration ===\n")
    }
    
    // MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
    
    // Track frame counts for logging
    private var videoFrameCount = 0
    private var audioFrameCount = 0
    private var successfulVideoFrames = 0
    private var failedVideoFrames = 0
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let assetWriter = assetWriter,
              assetWriter.status == .writing else {
            return
        }
        
        // Handle video data
        if output == videoDataOutput,
           let assetWriterInput = assetWriterInput,
           assetWriterInput.isReadyForMoreMediaData {
            
            videoFrameCount += 1
            
            // Log every 30 frames to avoid flooding
            let shouldLog = videoFrameCount % 30 == 0
            if shouldLog {
                print("üìΩÔ∏è Processing video frame #\(videoFrameCount), writer status: \(assetWriter.status.rawValue)")
            }
            
            if let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) {
                if let lutFilter = tempLUTFilter ?? lutManager.currentLUTFilter {
                    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
                    if let processedImage = applyLUT(to: ciImage, using: lutFilter),
                       let processedPixelBuffer = createPixelBuffer(from: processedImage, with: pixelBuffer) {
                        
                        // Use original timing information
                        var timing = CMSampleTimingInfo()
                        CMSampleBufferGetSampleTimingInfo(sampleBuffer, at: 0, timingInfoOut: &timing)
                        
                        // Create format description for processed buffer
                        var info: CMFormatDescription?
                        let status = CMVideoFormatDescriptionCreateForImageBuffer(
                            allocator: kCFAllocatorDefault,
                            imageBuffer: processedPixelBuffer,
                            formatDescriptionOut: &info
                        )
                        
                        if status == noErr, let info = info,
                           let newSampleBuffer = createSampleBuffer(
                            from: processedPixelBuffer,
                            formatDescription: info,
                            timing: &timing
                           ) {
                            assetWriterInput.append(newSampleBuffer)
                            successfulVideoFrames += 1
                            if shouldLog {
                                print("‚úÖ Successfully appended processed frame #\(successfulVideoFrames)")
                            }
                        } else {
                            failedVideoFrames += 1
                            print("‚ö†Ô∏è Failed to create format description for processed frame #\(videoFrameCount), status: \(status)")
                        }
                    }
                } else {
                    // No LUT processing needed - use original sample buffer directly
                    assetWriterInput.append(sampleBuffer)
                    successfulVideoFrames += 1
                    if shouldLog {
                        print("‚úÖ Successfully appended original frame #\(successfulVideoFrames)")
                    }
                }
            }
        }
        
        // Handle audio data
        if output == audioDataOutput,
           let audioInput = assetWriter.inputs.first(where: { $0.mediaType == .audio }),
           audioInput.isReadyForMoreMediaData {
            audioFrameCount += 1
            audioInput.append(sampleBuffer)
            if audioFrameCount % 100 == 0 {
                print("üéµ Processed audio frame #\(audioFrameCount)")
            }
        }
    }
    
    private func createPixelBuffer(from ciImage: CIImage, with template: CVPixelBuffer) -> CVPixelBuffer? {
        var newPixelBuffer: CVPixelBuffer?
        CVPixelBufferCreate(kCFAllocatorDefault,
                           CVPixelBufferGetWidth(template),
                           CVPixelBufferGetHeight(template),
                           CVPixelBufferGetPixelFormatType(template),
                           [kCVPixelBufferIOSurfacePropertiesKey as String: [:]] as CFDictionary,
                           &newPixelBuffer)
        
        guard let outputBuffer = newPixelBuffer else { 
            print("‚ö†Ô∏è Failed to create pixel buffer from CI image")
            return nil 
        }
        
        ciContext.render(ciImage, to: outputBuffer)
        return outputBuffer
    }

    // Add helper property for tracking keyframes
    private var lastKeyFrameTime: CMTime?

    // Add helper method for creating sample buffers
    private func createSampleBuffer(
        from pixelBuffer: CVPixelBuffer,
        formatDescription: CMFormatDescription,
        timing: UnsafeMutablePointer<CMSampleTimingInfo>
    ) -> CMSampleBuffer? {
        var sampleBuffer: CMSampleBuffer?
        let status = CMSampleBufferCreateForImageBuffer(
            allocator: kCFAllocatorDefault,
            imageBuffer: pixelBuffer,
            dataReady: true,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: formatDescription,
            sampleTiming: timing,
            sampleBufferOut: &sampleBuffer
        )
        
        if status != noErr {
            print("‚ö†Ô∏è Failed to create sample buffer: \(status)")
            return nil
        }
        
        return sampleBuffer
    }

    private func getVideoTransform(for orientation: UIInterfaceOrientation) -> CGAffineTransform {
        switch orientation {
        case .portrait:
            return CGAffineTransform(rotationAngle: .pi/2) // 90 degrees clockwise
        case .portraitUpsideDown:
            return CGAffineTransform(rotationAngle: -.pi/2) // 90 degrees counterclockwise
        case .landscapeLeft: // USB on right
            return CGAffineTransform(rotationAngle: .pi) // 180 degrees (was .identity)
        case .landscapeRight: // USB on left
            return .identity // No rotation (was .pi)
        default:
            return .identity
        }
    }
}

private extension Double {
    func clamped(to range: ClosedRange<Double>) -> Double {
        return min(max(self, range.lowerBound), range.upperBound)
    }
}

extension CameraError {
    static func configurationFailed(message: String = "Camera configuration failed") -> CameraError {
        return .custom(message: message)
    }
}

// MARK: - CameraSetupServiceDelegate

extension CameraViewModel: CameraSetupServiceDelegate {
    func didUpdateSessionStatus(_ status: Status) {
        DispatchQueue.main.async {
            self.status = status
        }
    }
    
    func didEncounterError(_ error: CameraError) {
        DispatchQueue.main.async {
            self.error = error
        }
    }
    
    func didInitializeCamera(device: AVCaptureDevice) {
        self.device = device
        exposureService.setDevice(device)
        recordingService.setDevice(device)
        cameraDeviceService.setDevice(device)
        videoFormatService.setDevice(device)
        
        // Initialize available lenses
        availableLenses = CameraLens.availableLenses()
    }
    
    func didStartRunning(_ isRunning: Bool) {
        DispatchQueue.main.async {
            self.isSessionRunning = isRunning
        }
    }
}

// MARK: - ExposureServiceDelegate

extension CameraViewModel: ExposureServiceDelegate {
    func didUpdateWhiteBalance(_ temperature: Float) {
        DispatchQueue.main.async {
            self.whiteBalance = temperature
        }
    }
    
    func didUpdateISO(_ iso: Float) {
        DispatchQueue.main.async {
            self.iso = iso
        }
    }
    
    func didUpdateShutterSpeed(_ speed: CMTime) {
        DispatchQueue.main.async {
            self.shutterSpeed = speed
        }
    }
}

// MARK: - RecordingServiceDelegate

extension CameraViewModel: RecordingServiceDelegate {
    func didStartRecording() {
        // Handled by the isRecording property
    }
    
    func didStopRecording() {
        // Handled by the isRecording property
    }
    
    func didFinishSavingVideo(thumbnail: UIImage?) {
        DispatchQueue.main.async {
            self.recordingFinished = true
            self.lastRecordedVideoThumbnail = thumbnail
        }
    }
    
    func didUpdateProcessingState(_ isProcessing: Bool) {
        DispatchQueue.main.async {
            self.isProcessingRecording = isProcessing
        }
    }
}

// MARK: - CameraDeviceServiceDelegate

extension CameraViewModel: CameraDeviceServiceDelegate {
    func didUpdateCurrentLens(_ lens: CameraLens) {
        DispatchQueue.main.async {
            self.currentLens = lens
        }
    }
    
    func didUpdateZoomFactor(_ factor: CGFloat) {
        DispatchQueue.main.async {
            self.currentZoomFactor = factor
        }
    }
}

// MARK: - VideoFormatServiceDelegate

extension CameraViewModel: VideoFormatServiceDelegate {
    func didUpdateFrameRate(_ frameRate: Double) {
        DispatchQueue.main.async {
            self.selectedFrameRate = frameRate
        }
    }
}

================
File: camera/Features/Camera/FlashlightManager.swift
================
import AVFoundation
import Foundation

class FlashlightManager: ObservableObject {
    @Published private(set) var isAvailable: Bool = false
    @Published var isEnabled: Bool = false {
        didSet {
            if oldValue != isEnabled {
                setTorchState()
            }
        }
    }
    @Published var intensity: Float = 1.0 {
        didSet {
            if isEnabled {
                setTorchState()
            }
        }
    }
    
    private var device: AVCaptureDevice? {
        AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back)
    }
    
    init() {
        checkAvailability()
    }
    
    private func checkAvailability() {
        guard let device = device else {
            isAvailable = false
            return
        }
        isAvailable = device.hasTorch && device.isTorchAvailable
    }
    
    private func setTorchState() {
        guard let device = device,
              device.hasTorch,
              device.isTorchAvailable else {
            return
        }
        
        do {
            try device.lockForConfiguration()
            
            if isEnabled {
                // Clamp intensity to a minimum of 0.001 (0.1%) for ultra-low light
                let clampedIntensity = max(0.001, min(1.0, intensity))
                
                // Configure torch with intensity
                try device.setTorchModeOn(level: clampedIntensity)
            } else {
                device.torchMode = .off
            }
            
            device.unlockForConfiguration()
        } catch {
            print("Error setting torch state: \(error.localizedDescription)")
        }
    }
    
    func performStartupSequence() async {
        // Store the user's preferred intensity
        let userIntensity = intensity
        
        // Flash sequence: 3-2-1 (faster)
        for count in (1...3).reversed() {
            // Calculate flash intensity based on count (3=30%, 2=60%, 1=90% of user's setting)
            let countIntensity = userIntensity * Float(count) / 3.0
            await flash(count: count, flashIntensity: countIntensity)
            try? await Task.sleep(nanoseconds: 100_000_000) // 0.1 second delay between flashes
        }
        
        // Set to normal recording state with user's preferred intensity
        await MainActor.run {
            self.intensity = userIntensity
            self.isEnabled = true
        }
    }
    
    private func flash(count: Int, flashIntensity: Float) async {
        await MainActor.run {
            self.intensity = flashIntensity
            self.isEnabled = true
        }
        
        try? await Task.sleep(nanoseconds: 50_000_000) // 0.05 second flash
        
        await MainActor.run {
            self.isEnabled = false
        }
    }
    
    func cleanup() {
        isEnabled = false
    }
    
    func turnOffForSettingsExit() {
        isEnabled = false
    }
}

================
File: camera/Features/LUT/Utils/LUTProcessor.swift
================
import CoreImage
import CoreVideo
import UIKit

/// Processes camera frames with LUT filters
class LUTProcessor {
    private var lutFilter: CIFilter?
    private var isLogEnabled: Bool = false
    private let ciContext = CIContext(options: [.cacheIntermediates: false])
    
    /// Sets the LUT filter to use for processing
    /// - Parameter filter: The CIFilter to apply (typically CIColorCube)
    func setLUTFilter(_ filter: CIFilter?) {
        self.lutFilter = filter
    }
    
    /// Sets whether LOG mode is enabled
    /// - Parameter enabled: True if LOG mode is enabled
    func setLogEnabled(_ enabled: Bool) {
        self.isLogEnabled = enabled
    }
    
    /// Processes an image with the current LUT filter
    /// - Parameter image: The input CIImage
    /// - Returns: The processed CIImage, or nil if no processing was done
    func processImage(_ image: CIImage) -> CIImage? {
        guard let filter = lutFilter else { return nil }
        
        // Apply the LUT filter
        filter.setValue(image, forKey: kCIInputImageKey)
        var outputImage = filter.outputImage
        
        // Apply additional processing for LOG mode if enabled
        if isLogEnabled, let output = outputImage {
            outputImage = output.applyingFilter("CIColorControls", parameters: [
                kCIInputContrastKey: 1.1,
                kCIInputBrightnessKey: 0.05
            ])
        }
        
        return outputImage
    }
    
    /// Creates a CGImage from a CIImage for display
    /// - Parameter image: The CIImage to convert
    /// - Returns: A CGImage, or nil if conversion failed
    func createCGImage(from image: CIImage) -> CGImage? {
        return ciContext.createCGImage(image, from: image.extent)
    }
}

================
File: camera/Features/LUT/Views/LUTVideoPreviewView.swift
================
import SwiftUI
import AVFoundation
import CoreImage

struct LUTVideoPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    @ObservedObject var lutManager: LUTManager
    let viewModel: CameraViewModel
    
    func makeUIView(context: Context) -> LUTPreviewView {
        // Lock the orientation to portrait
        CameraOrientationLock.lockToPortrait()
        
        // Create the preview view
        let previewView = LUTPreviewView()
        
        // Connect the session - this will use our backing layer approach
        previewView.setSession(session)
        
        // Set up video data output for LUT processing
        let videoOutput = AVCaptureVideoDataOutput()
        videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        videoOutput.setSampleBufferDelegate(context.coordinator, queue: DispatchQueue(label: "videoQueue"))
        
        if session.canAddOutput(videoOutput) {
            session.addOutput(videoOutput)
            
            // Set fixed orientation for video connection
            if let connection = videoOutput.connection(with: .video) {
                // Set rotation angle to 90 degrees (portrait)
                if connection.isVideoRotationAngleSupported(90) {
                    connection.videoRotationAngle = 90
                    print("üì± Video connection fixed to portrait orientation (90¬∞)")
                }
            }
        }
        
        // Store references in coordinator
        context.coordinator.previewView = previewView
        context.coordinator.session = session
        
        // Register for orientation change notifications to ensure preview stays fixed
        NotificationCenter.default.addObserver(
            context.coordinator,
            selector: #selector(context.coordinator.deviceOrientationDidChange),
            name: UIDevice.orientationDidChangeNotification,
            object: nil
        )
        
        // Register for orientation will change notification to prevent position shifts
        NotificationCenter.default.addObserver(
            previewView,
            selector: #selector(previewView.orientationWillChange),
            name: UIScene.willEnterForegroundNotification,
            object: nil
        )
        
        return previewView
    }
    
    func updateUIView(_ uiView: LUTPreviewView, context: Context) {
        // Update LUT filter in processor if it changed
        context.coordinator.lutProcessor.setLUTFilter(lutManager.currentLUTFilter)
        
        // Update LOG mode in processor if it changed
        context.coordinator.lutProcessor.setLogEnabled(viewModel.isAppleLogEnabled)
        
        // Update the preview view's LUT status
        uiView.isLUTEnabled = lutManager.currentLUTFilter != nil
        
        // Ensure preview layer orientation is fixed
        uiView.ensureFixedOrientation()
    }
    
    static func dismantleUIView(_ uiView: LUTPreviewView, coordinator: Coordinator) {
        NotificationCenter.default.removeObserver(coordinator)
        NotificationCenter.default.removeObserver(uiView)
        
        // Maintain portrait orientation when view is dismantled
        CameraOrientationLock.lockToPortrait()
    }
    
    func makeCoordinator() -> Coordinator {
        Coordinator(parent: self)
    }
    
    class Coordinator: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
        let parent: LUTVideoPreviewView
        var session: AVCaptureSession?
        weak var previewView: LUTPreviewView?
        let lutProcessor = LUTProcessor()
        
        // Track the current device orientation to detect landscape transitions
        private var lastDeviceOrientation: UIDeviceOrientation = .portrait
        
        init(parent: LUTVideoPreviewView) {
            self.parent = parent
            super.init()
            
            // Initialize the LUT processor with the current LUT filter
            lutProcessor.setLUTFilter(parent.lutManager.currentLUTFilter)
            lutProcessor.setLogEnabled(parent.viewModel.isAppleLogEnabled)
        }
        
        deinit {
            NotificationCenter.default.removeObserver(self)
        }
        
        @objc func deviceOrientationDidChange() {
            let currentOrientation = UIDevice.current.orientation
            print("üîÑ Device orientation changed to: \(currentOrientation.rawValue)")
            
            // Ensure the preview stays fixed in portrait
            DispatchQueue.main.async { [weak self] in
                // Re-lock to portrait orientation
                CameraOrientationLock.lockToPortrait()
                
                // Ensure connections are using the correct rotation angle
                if let connection = self?.session?.outputs.first as? AVCaptureVideoDataOutput,
                   let videoConnection = connection.connection(with: .video),
                   videoConnection.isVideoRotationAngleSupported(90) {
                    videoConnection.videoRotationAngle = 90  // 90¬∞ = portrait
                }
                
                // Ensure the preview layer orientation is fixed
                self?.previewView?.ensureFixedOrientation()
            }
        }
        
        func captureOutput(_ output: AVCaptureOutput,
                           didOutput sampleBuffer: CMSampleBuffer,
                           from connection: AVCaptureConnection) {
            // Ensure connection orientation stays fixed to 90 degrees (portrait)
            if connection.isVideoRotationAngleSupported(90) && connection.videoRotationAngle != 90 {
                connection.videoRotationAngle = 90
                print("üì± Reset video connection to portrait (90¬∞) in captureOutput")
            }
            
            // Only process if LUT is enabled
            if parent.lutManager.currentLUTFilter != nil {
                // Get the pixel buffer from the sample buffer
                guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
                    print("‚ùå Failed to get pixel buffer from sample buffer")
                    return
                }
                
                // Create a CIImage from the pixel buffer
                let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
                
                // Process the image with the LUT
                if let processedImage = lutProcessor.processImage(ciImage),
                   let cgImage = lutProcessor.createCGImage(from: processedImage) {
                    // Update the preview view with the processed frame
                    DispatchQueue.main.async { [weak self] in
                        self?.previewView?.displayProcessedImage(cgImage)
                    }
                }
            } else {
                // If no LUT is enabled, hide the processed view layer
                DispatchQueue.main.async { [weak self] in
                    self?.previewView?.hideProcessedLayer()
                }
            }
        }
    }
}

/// A UIView subclass that uses AVCaptureVideoPreviewLayer as its backing layer
class LUTPreviewView: UIView {
    private var processedLayer: CALayer?
    private var isHandlingRotation = false
    
    var isLUTEnabled: Bool = false {
        didSet {
            if isLUTEnabled != oldValue {
                if isLUTEnabled {
                    // Show the processed layer and hide the preview layer
                    processedLayer?.isHidden = false
                    layer.isHidden = true
                } else {
                    // Show the preview layer and hide the processed layer
                    processedLayer?.isHidden = true
                    layer.isHidden = false
                }
            }
        }
    }
    
    // Use the preview layer as the view's backing layer
    override class var layerClass: AnyClass {
        AVCaptureVideoPreviewLayer.self
    }
    
    // Access the layer as an AVCaptureVideoPreviewLayer
    var previewLayer: AVCaptureVideoPreviewLayer {
        return layer as! AVCaptureVideoPreviewLayer
    }
    
    // Disable autoresizing to prevent automatic frame changes during rotation
    override var translatesAutoresizingMaskIntoConstraints: Bool {
        get { return false }
        set { /* no-op to prevent changes */ }
    }
    
    // Override transform to prevent any rotation
    override var transform: CGAffineTransform {
        get { return .identity }
        set { /* no-op to prevent changes */ }
    }
    
    override init(frame: CGRect) {
        super.init(frame: frame)
        setupLayers()
        
        // Disable automatic transformations during rotation
        autoresizingMask = []
    }
    
    required init?(coder: NSCoder) {
        super.init(coder: coder)
        setupLayers()
        
        // Disable automatic transformations during rotation
        autoresizingMask = []
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
    }
    
    private func setupLayers() {
        // Disable implicit animations to prevent unwanted transitions
        CATransaction.begin()
        CATransaction.setDisableActions(true)
        
        // Configure the preview layer (which is our backing layer)
        previewLayer.videoGravity = .resizeAspectFill
        
        // Create a layer for displaying the processed frames
        let processedCALayer = CALayer()
        processedCALayer.frame = bounds
        processedCALayer.contentsGravity = .resizeAspectFill
        processedCALayer.isHidden = true // Initially hidden
        layer.addSublayer(processedCALayer)
        processedLayer = processedCALayer
        
        CATransaction.commit()
        
        // Ensure orientation is fixed
        ensureFixedOrientation()
    }
    
    func setSession(_ session: AVCaptureSession) {
        // Set the session on the preview layer
        previewLayer.session = session
        
        // Ensure orientation is fixed
        ensureFixedOrientation()
    }
    
    // Called just before orientation changes
    @objc func orientationWillChange(_ notification: Notification) {
        isHandlingRotation = true
        // Lock everything in place
        lockViewDuringRotation()
    }
    
    private func lockViewDuringRotation() {
        // Disable animations
        CATransaction.begin()
        CATransaction.setDisableActions(true)
        CATransaction.setAnimationDuration(0)
        
        // Reset any transforms
        previewLayer.transform = CATransform3DIdentity
        processedLayer?.transform = CATransform3DIdentity
        
        // Force our own positioning
        if let superview = superview {
            frame = CGRect(x: 0, y: 0, width: superview.bounds.width, height: superview.bounds.height)
        }
        
        // Ensure processed layer stays fixed
        if let processedLayer = processedLayer {
            processedLayer.frame = bounds
        }
        
        CATransaction.commit()
        
        // Re-apply orientation fix
        ensureFixedOrientation()
        
        // Re-enable normal handling after a delay
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) { [weak self] in
            self?.isHandlingRotation = false
        }
    }
    
    func ensureFixedOrientation() {
        // Ensure the preview layer's orientation is fixed to portrait (90 degrees)
        CATransaction.begin()
        CATransaction.setDisableActions(true)
        
        if let connection = previewLayer.connection {
            if connection.isVideoRotationAngleSupported(90) {
                let currentAngle = connection.videoRotationAngle
                if currentAngle != 90 {
                    connection.videoRotationAngle = 90
                    print("üîÑ Preview layer orientation corrected from \(currentAngle)¬∞ to 90¬∞")
                } else {
                    print("‚úÖ Preview layer already in portrait orientation (90¬∞)")
                }
            } else {
                print("‚ö†Ô∏è Video rotation angle not supported on preview layer connection")
            }
        }
        
        // Ensure the layer's transform is identity to prevent any rotation
        previewLayer.transform = CATransform3DIdentity
        processedLayer?.transform = CATransform3DIdentity
        
        // Apply additional transforms to counteract any rotation effects
        if UIDevice.current.orientation == .portraitUpsideDown {
            // For upside-down orientation, we need to ensure the view doesn't flip
            print("üì± Applying additional fixes for upside-down orientation")
            
            // Force our own positioning to stay centered in the superview
            if let superview = superview {
                center = CGPoint(x: superview.bounds.width/2, y: superview.bounds.height/2)
            }
        }
        
        CATransaction.commit()
    }
    
    func displayProcessedImage(_ image: CGImage) {
        // Disable implicit animations when updating content
        CATransaction.begin()
        CATransaction.setDisableActions(true)
        
        processedLayer?.contents = image
        
        CATransaction.commit()
    }
    
    func hideProcessedLayer() {
        // Disable implicit animations
        CATransaction.begin()
        CATransaction.setDisableActions(true)
        
        processedLayer?.isHidden = true
        layer.isHidden = false
        
        CATransaction.commit()
        
        // Ensure orientation is fixed when switching back to preview layer
        ensureFixedOrientation()
    }
    
    override func layoutSubviews() {
        super.layoutSubviews()
        
        if !isHandlingRotation {
            // Completely disable any layout changes during rotation
            CATransaction.begin()
            CATransaction.setDisableActions(true)
            CATransaction.setAnimationDuration(0)
            
            processedLayer?.frame = bounds
            
            // Ensure the layer's transform is identity to prevent any rotation
            previewLayer.transform = CATransform3DIdentity
            processedLayer?.transform = CATransform3DIdentity
            
            CATransaction.commit()
            
            // Ensure orientation stays fixed after layout changes
            ensureFixedOrientation()
        }
    }
    
    // Override to prevent automatic transforms during rotation
    override func didMoveToSuperview() {
        super.didMoveToSuperview()
        ensureFixedOrientation()
    }
    
    // Override to prevent bounds changes from affecting layout
    override var bounds: CGRect {
        didSet {
            if !isHandlingRotation && bounds != oldValue {
                ensureFixedOrientation()
            }
        }
    }
    
    // Override to prevent frame changes from affecting layout
    override var frame: CGRect {
        didSet {
            if !isHandlingRotation && frame != oldValue {
                ensureFixedOrientation()
            }
        }
    }
}

================
File: camera/Features/LUT/CubeLUTLoader.swift
================
//
//  CubeLUTLoader.swift
//  camera
//
//  Created by spencer on 2024-12-30.
//

import Foundation
import CoreImage

/// Loads a .cube file and returns data for CIColorCube filter
class CubeLUTLoader {
    
    /// Parse .cube file into Float array and return dimension + LUT data
    static func loadCubeFile(name: String) throws -> (dimension: Int, data: [Float]) {
        guard let filePath = Bundle.main.url(forResource: name, withExtension: "cube") else {
            print("‚ùå CubeLUTLoader: LUT file '\(name).cube' not found in bundle")
            throw NSError(domain: "CubeLUTLoader", code: 1, userInfo: [NSLocalizedDescriptionKey: "LUT file not found"])
        }
        return try loadCubeFile(from: filePath)
    }
    
    /// Parse .cube file into Float array and return dimension + LUT data
    static func loadCubeFile(from url: URL) throws -> (dimension: Int, data: [Float]) {
        print("\nüîÑ CubeLUTLoader: Loading LUT from \(url.path)")
        
        // First check if URL can be accessed (file might be security-scoped)
        if !FileManager.default.fileExists(atPath: url.path) {
            print("‚ùå CubeLUTLoader: File doesn't exist at path: \(url.path)")
            throw NSError(domain: "CubeLUTLoader", code: 5, userInfo: [
                NSLocalizedDescriptionKey: "LUT file not found at specified path"
            ])
        }
        
        // Start accessing security-scoped resource if needed
        var didStartAccess = false
        if url.startAccessingSecurityScopedResource() {
            didStartAccess = true
            print("‚úÖ CubeLUTLoader: Started accessing security-scoped resource")
        }
        
        // Make sure we stop accessing the resource when we're done
        defer {
            if didStartAccess {
                url.stopAccessingSecurityScopedResource()
                print("‚úÖ CubeLUTLoader: Stopped accessing security-scoped resource")
            }
        }
        
        // Validate file exists and is readable
        try validateLUTFile(at: url)
        
        var fileContents: String
        do {
            fileContents = try String(contentsOf: url, encoding: .utf8)
            print("‚úÖ Read \(fileContents.count) characters from LUT file")
            
            // Print first few lines of the file for debugging
            let previewLines = fileContents.components(separatedBy: .newlines).prefix(5).joined(separator: "\n")
            print("üìÉ LUT File Preview (first 5 lines):\n\(previewLines)")
        } catch {
            print("‚ö†Ô∏è Failed to read LUT file as UTF8 text: \(error.localizedDescription)")
            
            // Try other encodings
            for encoding in [String.Encoding.ascii, .isoLatin1, .isoLatin2, .macOSRoman] {
                do {
                    fileContents = try String(contentsOf: url, encoding: encoding)
                    print("‚úÖ Successfully read file using \(encoding) encoding")
                    break
                } catch {
                    // Continue trying other encodings
                }
            }
            
            // If we get here, try binary approach
            do {
                print("‚ö†Ô∏è Trying binary approach for LUT file")
                return try loadBinaryCubeFile(from: url)
            } catch let binaryError {
                print("‚ùå Binary loading also failed: \(binaryError.localizedDescription)")
                throw NSError(domain: "CubeLUTLoader", code: 3, userInfo: [
                    NSLocalizedDescriptionKey: "The LUT file could not be read with any known encoding. Try a standard plain text .cube format.",
                    NSUnderlyingErrorKey: error
                ])
            }
        }
        
        let lines = fileContents.components(separatedBy: .newlines)
        print("üìù Parsing \(lines.count) lines from LUT file")
        
        var dimension = 0
        var cubeData = [Float]()
        var foundSize = false
        var headerEnded = false
        var linesProcessed = 0
        var dataLinesProcessed = 0
        var headerLines: [String] = []
        
        for line in lines {
            linesProcessed += 1
            let trimmed = line.trimmingCharacters(in: .whitespacesAndNewlines)
            
            // Skip comments and empty lines
            if trimmed.hasPrefix("#") || trimmed.isEmpty {
                if !headerEnded && !trimmed.isEmpty {
                    headerLines.append(trimmed)
                }
                continue
            }
            
            // Process LUT size
            if trimmed.lowercased().contains("lut_3d_size") {
                let parts = trimmed.components(separatedBy: CharacterSet.whitespaces)
                if let sizeString = parts.last, let size = Int(sizeString) {
                    dimension = size
                    let totalCount = size * size * size * 3
                    cubeData.reserveCapacity(totalCount)
                    foundSize = true
                    print("‚úÖ Found LUT_3D_SIZE: \(size), expecting \(totalCount) data points")
                    headerLines.append(trimmed)
                } else {
                    print("‚ö†Ô∏è Found LUT_3D_SIZE but couldn't parse value: \(trimmed)")
                }
            } 
            // Process data lines
            else if dimension > 0 {
                if !headerEnded {
                    headerEnded = true
                    print("üìã LUT Header:\n\(headerLines.joined(separator: "\n"))")
                }
                
                // Parse RGB values on this line
                let components = trimmed.components(separatedBy: CharacterSet.whitespaces)
                    .filter { !$0.isEmpty }
                    .compactMap { Float($0) }
                
                if components.count == 3 {
                    cubeData.append(contentsOf: components)
                    dataLinesProcessed += 1
                    
                    // Print the first few data points for debugging
                    if dataLinesProcessed <= 3 {
                        print("üìä Data Line \(dataLinesProcessed): \(components)")
                    }
                } else if !trimmed.isEmpty {
                    print("‚ö†Ô∏è Line \(linesProcessed): Invalid data format - expected 3 values, got \(components.count): \(trimmed)")
                }
            }
        }
        
        let expectedDataLines = dimension * dimension * dimension
        print("üìä LUT Stats: Processed \(dataLinesProcessed) data lines out of expected \(expectedDataLines)")
        print("üìä Parsed \(cubeData.count) values, expected \(dimension * dimension * dimension * 3)")
        
        // If we failed to find the size or have no data, but this seems to be an actual LUT file,
        // try to infer a reasonable dimension and generate a basic identity LUT
        if (dimension == 0 || !foundSize || cubeData.isEmpty) && fileContents.contains("LUT") {
            print("‚ö†Ô∏è No valid LUT data found, but file appears to be a LUT. Creating fallback identity LUT")
            return createIdentityLUT(dimension: 32)
        }
        
        if dimension == 0 || !foundSize {
            print("‚ùå Missing LUT_3D_SIZE in file")
            throw NSError(domain: "CubeLUTLoader", code: 2, userInfo: [NSLocalizedDescriptionKey: "Invalid .cube file - Missing LUT_3D_SIZE"])
        }
        
        if cubeData.isEmpty {
            print("‚ùå No valid data found in LUT file")
            throw NSError(domain: "CubeLUTLoader", code: 2, userInfo: [NSLocalizedDescriptionKey: "Invalid or empty .cube file - No data found"])
        }
        
        // If we have at least some data but not the complete amount, warn but continue
        let expectedDataCount = dimension * dimension * dimension * 3
        if cubeData.count < expectedDataCount {
            print("‚ö†Ô∏è Incomplete LUT data: Found \(cubeData.count) values, expected \(expectedDataCount)")
            
            // If we're significantly short, throw an error
            if cubeData.count < expectedDataCount / 2 {
                print("‚ùå Insufficient LUT data: Found \(cubeData.count) values, expected \(expectedDataCount)")
                throw NSError(domain: "CubeLUTLoader", code: 4, userInfo: [
                    NSLocalizedDescriptionKey: "Incomplete LUT data: Only \(cubeData.count)/\(expectedDataCount) values found"
                ])
            }
            
            // For minor shortfalls, pad with zeros to maintain expected dimensions
            let shortfall = expectedDataCount - cubeData.count
            if shortfall > 0 && shortfall < expectedDataCount / 10 {  // Less than 10% missing
                print("‚ö†Ô∏è Padding \(shortfall) missing values with zeros")
                cubeData.append(contentsOf: Array(repeating: 0.0, count: shortfall))
            }
        }
        
        // Validate values are in reasonable range (0.0-1.0)
        let outOfRangeValues = cubeData.filter { $0 < 0.0 || $0 > 1.0 }
        if !outOfRangeValues.isEmpty {
            print("‚ö†Ô∏è Found \(outOfRangeValues.count) values outside the expected 0.0-1.0 range")
            print("‚ö†Ô∏è Example out-of-range values: \(outOfRangeValues.prefix(5))")
            
            // Clamp values to valid range
            for i in 0..<cubeData.count {
                cubeData[i] = max(0.0, min(1.0, cubeData[i]))
            }
            print("‚úÖ Clamped all values to 0.0-1.0 range for compatibility")
        }
        
        print("‚úÖ Successfully parsed LUT file with dimension \(dimension) and \(cubeData.count) values")
        return (dimension, cubeData)
    }
    
    /// Try to load a binary format LUT file
    private static func loadBinaryCubeFile(from url: URL) throws -> (dimension: Int, data: [Float]) {
        print("üîÑ Attempting to load binary LUT file")
        
        // Create a 32x32x32 identity LUT as fallback
        let dimension = 32
        
        // Try to read the binary data
        do {
            let data = try Data(contentsOf: url)
            print("üìä Read \(data.count) bytes from binary file")
            
            // Create the identity LUT with the inferred dimension
            return createIdentityLUT(dimension: dimension)
        } catch {
            print("‚ùå Failed to read binary data: \(error.localizedDescription)")
            throw error
        }
    }
    
    /// Creates an identity LUT (no color changes) with the specified dimension
    private static func createIdentityLUT(dimension: Int) -> (dimension: Int, data: [Float]) {
        print("üé® Creating identity LUT with dimension \(dimension)")
        
        var lutData: [Float] = []
        lutData.reserveCapacity(dimension * dimension * dimension * 3)
        
        for b in 0..<dimension {
            for g in 0..<dimension {
                for r in 0..<dimension {
                    let rf = Float(r) / Float(dimension - 1)
                    let gf = Float(g) / Float(dimension - 1)
                    let bf = Float(b) / Float(dimension - 1)
                    lutData.append(rf)
                    lutData.append(gf)
                    lutData.append(bf)
                }
            }
        }
        
        return (dimension, lutData)
    }
    
    /// Validates that a LUT file exists and is readable
    static func validateLUTFile(at url: URL) throws {
        print("üîç Validating LUT file at \(url.path)")
        
        // Check if the file exists
        let fileManager = FileManager.default
        guard fileManager.fileExists(atPath: url.path) else {
            print("‚ùå LUT file does not exist at \(url.path)")
            throw NSError(domain: "CubeLUTLoader", code: 5, userInfo: [
                NSLocalizedDescriptionKey: "LUT file does not exist at path"
            ])
        }
        
        // Check if file is readable
        do {
            let resourceValues = try url.resourceValues(forKeys: [.isReadableKey, .fileSizeKey, .contentTypeKey])
            guard resourceValues.isReadable == true else {
                print("‚ùå LUT file is not readable")
                throw NSError(domain: "CubeLUTLoader", code: 6, userInfo: [
                    NSLocalizedDescriptionKey: "LUT file is not readable"
                ])
            }
            
            if let fileSize = resourceValues.fileSize {
                print("‚úÖ LUT file size: \(fileSize) bytes")
                if fileSize == 0 {
                    print("‚ùå LUT file is empty (0 bytes)")
                    throw NSError(domain: "CubeLUTLoader", code: 7, userInfo: [
                        NSLocalizedDescriptionKey: "LUT file is empty"
                    ])
                }
            }
            
            if let contentType = resourceValues.contentType {
                print("‚úÖ LUT file content type: \(contentType.identifier)")
            }
        } catch {
            print("‚ö†Ô∏è Failed to get resource values: \(error.localizedDescription)")
            // Continue anyway, this isn't critical
        }
        
        // Attempt to check file permissions
        do {
            let attributes = try fileManager.attributesOfItem(atPath: url.path)
            if let permissions = attributes[.posixPermissions] as? NSNumber {
                print("‚úÖ LUT file permissions: \(permissions.intValue)")
            }
            
            if let fileSize = attributes[.size] as? NSNumber {
                print("‚úÖ LUT file size: \(fileSize.intValue) bytes")
            }
            
            if let fileType = attributes[.type] as? String {
                print("‚úÖ LUT file type: \(fileType)")
            }
        } catch {
            print("‚ö†Ô∏è Failed to get file attributes: \(error.localizedDescription)")
            // Continue anyway, this isn't critical
        }
        
        print("‚úÖ LUT file validation passed")
    }
}

================
File: camera/Features/LUT/LUTManager.swift
================
import SwiftUI
import CoreImage
import UniformTypeIdentifiers

class LUTManager: ObservableObject {
    
    @Published var currentLUTFilter: CIFilter?
    private var dimension: Int = 0
    @Published var selectedLUTURL: URL?
    @Published var recentLUTs: [String: URL]? = [:]
    
    // Maximum number of recent LUTs to store
    private let maxRecentLUTs = 5
    
    // UserDefaults key for storing recent LUTs
    private let recentLUTsKey = "recentLUTs"
    
    // New properties to store cube data
    private var cubeDimension: Int = 0
    private var cubeData: Data?
    
    // Computed property for the current LUT name
    var currentLUTName: String {
        selectedLUTURL?.lastPathComponent ?? "Custom LUT"
    }
    
    // Supported file types
    static let supportedTypes: [UTType] = [
        UTType(filenameExtension: "cube") ?? UTType.data,
        UTType(filenameExtension: "3dl") ?? UTType.data,
        UTType(filenameExtension: "lut") ?? UTType.data,
        UTType(filenameExtension: "look") ?? UTType.data,
        UTType.data // Fallback
    ]
    
    init() {
        loadRecentLUTs()
    }
    
    // MARK: - LUT Loading Methods
    
    /// Imports a LUT file from the given URL with completion handler
    /// - Parameters:
    ///   - url: The URL of the LUT file
    ///   - completion: Completion handler with success boolean
    func importLUT(from url: URL, completion: @escaping (Bool) -> Void) {
        print("\nüìä LUTManager: Attempting to import LUT from URL: \(url.path)")
        
        // First check if the file exists
        guard FileManager.default.fileExists(atPath: url.path) else {
            print("‚ùå LUTManager Error: File does not exist at path: \(url.path)")
            completion(false)
            return
        }
        
        // Get file information before processing
        do {
            let attributes = try FileManager.default.attributesOfItem(atPath: url.path)
            if let fileSize = attributes[.size] as? NSNumber {
                print("üìä LUTManager: Original file size: \(fileSize.intValue) bytes")
            }
        } catch {
            print("‚ö†Ô∏è LUTManager: Could not read file attributes: \(error.localizedDescription)")
        }
        
        // Create a secure bookmarked copy if needed (for files from iCloud or external sources)
        let documentsDirectory = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let destinationURL = documentsDirectory.appendingPathComponent("LUTs/\(url.lastPathComponent)")
        
        do {
            // Create LUTs directory if it doesn't exist
            let lutsDirectory = documentsDirectory.appendingPathComponent("LUTs")
            if !FileManager.default.fileExists(atPath: lutsDirectory.path) {
                try FileManager.default.createDirectory(at: lutsDirectory, withIntermediateDirectories: true)
                print("üìÅ LUTManager: Created LUTs directory at \(lutsDirectory.path)")
            }
            
            // Only copy if not already in our LUTs folder
            if url.path != destinationURL.path {
                // Remove existing file at destination if needed
                if FileManager.default.fileExists(atPath: destinationURL.path) {
                    try FileManager.default.removeItem(at: destinationURL)
                    print("üóëÔ∏è LUTManager: Removed existing file at destination")
                }
                
                // Copy the file to our safe location
                try FileManager.default.copyItem(at: url, to: destinationURL)
                print("‚úÖ LUTManager: Copied LUT to permanent storage: \(destinationURL.path)")
            } else {
                print("‚ÑπÔ∏è LUTManager: File is already in the correct location")
            }
            
            // Now load the LUT from the permanent location
            loadLUT(from: destinationURL)
            
            // Update successful
            DispatchQueue.main.async {
                self.selectedLUTURL = destinationURL
                completion(true)
            }
        } catch {
            print("‚ùå LUTManager Error: Failed to copy or load LUT: \(error.localizedDescription)")
            
            // Try to load directly from the original location as a fallback
            loadLUT(from: url)
            DispatchQueue.main.async {
                self.selectedLUTURL = url
                completion(true)
            }
        }
    }
    
    func loadLUT(named fileName: String) {
        print("üîç LUTManager: Attempting to load LUT file named '\(fileName)'")
        do {
            guard let fileURL = Bundle.main.url(forResource: fileName, withExtension: "cube") else {
                print("‚ùå LUTManager Error: File '\(fileName).cube' not found in bundle")
                print("üìÇ Bundle path: \(Bundle.main.bundlePath)")
                print("üìÇ Available resources: \(Bundle.main.paths(forResourcesOfType: "cube", inDirectory: nil))")
                throw NSError(domain: "LUTManager", code: 1, userInfo: [NSLocalizedDescriptionKey: "LUT file not found in bundle"])
            }
            
            print("‚úÖ LUT file found at: \(fileURL.path)")
            let lutInfo = try CubeLUTLoader.loadCubeFile(name: fileName)
            print("‚úÖ LUT data loaded: dimension=\(lutInfo.dimension), data.count=\(lutInfo.data.count)")
            setupLUTFilter(lutInfo: lutInfo)
            addToRecentLUTs(url: fileURL)
            print("‚úÖ LUT successfully loaded and configured")
        } catch let error {
            print("‚ùå LUTManager Error: Failed to load LUT '\(fileName)': \(error.localizedDescription)")
        }
    }
    
    func loadLUT(from url: URL) {
        print("\nüìä LUTManager: Attempting to load LUT from URL: \(url.path)")
        print("üìä LUTManager: URL is file URL: \(url.isFileURL)")
        
        // Verify file exists
        guard FileManager.default.fileExists(atPath: url.path) else {
            print("‚ùå LUTManager Error: File does not exist at path: \(url.path)")
            return
        }
        
        print("üìä LUTManager: File exists: true")
        
        do {
            let attributes = try FileManager.default.attributesOfItem(atPath: url.path)
            if let fileSize = attributes[.size] as? NSNumber {
                print("üìä LUTManager: File size: \(fileSize.intValue) bytes")
            }
            if let fileType = attributes[.type] as? String {
                print("üìä LUTManager: File type: \(fileType)")
            }
        } catch {
            print("‚ö†Ô∏è LUTManager Error: Could not read file attributes: \(error.localizedDescription)")
        }
        
        // First try to read the file content preview
        do {
            let handle = try FileHandle(forReadingFrom: url)
            defer { try? handle.close() }
            if let data = try handle.readToEnd(), let preview = String(data: data.prefix(100), encoding: .utf8) {
                print("üìä LUTManager: File content preview: \(preview.prefix(50))")
            } else {
                print("üìä LUTManager: Could not read file content preview (may be binary data)")
            }
        } catch {
            print("‚ö†Ô∏è LUTManager Error: Failed to read file content preview: \(error.localizedDescription)")
        }
        
        do {
            // Direct access to load the LUT data
            let lutInfo = try CubeLUTLoader.loadCubeFile(from: url)
            print("‚úÖ LUT data loaded from URL: dimension=\(lutInfo.dimension), data.count=\(lutInfo.data.count)")
            setupLUTFilter(lutInfo: lutInfo)
            addToRecentLUTs(url: url)
            DispatchQueue.main.async {
                self.selectedLUTURL = url
            }
            print("‚úÖ LUT successfully loaded and configured from URL")
        } catch {
            print("‚ùå LUTManager Error: Failed to load LUT from URL: \(error.localizedDescription)")
            
            // Try a fallback approach for binary LUT files
            if error.localizedDescription.contains("Invalid LUT format") || error.localizedDescription.contains("not properly formatted") {
                print("üîÑ Attempting fallback for binary LUT format...")
                tryLoadBinaryLUT(from: url)
            }
        }
    }
    
    // Attempt to load a binary format LUT as a fallback
    private func tryLoadBinaryLUT(from url: URL) {
        do {
            // Read the file as binary data
            let data = try Data(contentsOf: url)
            print("üìä Read \(data.count) bytes from binary LUT file")
            
            // Create a basic identity LUT (no color changes) as fallback
            let dimension = 32 // Standard dimension for basic LUTs
            var lutData = [Float]()
            
            // Generate a basic identity LUT
            for b in 0..<dimension {
                for g in 0..<dimension {
                    for r in 0..<dimension {
                        let rf = Float(r) / Float(dimension - 1)
                        let gf = Float(g) / Float(dimension - 1)
                        let bf = Float(b) / Float(dimension - 1)
                        lutData.append(rf)
                        lutData.append(gf)
                        lutData.append(bf)
                    }
                }
            }
            
            // Setup the fallback LUT
            setupLUTFilter(lutInfo: (dimension: dimension, data: lutData))
            print("‚ö†Ô∏è Created fallback identity LUT with dimension \(dimension)")
            DispatchQueue.main.async {
                self.selectedLUTURL = url
            }
            addToRecentLUTs(url: url)
        } catch {
            print("‚ùå Binary LUT fallback also failed: \(error.localizedDescription)")
        }
    }
    
    // Sets up the CIColorCube filter with the provided LUT information
    func setupLUTFilter(lutInfo: (dimension: Int, data: [Float])) {
        self.cubeDimension = lutInfo.dimension
        self.dimension = lutInfo.dimension
        let expectedCount = lutInfo.dimension * lutInfo.dimension * lutInfo.dimension * 4 // RGBA
        
        // Convert RGB data to RGBA (required by CIFilter)
        var rgbaData = [Float]()
        rgbaData.reserveCapacity(expectedCount)
        
        // Original data is in RGB format, we need to add alpha = 1.0 for each entry
        for i in stride(from: 0, to: lutInfo.data.count, by: 3) {
            if i + 2 < lutInfo.data.count {
                rgbaData.append(lutInfo.data[i])     // R
                rgbaData.append(lutInfo.data[i+1])   // G
                rgbaData.append(lutInfo.data[i+2])   // B
                rgbaData.append(1.0)                 // A (always 1.0)
            }
        }
        
        // Create a copy of the data and convert to NSData
        // Fix for the dangling buffer pointer issue
        var dataCopy = rgbaData
        let nsData = NSData(bytes: &dataCopy, length: dataCopy.count * MemoryLayout<Float>.size)
        self.cubeData = nsData as Data
        
        if let filter = CIFilter(name: "CIColorCube") {
            filter.setValue(lutInfo.dimension, forKey: "inputCubeDimension")
            filter.setValue(self.cubeData, forKey: "inputCubeData")
            self.currentLUTFilter = filter
            print("‚úÖ LUT filter created: dimension=\(lutInfo.dimension), data size=\(self.cubeData?.count ?? 0) bytes")
        } else {
            print("‚ùå Failed to create CIColorCube filter")
        }
    }
    
    // Applies the LUT to the given CIImage using a freshly created filter instance
    func applyLUT(to image: CIImage) -> CIImage? {
        guard let cubeData = self.cubeData else { return nil }
        
        let colorSpace = CGColorSpace(name: CGColorSpace.sRGB)
        
        let params: [String: Any] = [
            "inputCubeDimension": cubeDimension,
            "inputCubeData": cubeData,
            "inputColorSpace": colorSpace as Any, // Explicit cast to Any
            kCIInputImageKey: image
        ]
        
        return CIFilter(name: "CIColorCube", parameters: params)?.outputImage
    }
    
    // Creates a basic programmatic LUT when no files are available
    func setupProgrammaticLUT(dimension: Int, data: [Float]) {
        print("üé® Creating programmatic LUT: dimension=\(dimension), points=\(data.count/3)")
        
        var rgbaData = [Float]()
        rgbaData.reserveCapacity(dimension * dimension * dimension * 4)
        
        // Convert RGB data to RGBA (required by CIFilter)
        for i in stride(from: 0, to: data.count, by: 3) {
            if i + 2 < data.count {
                rgbaData.append(data[i])     // R
                rgbaData.append(data[i+1])   // G
                rgbaData.append(data[i+2])   // B
                rgbaData.append(1.0)         // A (always 1.0)
            }
        }
        
        // Create a copy of the data to avoid dangling pointer
        var dataCopy = rgbaData
        let nsData = NSData(bytes: &dataCopy, length: dataCopy.count * MemoryLayout<Float>.size)
        self.cubeData = nsData as Data
        self.cubeDimension = dimension
        
        if let filter = CIFilter(name: "CIColorCube") {
            filter.setValue(dimension, forKey: "inputCubeDimension")
            filter.setValue(self.cubeData, forKey: "inputCubeData")
            self.currentLUTFilter = filter
            print("‚úÖ Programmatic LUT filter created")
        } else {
            print("‚ùå Failed to create programmatic CIColorCube filter")
        }
    }
    
    // MARK: - Recent LUT Management
    
    private func loadRecentLUTs() {
        if let recentDict = UserDefaults.standard.dictionary(forKey: recentLUTsKey) as? [String: String] {
            var loadedLUTs: [String: URL] = [:]
            
            for (name, urlString) in recentDict {
                if let url = URL(string: urlString) {
                    loadedLUTs[name] = url
                }
            }
            
            self.recentLUTs = loadedLUTs
        }
    }
    
    private func addToRecentLUTs(url: URL) {
        if recentLUTs == nil {
            recentLUTs = [:]
        }
        
        // Add or update the URL
        recentLUTs?[url.lastPathComponent] = url
        
        // Ensure we don't exceed the maximum number of recent LUTs
        if let count = recentLUTs?.count, count > maxRecentLUTs {
            // Remove oldest entries
            let sortedKeys = recentLUTs?.keys.sorted { lhs, rhs in
                if let lhsURL = recentLUTs?[lhs], let rhsURL = recentLUTs?[rhs] {
                    return (try? lhsURL.resourceValues(forKeys: [.contentModificationDateKey]).contentModificationDate) ?? Date() >
                           (try? rhsURL.resourceValues(forKeys: [.contentModificationDateKey]).contentModificationDate) ?? Date()
                }
                return false
            }
            
            if let keysToRemove = sortedKeys?.suffix(from: maxRecentLUTs) {
                for key in keysToRemove {
                    recentLUTs?.removeValue(forKey: key)
                }
            }
        }
        
        // Save to UserDefaults
        let urlDict = recentLUTs?.mapValues { $0.absoluteString }
        UserDefaults.standard.set(urlDict, forKey: recentLUTsKey)
    }
    
    // MARK: - LUT Management
    
    /// Clears the current LUT filter
    func clearLUT() {
        currentLUTFilter = nil
        selectedLUTURL = nil
        print("‚úÖ LUT filter cleared")
    }
    
    /// Alias for clearLUT() for more readable API
    func clearCurrentLUT() {
        clearLUT()
    }
}

================
File: camera/Features/Settings/FlashlightSettingsView.swift
================
import SwiftUI

struct FlashlightSettingsView: View {
    @ObservedObject var settingsModel: SettingsModel
    @StateObject private var flashlightManager = FlashlightManager()
    
    var body: some View {
        Section {
            if flashlightManager.isAvailable {
                Toggle("Enable Recording Light", isOn: $settingsModel.isFlashlightEnabled)
                    .onChange(of: settingsModel.isFlashlightEnabled) { oldValue, newValue in
                        flashlightManager.isEnabled = newValue
                    }
                
                if settingsModel.isFlashlightEnabled {
                    VStack(alignment: .leading, spacing: 12) {
                        HStack {
                            Text("Light Intensity")
                            Spacer()
                            Text("\(String(format: "%.1f", settingsModel.flashlightIntensity * 100))%")
                        }
                        
                        Slider(value: $settingsModel.flashlightIntensity, in: 0.001...1.0) { editing in
                            flashlightManager.isEnabled = editing
                            flashlightManager.intensity = settingsModel.flashlightIntensity
                        }
                    }
                }
            } else {
                Text("Flashlight not available on this device")
                    .foregroundColor(.secondary)
            }
        } header: {
            Text("Recording Light")
        } footer: {
            Text("When enabled, the flashlight will be used as a recording indicator light. Adjust the intensity from 0.1% to 100% to your preference.")
        }
        .onDisappear {
            flashlightManager.turnOffForSettingsExit()
        }
    }
}

================
File: camera/Features/Settings/SettingsModel.swift
================
import Foundation
import AVFoundation
import CoreMedia

class SettingsModel: ObservableObject {
    @Published var isAppleLogEnabled: Bool {
        didSet {
            UserDefaults.standard.set(isAppleLogEnabled, forKey: "isAppleLogEnabled")
            NotificationCenter.default.post(name: .appleLogSettingChanged, object: nil)
        }
    }
    
    @Published var isFlashlightEnabled: Bool {
        didSet {
            UserDefaults.standard.set(isFlashlightEnabled, forKey: "isFlashlightEnabled")
            NotificationCenter.default.post(name: .flashlightSettingChanged, object: nil)
        }
    }
    
    @Published var flashlightIntensity: Float {
        didSet {
            UserDefaults.standard.set(flashlightIntensity, forKey: "flashlightIntensity")
            NotificationCenter.default.post(name: .flashlightSettingChanged, object: nil)
        }
    }
    
    @Published var isBakeInLUTEnabled: Bool {
        didSet {
            UserDefaults.standard.set(isBakeInLUTEnabled, forKey: "isBakeInLUTEnabled")
            NotificationCenter.default.post(name: .bakeInLUTSettingChanged, object: nil)
        }
    }
    
    var isAppleLogSupported: Bool {
        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
            return false
        }
        
        // Check if any format supports Apple Log
        return device.formats.contains { format in
            let colorSpaces = format.supportedColorSpaces.map { $0.rawValue }
            return colorSpaces.contains(AVCaptureColorSpace.appleLog.rawValue)
        }
    }
    
    init() {
        self.isAppleLogEnabled = UserDefaults.standard.bool(forKey: "isAppleLogEnabled")
        self.isFlashlightEnabled = UserDefaults.standard.bool(forKey: "isFlashlightEnabled")
        self.flashlightIntensity = UserDefaults.standard.float(forKey: "flashlightIntensity")
        self.isBakeInLUTEnabled = UserDefaults.standard.bool(forKey: "isBakeInLUTEnabled")
        
        // Set default values if not set
        if self.flashlightIntensity == 0 {
            self.flashlightIntensity = 1.0
            UserDefaults.standard.set(self.flashlightIntensity, forKey: "flashlightIntensity")
        }
        
        // By default, bake in LUT is enabled (matches current behavior)
        if UserDefaults.standard.object(forKey: "isBakeInLUTEnabled") == nil {
            self.isBakeInLUTEnabled = true
            UserDefaults.standard.set(self.isBakeInLUTEnabled, forKey: "isBakeInLUTEnabled")
        }
    }
}

extension Notification.Name {
    static let appleLogSettingChanged = Notification.Name("appleLogSettingChanged")
    static let flashlightSettingChanged = Notification.Name("flashlightSettingChanged")
    static let bakeInLUTSettingChanged = Notification.Name("bakeInLUTSettingChanged")
}

================
File: camera/Features/VideoLibrary/VideoLibraryView.swift
================
import SwiftUI
import Photos
import AVKit
import UIKit

struct VideoLibraryView: View {
    @StateObject private var viewModel = VideoLibraryViewModel()
    @Environment(\.dismiss) private var dismiss
    @State private var currentOrientation = UIDevice.current.orientation
    @State private var rotationLockApplied = false
    
    var body: some View {
        OrientationFixView(allowsLandscapeMode: true) {
            NavigationStack {
                ZStack {
                    Color.black.edgesIgnoringSafeArea(.all)
                    
                    if viewModel.isLoading {
                        ProgressView()
                            .scaleEffect(1.5)
                            .tint(.white)
                    } else if viewModel.videos.isEmpty {
                        VStack(spacing: 20) {
                            Image(systemName: "video.slash")
                                .font(.system(size: 60))
                                .foregroundColor(.gray)
                                .padding()
                            
                            Text("No videos found")
                                .font(.title2)
                                .foregroundColor(.gray)
                            
                            if viewModel.authorizationStatus == .denied || viewModel.authorizationStatus == .restricted {
                                Text("Please enable photo library access in Settings")
                                    .font(.callout)
                                    .foregroundColor(.orange)
                                    .multilineTextAlignment(.center)
                                    .padding(.horizontal, 30)
                                
                                Button(action: {
                                    if let url = URL(string: UIApplication.openSettingsURLString) {
                                        UIApplication.shared.open(url)
                                    }
                                }) {
                                    Text("Open Settings")
                                        .padding(.horizontal, 20)
                                        .padding(.vertical, 10)
                                        .background(Color.blue)
                                        .foregroundColor(.white)
                                        .cornerRadius(8)
                                }
                            } else {
                                Text("Try refreshing the library")
                                    .font(.callout)
                                    .foregroundColor(.gray)
                                
                                Button(action: {
                                    viewModel.refreshVideos()
                                }) {
                                    Text("Refresh")
                                        .padding(.horizontal, 20)
                                        .padding(.vertical, 10)
                                        .background(Color.blue)
                                        .foregroundColor(.white)
                                        .cornerRadius(8)
                                }
                            }
                        }
                    } else {
                        ScrollView {
                            LazyVGrid(columns: [GridItem(.adaptive(minimum: 160, maximum: 200))], spacing: 10) {
                                ForEach(viewModel.videos) { video in
                                    VideoThumbnailView(video: video)
                                        .cornerRadius(10)
                                        .shadow(radius: 2)
                                        .aspectRatio(9/16, contentMode: .fit)
                                        .onTapGesture {
                                            viewModel.selectedVideo = video
                                        }
                                }
                            }
                            .padding()
                        }
                    }
                }
                .navigationTitle("My Videos")
                .navigationBarTitleDisplayMode(.inline)
                .toolbar {
                    ToolbarItem(placement: .topBarLeading) {
                        Button("Close") {
                            print("DEBUG: Dismissing VideoLibraryView")
                            AppDelegate.isVideoLibraryPresented = false
                            dismiss()
                        }
                    }
                    
                    ToolbarItem(placement: .topBarTrailing) {
                        Button(action: {
                            viewModel.refreshVideos()
                        }) {
                            Image(systemName: "arrow.clockwise")
                        }
                        .disabled(viewModel.isLoading)
                    }
                }
                .background(Color.black)
            }
            .sheet(item: $viewModel.selectedVideo) { video in
                VideoPlayerView(asset: video.asset)
            }
            .onAppear {
                print("DEBUG: VideoLibraryView appeared")
                print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView onAppear - interface orientation: \(UIApplication.shared.connectedScenes.first(where: { $0 is UIWindowScene }).flatMap { $0 as? UIWindowScene }?.interfaceOrientation.rawValue ?? 0)")
                print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView onAppear - device orientation: \(UIDevice.current.orientation.rawValue)")
                
                // Set global flag for AppDelegate to enable landscape
                AppDelegate.isVideoLibraryPresented = true
                
                viewModel.requestAccess()
                setupOrientationNotification()
                
                // Apply rotation lock with a delay to ensure the view is fully initialized
                if !rotationLockApplied {
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        rotationLockApplied = true
                        print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView applying initial landscape orientation")
                        enableLandscapeOrientation()
                        
                        // Apply a second time after a short delay to ensure it takes effect
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                            enableLandscapeOrientation()
                            
                            // Force orientation update on all controllers
                            if let windowScene = UIApplication.shared.connectedScenes.first(where: { $0 is UIWindowScene }) as? UIWindowScene {
                                for window in windowScene.windows {
                                    window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                                }
                            }
                        }
                    }
                }
            }
            .onDisappear {
                print("DEBUG: VideoLibraryView disappeared")
                print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView onDisappear - device orientation: \(UIDevice.current.orientation.rawValue)")
                
                // Set flag to false when view disappears
                DispatchQueue.main.async {
                    AppDelegate.isVideoLibraryPresented = false
                }
                
                NotificationCenter.default.removeObserver(
                    NSObject(),
                    name: UIDevice.orientationDidChangeNotification,
                    object: nil
                )
            }
            .onChange(of: currentOrientation) { _, newOrientation in
                print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView orientation changed to: \(newOrientation.rawValue)")
                if newOrientation.isLandscape {
                    print("DEBUG: Library view detected landscape orientation: \(newOrientation.rawValue)")
                    print("DEBUG: [ORIENTATION-DEBUG] AppDelegate.isVideoLibraryPresented = \(AppDelegate.isVideoLibraryPresented)")
                    // Ensure AppDelegate knows we're in landscape mode
                    AppDelegate.isVideoLibraryPresented = true
                    
                    // Reapply landscape orientation to prevent popping back
                    if rotationLockApplied {
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView re-enabling landscape after orientation change")
                            enableLandscapeOrientation()
                        }
                    }
                } else if newOrientation == .portrait {
                    print("DEBUG: [ORIENTATION-DEBUG] Portrait orientation detected - checking if we need to prevent it")
                    // If we're in the video library, try to maintain landscape if that's what we want
                    if AppDelegate.isVideoLibraryPresented {
                        print("DEBUG: [ORIENTATION-DEBUG] VideoLibraryView still active, trying to maintain landscape")
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            enableLandscapeOrientation()
                        }
                    }
                }
            }
            .preferredColorScheme(.dark)
        }
    }
    
    private func setupOrientationNotification() {
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
        
        NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main
        ) { [self] _ in
            let newOrientation = UIDevice.current.orientation
            self.currentOrientation = newOrientation
            print("DEBUG: [ORIENTATION-DEBUG] Orientation notification received: \(newOrientation.rawValue)")
            print("DEBUG: Container detected device orientation change")
        }
    }
    
    private func enableLandscapeOrientation() {
        // Update AppDelegate flag first to ensure proper orientation support
        AppDelegate.isVideoLibraryPresented = true
        
        // Find the active window scene
        if let windowScene = UIApplication.shared.connectedScenes.first(where: { $0 is UIWindowScene }) as? UIWindowScene {
            // Get current device orientation
            let deviceOrientation = UIDevice.current.orientation
            
            // Determine the target interface orientation based on device orientation
            var targetOrientation: UIInterfaceOrientation = .landscapeRight // Default if we can't determine
            
            // More aggressively map device orientation to interface orientation
            if deviceOrientation.isLandscape {
                // Map device orientation to interface orientation
                // Note: UIDeviceOrientation.landscapeLeft maps to UIInterfaceOrientation.landscapeRight and vice versa
                targetOrientation = deviceOrientation == .landscapeLeft ? .landscapeRight : .landscapeLeft
                print("DEBUG: Video library forcing landscape: \(targetOrientation.rawValue) based on device: \(deviceOrientation.rawValue)")
            } else if deviceOrientation == .faceUp || deviceOrientation == .faceDown {
                // When device is face up/down, use the current interface orientation if it's landscape
                // otherwise default to landscape right
                let currentInterfaceOrientation = windowScene.interfaceOrientation
                if currentInterfaceOrientation.isLandscape {
                    targetOrientation = currentInterfaceOrientation
                }
                print("DEBUG: Video library handling face up/down orientation, using: \(targetOrientation.rawValue)")
            }
            
            // Set supported orientations to include landscape
            let orientations: UIInterfaceOrientationMask = [.portrait, .landscapeLeft, .landscapeRight]
            _ = UIWindowScene.GeometryPreferences.iOS(interfaceOrientations: orientations)
            
            print("DEBUG: Applying landscape orientation using scene geometry update")
            print("DEBUG: [ORIENTATION-DEBUG] Current interface orientation before update: \(windowScene.interfaceOrientation.rawValue)")
            print("DEBUG: [ORIENTATION-DEBUG] Target orientation: \(targetOrientation.rawValue)")
            
            // Force the geometry update with explicit target orientation
            // Create proper mask from single orientation
            let orientationMask: UIInterfaceOrientationMask
            switch targetOrientation {
            case .portrait: orientationMask = .portrait
            case .portraitUpsideDown: orientationMask = .portraitUpsideDown
            case .landscapeLeft: orientationMask = .landscapeLeft
            case .landscapeRight: orientationMask = .landscapeRight
            default: orientationMask = .portrait
            }
            
            let specificGeometryPreferences = UIWindowScene.GeometryPreferences.iOS(interfaceOrientations: orientationMask)
            windowScene.requestGeometryUpdate(specificGeometryPreferences) { error in
                print("DEBUG: [ORIENTATION-DEBUG] Specific landscape geometry update result: \(error.localizedDescription)")
            }
            
            // Also update all view controllers to make sure they respect the orientation
            for window in windowScene.windows {
                window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                print("DEBUG: [ORIENTATION-DEBUG] Called setNeedsUpdateOfSupportedInterfaceOrientations on root controller")
                if let presented = window.rootViewController?.presentedViewController {
                    presented.setNeedsUpdateOfSupportedInterfaceOrientations()
                    print("DEBUG: [ORIENTATION-DEBUG] Called setNeedsUpdateOfSupportedInterfaceOrientations on presented controller")
                    
                    // Recursively update all child controllers
                    updateOrientationForChildren(of: presented)
                }
            }
            
            // Attempt additional force rotation after a short delay
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                // Ensure AppDelegate flag is still set
                AppDelegate.isVideoLibraryPresented = true
                
                if UIDevice.current.orientation.isLandscape {
                    print("DEBUG: [ORIENTATION-DEBUG] Device already in landscape after 0.3s")
                } else {
                    print("DEBUG: [ORIENTATION-DEBUG] Device still not in landscape after 0.3s - forcing rotation")
                    
                    // Try a second geometry update with the specific orientation
                    // Create a new specific geometry preferences to ensure we use the updated API
                    let secondOrientationMask: UIInterfaceOrientationMask
                    // Default to landscape right if needed
                    let secondTargetOrientation: UIInterfaceOrientation = targetOrientation
                    
                    switch secondTargetOrientation {
                    case .portrait: secondOrientationMask = .portrait
                    case .portraitUpsideDown: secondOrientationMask = .portraitUpsideDown
                    case .landscapeLeft: secondOrientationMask = .landscapeLeft
                    case .landscapeRight: secondOrientationMask = .landscapeRight
                    default: secondOrientationMask = .landscapeRight
                    }
                    
                    let secondSpecificPreferences = UIWindowScene.GeometryPreferences.iOS(interfaceOrientations: secondOrientationMask)
                    windowScene.requestGeometryUpdate(secondSpecificPreferences) { error in
                        print("DEBUG: [ORIENTATION-DEBUG] Second specific landscape update: \(error.localizedDescription)")
                    }
                    
                    // Update to modern API for iOS 16+
                    if let windowScene = UIApplication.shared.connectedScenes
                        .first(where: { $0.activationState == .foregroundActive }) as? UIWindowScene {
                        for window in windowScene.windows {
                            window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                            
                            // Force update on presented controllers too
                            if let presented = window.rootViewController?.presentedViewController {
                                presented.setNeedsUpdateOfSupportedInterfaceOrientations()
                            }
                        }
                    }
                }
            }
        }
    }
    
    // Helper method to update orientation for all child controllers
    private func updateOrientationForChildren(of viewController: UIViewController) {
        for child in viewController.children {
            child.setNeedsUpdateOfSupportedInterfaceOrientations()
            updateOrientationForChildren(of: child)
        }
    }
}

struct VideoThumbnailView: View {
    let video: VideoAsset
    @State private var thumbnail: UIImage?
    @State private var duration: String = ""
    
    var body: some View {
        ZStack(alignment: .bottomTrailing) {
            if let thumbnail = thumbnail {
                Image(uiImage: thumbnail)
                    .resizable()
                    .aspectRatio(contentMode: .fill)
            } else {
                Rectangle()
                    .fill(Color.gray.opacity(0.3))
            }
            
            VStack(alignment: .trailing) {
                HStack {
                    Spacer()
                    Text(duration)
                        .font(.caption)
                        .padding(6)
                        .background(.ultraThinMaterial)
                        .cornerRadius(4)
                }
                .padding(8)
            }
            
            Image(systemName: "play.fill")
                .font(.title2)
                .padding(8)
                .background(.ultraThinMaterial)
                .clipShape(Circle())
                .padding(8)
        }
        .onAppear {
            loadThumbnail()
            formatDuration()
        }
    }
    
    private func loadThumbnail() {
        let imageManager = PHImageManager.default()
        let requestOptions = PHImageRequestOptions()
        requestOptions.isNetworkAccessAllowed = true
        requestOptions.deliveryMode = .highQualityFormat
        
        imageManager.requestImage(
            for: video.asset, 
            targetSize: CGSize(width: 300, height: 300),
            contentMode: .aspectFill,
            options: requestOptions
        ) { image, _ in
            self.thumbnail = image
        }
    }
    
    private func formatDuration() {
        let seconds = video.asset.duration
        let formatter = DateComponentsFormatter()
        formatter.allowedUnits = seconds >= 3600 ? [.hour, .minute, .second] : [.minute, .second]
        formatter.unitsStyle = .positional
        formatter.zeroFormattingBehavior = .pad
        duration = formatter.string(from: seconds) ?? ""
    }
}

struct VideoPlayerView: View {
    let asset: PHAsset
    @State private var player: AVPlayer?
    @Environment(\.dismiss) private var dismiss
    @State private var currentOrientation = UIDevice.current.orientation
    
    var body: some View {
        OrientationFixView(allowsLandscapeMode: true) {
            ZStack {
                if let player = player {
                    VideoPlayer(player: player)
                        .ignoresSafeArea()
                } else {
                    ProgressView()
                        .scaleEffect(1.5)
                        .tint(.white)
                }
            }
            .onAppear {
                // Also set flag for video player
                AppDelegate.isVideoLibraryPresented = true
                loadVideo()
                setupOrientationNotification()
                
                // Apply landscape orientation
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) {
                    enableLandscapeOrientation()
                }
            }
            .onDisappear {
                player?.pause()
                // Don't reset the flag here in case we're returning to library
                NotificationCenter.default.removeObserver(
                    NSObject(), 
                    name: UIDevice.orientationDidChangeNotification,
                    object: nil
                )
            }
            .onChange(of: currentOrientation) { _, newOrientation in
                if newOrientation.isLandscape {
                    print("DEBUG: VideoPlayer detected landscape orientation: \(newOrientation.rawValue)")
                    // Ensure AppDelegate knows we're in landscape mode
                    AppDelegate.isVideoLibraryPresented = true
                    
                    // Reapply landscape orientation to prevent popping back
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        enableLandscapeOrientation()
                    }
                }
            }
            .toolbar {
                ToolbarItem(placement: .navigationBarTrailing) {
                    Button("Done") {
                        dismiss()
                    }
                }
            }
            .preferredColorScheme(.dark)
        }
    }
    
    private func loadVideo() {
        let manager = PHImageManager.default()
        let options = PHVideoRequestOptions()
        options.deliveryMode = .highQualityFormat
        options.isNetworkAccessAllowed = true
        
        manager.requestAVAsset(forVideo: asset, options: options) { avAsset, _, _ in
            DispatchQueue.main.async {
                if let avAsset = avAsset {
                    self.player = AVPlayer(playerItem: AVPlayerItem(asset: avAsset))
                    self.player?.play()
                }
            }
        }
    }
    
    private func setupOrientationNotification() {
        UIDevice.current.beginGeneratingDeviceOrientationNotifications()
        
        NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main
        ) { _ in
            self.currentOrientation = UIDevice.current.orientation
        }
    }
    
    private func enableLandscapeOrientation() {
        // Use the modern API instead of setting device orientation directly
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
            let orientations: UIInterfaceOrientationMask = [.portrait, .landscapeLeft, .landscapeRight]
            let geometryPreferences = UIWindowScene.GeometryPreferences.iOS(interfaceOrientations: orientations)
            
            print("DEBUG: Applying landscape orientation using scene geometry update")
            windowScene.requestGeometryUpdate(geometryPreferences) { error in
                print("DEBUG: VideoPlayer landscape update result: \(error.localizedDescription)")
            }
            
            // Also update all view controllers to make sure they respect the orientation
            for window in windowScene.windows {
                window.rootViewController?.setNeedsUpdateOfSupportedInterfaceOrientations()
                if let presented = window.rootViewController?.presentedViewController {
                    presented.setNeedsUpdateOfSupportedInterfaceOrientations()
                }
            }
        }
    }
}

================
File: camera/Features/VideoLibrary/VideoLibraryViewModel.swift
================
import SwiftUI
import Photos
import Combine
import os.log

struct VideoAsset: Identifiable {
    let id = UUID()
    let asset: PHAsset
    let creationDate: Date?
    
    init(asset: PHAsset) {
        self.asset = asset
        self.creationDate = asset.creationDate
    }
}

class VideoLibraryViewModel: ObservableObject {
    @Published var videos: [VideoAsset] = []
    @Published var isLoading: Bool = false
    @Published var selectedVideo: VideoAsset?
    @Published var authorizationStatus: PHAuthorizationStatus = .notDetermined
    
    private var cancellables = Set<AnyCancellable>()
    private let logger = Logger(subsystem: "com.camera.app", category: "VideoLibrary")
    
    init() {
        // Request access immediately upon initialization
        requestAccess()
    }
    
    func requestAccess() {
        isLoading = true
        logger.debug("Requesting photo library access")
        
        PHPhotoLibrary.requestAuthorization(for: .readWrite) { [weak self] status in
            guard let self = self else { return }
            
            DispatchQueue.main.async {
                self.authorizationStatus = status
                self.logger.debug("Photo library authorization status: \(status.rawValue)")
                
                switch status {
                case .authorized, .limited:
                    self.fetchVideos()
                case .denied, .restricted:
                    self.isLoading = false
                    self.logger.error("Photo library access denied or restricted")
                case .notDetermined:
                    self.isLoading = false
                    self.logger.error("Photo library access not determined")
                @unknown default:
                    self.isLoading = false
                    self.logger.error("Unknown photo library access status")
                }
            }
        }
    }
    
    private func fetchVideos() {
        logger.debug("Fetching videos from photo library")
        
        // Create fetch options for video assets
        let options = PHFetchOptions()
        options.sortDescriptors = [NSSortDescriptor(key: "creationDate", ascending: false)]
        options.predicate = NSPredicate(format: "mediaType = %d", PHAssetMediaType.video.rawValue)
        options.includeAllBurstAssets = false
        options.includeHiddenAssets = false
        
        // Perform the fetch
        let fetchResult = PHAsset.fetchAssets(with: .video, options: options)
        logger.debug("Found \(fetchResult.count) videos in photo library")
        
        var newVideos: [VideoAsset] = []
        
        fetchResult.enumerateObjects { [weak self] (asset, index, stop) in
            guard let self = self else { return }
            let videoAsset = VideoAsset(asset: asset)
            newVideos.append(videoAsset)
            
            // Log some info about first few videos for debugging
            if index < 5 {
                self.logger.debug("Video \(index): duration \(asset.duration)s, created \(String(describing: asset.creationDate))")
            }
        }
        
        DispatchQueue.main.async {
            self.videos = newVideos
            self.isLoading = false
            
            if newVideos.isEmpty {
                self.logger.error("No videos found in photo library after fetch")
            } else {
                self.logger.debug("Successfully loaded \(newVideos.count) videos")
            }
        }
    }
    
    // Force a refresh of the video library
    func refreshVideos() {
        isLoading = true
        fetchVideos()
    }
}

================
File: camera/Preview Content/Preview Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/cameraApp.swift
================
//
//  cameraApp.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import SwiftUI

@main
struct cameraApp: App {
    let persistenceController = PersistenceController.shared
    // Register AppDelegate for orientation control
    @UIApplicationDelegateAdaptor(AppDelegate.self) var appDelegate
    
    init() {
        // Set background color for the entire app to black
        UIWindow.appearance().backgroundColor = UIColor.black
        
        // Force dark mode for the entire app using modern API
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
            windowScene.windows.forEach { window in
                window.overrideUserInterfaceStyle = .dark
            }
        }
        
        // Set dark mode for all windows that will be created
        if #available(iOS 13.0, *) {
            UIWindow.appearance().overrideUserInterfaceStyle = .dark
        }
        
        print("DEBUG: Set window appearance background to black and enforced dark mode")
    }
    
    var body: some Scene {
        WindowGroup {
            // Add background color to root view
            ZStack {
                Color.black.edgesIgnoringSafeArea(.all)
                
                // Wrap CameraView in OrientationFixView for strict orientation control
                OrientationFixView {
                    CameraView()
                }
            }
            .disableSafeArea() // Use our custom modifier to completely disable safe areas
            .environment(\.managedObjectContext, persistenceController.container.viewContext)
            // ADD: Hide status bar at app level
            .hideStatusBar()
            .preferredColorScheme(.dark) // Force dark mode at the SwiftUI level
            .onAppear {
                // Set the window's background color and interface style
                if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene,
                   let window = windowScene.windows.first {
                    window.backgroundColor = .black
                    window.overrideUserInterfaceStyle = .dark
                    print("DEBUG: Set window background to black and enforced dark mode")
                    
                    // Apply negative safe area insets to completely remove safe areas
                    window.rootViewController?.additionalSafeAreaInsets = UIEdgeInsets(top: -60, left: 0, bottom: 0, right: 0)
                    
                    // Force update layout
                    window.rootViewController?.view.setNeedsLayout()
                    window.rootViewController?.view.layoutIfNeeded()
                }
            }
        }
    }
}

================
File: camera/Info.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIApplicationSceneManifest</key>
	<dict>
		<key>UIApplicationSupportsMultipleScenes</key>
		<false/>
	</dict>
	<key>NSCameraUsageDescription</key>
	<string>This app needs camera access to record video</string>
	<key>NSMicrophoneUsageDescription</key>
	<string>This app needs microphone access to record audio</string>
	<key>NSPhotoLibraryUsageDescription</key>
	<string>This app needs access to your video library to display your recorded videos</string>
	<key>NSPhotoLibraryAddUsageDescription</key>
	<string>This app needs permission to save recorded videos to your photo library</string>
	<key>LSSupportsOpeningDocumentsInPlace</key>
	<true/>
	<key>UIFileSharingEnabled</key>
	<true/>
	<key>UIViewControllerBasedStatusBarAppearance</key>
	<false/>
	<key>UIStatusBarHidden</key>
	<true/>
	<key>UISupportedInterfaceOrientations</key>
	<array>
		<string>UIInterfaceOrientationPortrait</string>
		<string>UIInterfaceOrientationLandscapeLeft</string>
		<string>UIInterfaceOrientationLandscapeRight</string>
	</array>
</dict>
</plist>

================
File: camera/Persistence.swift
================
//
//  Persistence.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import CoreData

struct PersistenceController {
    static let shared = PersistenceController()

    @MainActor
    static let preview: PersistenceController = {
        let result = PersistenceController(inMemory: true)
        let viewContext = result.container.viewContext
        for _ in 0..<10 {
            let newItem = Item(context: viewContext)
            newItem.timestamp = Date()
        }
        do {
            try viewContext.save()
        } catch {
            // Replace this implementation with code to handle the error appropriately.
            // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.
            let nsError = error as NSError
            fatalError("Unresolved error \(nsError), \(nsError.userInfo)")
        }
        return result
    }()

    let container: NSPersistentContainer

    init(inMemory: Bool = false) {
        container = NSPersistentContainer(name: "camera")
        if inMemory {
            container.persistentStoreDescriptions.first!.url = URL(fileURLWithPath: "/dev/null")
        }
        container.loadPersistentStores(completionHandler: { (storeDescription, error) in
            if let error = error as NSError? {
                // Replace this implementation with code to handle the error appropriately.
                // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.

                /*
                 Typical reasons for an error here include:
                 * The parent directory does not exist, cannot be created, or disallows writing.
                 * The persistent store is not accessible, due to permissions or data protection when the device is locked.
                 * The device is out of space.
                 * The store could not be migrated to the current model version.
                 Check the error message to determine what the actual problem was.
                 */
                fatalError("Unresolved error \(error), \(error.userInfo)")
            }
        })
        container.viewContext.automaticallyMergesChangesFromParent = true
    }
}

================
File: camera.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcdebugger/Breakpoints_v2.xcbkptlist
================
<?xml version="1.0" encoding="UTF-8"?>
<Bucket
   uuid = "8C0EC471-D08D-4370-AFA9-6904E3CF5246"
   type = "1"
   version = "2.0">
</Bucket>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcschemes/xcschememanagement.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>SchemeUserState</key>
	<dict>
		<key>camera.xcscheme_^#shared#^_</key>
		<dict>
			<key>orderHint</key>
			<integer>0</integer>
		</dict>
	</dict>
</dict>
</plist>

================
File: camera.xcodeproj/project.pbxproj
================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXFileReference section */
		09562D7C2D18A1EC009A9B07 /* camera.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = camera.app; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = 09562D7B2D18A1EC009A9B07 /* camera */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		09562D7E2D18A1EC009A9B07 /* camera */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */,
			);
			path = camera;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		09562D792D18A1EC009A9B07 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		09562D732D18A1EC009A9B07 = {
			isa = PBXGroup;
			children = (
				09562D7E2D18A1EC009A9B07 /* camera */,
				09562D7D2D18A1EC009A9B07 /* Products */,
			);
			sourceTree = "<group>";
		};
		09562D7D2D18A1EC009A9B07 /* Products */ = {
			isa = PBXGroup;
			children = (
				09562D7C2D18A1EC009A9B07 /* camera.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		09562D7B2D18A1EC009A9B07 /* camera */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */;
			buildPhases = (
				09562D782D18A1EC009A9B07 /* Sources */,
				09562D792D18A1EC009A9B07 /* Frameworks */,
				09562D7A2D18A1EC009A9B07 /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				09562D7E2D18A1EC009A9B07 /* camera */,
			);
			name = camera;
			packageProductDependencies = (
			);
			productName = camera;
			productReference = 09562D7C2D18A1EC009A9B07 /* camera.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		09562D742D18A1EC009A9B07 /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1620;
				LastUpgradeCheck = 1620;
				TargetAttributes = {
					09562D7B2D18A1EC009A9B07 = {
						CreatedOnToolsVersion = 16.2;
					};
				};
			};
			buildConfigurationList = 09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 09562D732D18A1EC009A9B07;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = 09562D7D2D18A1EC009A9B07 /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				09562D7B2D18A1EC009A9B07 /* camera */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		09562D7A2D18A1EC009A9B07 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		09562D782D18A1EC009A9B07 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		09562D8D2D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		09562D8E2D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		09562D902D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		09562D912D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D8D2D18A1EE009A9B07 /* Debug */,
				09562D8E2D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D902D18A1EE009A9B07 /* Debug */,
				09562D912D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = 09562D742D18A1EC009A9B07 /* Project object */;
}

================
File: .gitignore
================
.DS_Store
