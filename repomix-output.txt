This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-30T19:45:30.958Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
camera/
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    Contents.json
  camera.xcdatamodeld/
    camera.xcdatamodel/
      contents
    .xccurrentversion
  Core/
    Extensions/
      CIContext+Shared.swift
  Features/
    Camera/
      CameraError.swift
      CameraPreviewView.swift
      CameraViewModel.swift
      DocumentPicker.swift
      ShutterAngle.swift
      VideoOutputDelegate.swift
    LUT/
      LUTManager.swift
    Settings/
      SettingsModel.swift
      SettingsView.swift
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  cameraApp.swift
  ContentView.swift
  Info.plist
  Persistence.swift
camera.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcuserdata/
    spencer.xcuserdatad/
      xcdebugger/
        Breakpoints_v2.xcbkptlist
      xcschemes/
        xcschememanagement.plist
  project.pbxproj

================================================================
Repository Files
================================================================

================
File: camera/Assets.xcassets/AccentColor.colorset/Contents.json
================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/AppIcon.appiconset/Contents.json
================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/camera.xcdatamodeld/camera.xcdatamodel/contents
================
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<model type="com.apple.IDECoreDataModeler.DataModel" documentVersion="1.0" lastSavedToolsVersion="1" systemVersion="11A491" minimumToolsVersion="Automatic" sourceLanguage="Swift" usedWithCloudKit="false" userDefinedModelVersionIdentifier="">
    <entity name="Item" representedClassName="Item" syncable="YES" codeGenerationType="class">
        <attribute name="timestamp" optional="YES" attributeType="Date" usesScalarValueType="NO"/>
    </entity>
    <elements>
        <element name="Item" positionX="-63" positionY="-18" width="128" height="44"/>
    </elements>
</model>

================
File: camera/camera.xcdatamodeld/.xccurrentversion
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>_XCCurrentVersionName</key>
	<string>camera.xcdatamodel</string>
</dict>
</plist>

================
File: camera/Core/Extensions/CIContext+Shared.swift
================
import CoreImage

extension CIContext {
    static let shared: CIContext = {
        let options = [
            CIContextOption.workingColorSpace: CGColorSpace(name: CGColorSpace.displayP3)!,
            CIContextOption.useSoftwareRenderer: false
        ]
        return CIContext(options: options)
    }()
}

================
File: camera/Features/Camera/CameraError.swift
================
import Foundation

enum CameraError: Error, Identifiable {
    case cameraUnavailable
    case setupFailed
    case configurationFailed
    case recordingFailed
    case savingFailed
    case whiteBalanceError
    
    var id: String { description }
    
    var description: String {
        switch self {
        case .cameraUnavailable:
            return "Camera device not available"
        case .setupFailed:
            return "Failed to setup camera"
        case .configurationFailed:
            return "Failed to configure camera settings"
        case .recordingFailed:
            return "Failed to record video"
        case .savingFailed:
            return "Failed to save video to photo library"
        case .whiteBalanceError:
            return "Failed to adjust white balance settings"
        }
    }
}

================
File: camera/Features/Camera/CameraPreviewView.swift
================
import SwiftUI
import AVFoundation

struct CameraPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    let lutManager: LUTManager
    let viewModel: CameraViewModel
    
    class PreviewView: UIView {
        override class var layerClass: AnyClass {
            AVCaptureVideoPreviewLayer.self
        }
        
        var videoPreviewLayer: AVCaptureVideoPreviewLayer {
            return layer as! AVCaptureVideoPreviewLayer
        }
        
        var videoOutput: AVCaptureVideoDataOutput?
        
        func setupVideoOutput(session: AVCaptureSession, delegate: AVCaptureVideoDataOutputSampleBufferDelegate) {
            // Remove any existing output
            if let existingOutput = videoOutput {
                session.removeOutput(existingOutput)
            }
            
            // Create and configure video output
            let output = AVCaptureVideoDataOutput()
            output.setSampleBufferDelegate(delegate, queue: DispatchQueue(label: "videoQueue"))
            
            if session.canAddOutput(output) {
                session.addOutput(output)
                videoOutput = output
                print("âœ… Video output added to session")
            } else {
                print("âŒ Could not add video output to session")
            }
        }
    }
    
    func makeUIView(context: Context) -> PreviewView {
        let view = PreviewView()
        view.videoPreviewLayer.session = session
        view.videoPreviewLayer.videoGravity = .resizeAspect
        
        // Create and set up video output delegate
        let videoDelegate = VideoOutputDelegate(lutManager: lutManager, viewModel: viewModel)
        view.setupVideoOutput(session: session, delegate: videoDelegate)
        
        // Initial orientation setup
        updatePreviewLayerOrientation(view.videoPreviewLayer)
        
        // Add orientation change observer
        NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { _ in
                updatePreviewLayerOrientation(view.videoPreviewLayer)
            }
        
        return view
    }
    
    func updateUIView(_ uiView: PreviewView, context: Context) {
        CATransaction.begin()
        CATransaction.setAnimationDuration(0.25)
        updatePreviewLayerOrientation(uiView.videoPreviewLayer)
        CATransaction.commit()
    }
    
    private func updatePreviewLayerOrientation(_ layer: AVCaptureVideoPreviewLayer) {
        guard let connection = layer.connection else { return }
        
        let currentDevice = UIDevice.current
        let orientation = currentDevice.orientation
        
        if #available(iOS 17.0, *) {
            switch orientation {
            case .portrait:
                connection.videoRotationAngle = 90
            case .landscapeRight: // Device rotated left
                connection.videoRotationAngle = 180
            case .landscapeLeft: // Device rotated right
                connection.videoRotationAngle = 0
            case .portraitUpsideDown:
                connection.videoRotationAngle = 270
            default:
                connection.videoRotationAngle = 90
            }
        } else {
            switch orientation {
            case .portrait:
                connection.videoOrientation = .portrait
            case .landscapeRight: // Device rotated left
                connection.videoOrientation = .landscapeLeft
            case .landscapeLeft: // Device rotated right
                connection.videoOrientation = .landscapeRight
            case .portraitUpsideDown:
                connection.videoOrientation = .portraitUpsideDown
            default:
                connection.videoOrientation = .portrait
            }
        }
    }
    
    // Add this method to track image processing
    private func processImage(_ image: CIImage) -> CIImage {
        var processedImage = image
        
        // Log the initial state
        print("ðŸŽ¥ Processing image pipeline:")
        print("  - Input image: \(image)")
        
        if viewModel.isAppleLogEnabled {
            print("  - Apple Log enabled, converting...")
            // Apple Log processing here
        }
        
        // Check if LUT should be applied and apply it
        if lutManager.currentLUTFilter != nil {
            print("  - Applying LUT filter...")
            if let lutImage = lutManager.applyLUT(to: processedImage) {
                processedImage = lutImage
                print("  âœ… LUT applied successfully")
            } else {
                print("  âŒ Failed to apply LUT")
            }
        } else {
            print("  â„¹ï¸ No LUT filter active")
        }
        
        print("  - Final output image: \(processedImage)")
        return processedImage
    }
}

================
File: camera/Features/Camera/CameraViewModel.swift
================
import AVFoundation
import SwiftUI
import Photos
import VideoToolbox
import CoreVideo
import os.log
import CoreImage

class CameraViewModel: NSObject, ObservableObject {
    enum Status {
        case unknown
        case running
        case failed
        case unauthorized
    }
    @Published private(set) var status: Status = .unknown
    
    enum CaptureMode {
        case photo
        case video
    }
    @Published var captureMode: CaptureMode = .video
    
    private let logger = Logger(subsystem: "com.camera", category: "CameraViewModel")
    
    @Published var isSessionRunning = false
    @Published var error: CameraError?
    @Published var whiteBalance: Float = 5000 // Kelvin
    @Published var iso: Float = 100
    @Published var shutterSpeed: CMTime = CMTimeMake(value: 1, timescale: 60) // Initialize for 180Â° at 30fps
    @Published var isRecording = false
    @Published var recordingFinished = false
    @Published var isSettingsPresented = false
    @Published var isProcessingRecording = false
    
    // Enable Apple Log (4K ProRes) by default if device supports it
    @Published var isAppleLogEnabled = false {
        didSet {
            print("\n=== Apple Log Toggle ===")
            print("ðŸ”„ Status: \(status)")
            print("ðŸ“¹ Capture Mode: \(captureMode)")
            print("âœ… Attempting to set Apple Log to: \(isAppleLogEnabled)")
            
            guard status == .running, captureMode == .video else {
                print("âŒ Cannot configure Apple Log - Status or mode incorrect")
                print("Required: status == .running (is: \(status))")
                print("Required: captureMode == .video (is: \(captureMode))")
                return
            }
            
            // Use Task with proper error handling
            Task {
                do {
                    if isAppleLogEnabled {
                        print("ðŸŽ¥ Configuring Apple Log...")
                        try await configureAppleLog()
                    } else {
                        print("â†©ï¸ Resetting Apple Log...")
                        try await resetAppleLog()
                    }
                } catch {
                    await MainActor.run {
                        self.error = .configurationFailed
                    }
                    logger.error("Failed to configure Apple Log: \(error.localizedDescription)")
                    print("âŒ Apple Log configuration failed: \(error)")
                }
            }
            print("=== End Apple Log Toggle ===\n")
        }
    }
    
    @Published private(set) var isAppleLogSupported = false
    
    let session = AVCaptureSession()
    private var device: AVCaptureDevice?
    
    // Add movie file output
    private let movieOutput = AVCaptureMovieFileOutput()
    private var currentRecordingURL: URL?
    
    private var defaultFormat: AVCaptureDevice.Format?
    
    var minISO: Float {
        device?.activeFormat.minISO ?? 50
    }
    var maxISO: Float {
        device?.activeFormat.maxISO ?? 1600
    }
    
    // Add new property for frame rate
    @Published var selectedFrameRate: Double = 30.0
    
    // Add available frame rates
    let availableFrameRates: [Double] = [23.976, 24.0, 25.0, 29.97, 30.0]
    
    private var orientationObserver: NSObjectProtocol?
    
    // Add property to track interface orientation
    @Published private(set) var currentInterfaceOrientation: UIInterfaceOrientation = .portrait
    
    private let processingQueue = DispatchQueue(
        label: "com.camera.processing",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem
    )
    
    // Add properties for frame rate monitoring
    private var lastFrameTimestamp: CFAbsoluteTime = 0
    
    private var lastFrameTime: CMTime?
    private var frameCount: Int = 0
    private var frameRateAccumulator: Double = 0
    private var frameRateUpdateInterval: Int = 30 // Update every 30 frames
    
    // Add property to store supported frame rate range
    private var supportedFrameRateRange: AVFrameRateRange? {
        device?.activeFormat.videoSupportedFrameRateRanges.first
    }
    
    // Add properties for advanced configuration
    private var videoConfiguration: [String: Any] = [
        AVVideoCodecKey: AVVideoCodecType.proRes422,
        AVVideoCompressionPropertiesKey: [
            AVVideoAverageBitRateKey: 50_000_000, // 50 Mbps
            AVVideoMaxKeyFrameIntervalKey: 1, // Every frame is keyframe
            AVVideoAllowFrameReorderingKey: false,
            AVVideoExpectedSourceFrameRateKey: 30
        ]
    ]
    
    // Add these constants
    private struct FrameRates {
        static let ntsc23_976 = CMTime(value: 1001, timescale: 24000)  // 23.976 fps
        static let ntsc29_97 = CMTime(value: 1001, timescale: 30000)   // 29.97 fps
        static let film24 = CMTime(value: 1, timescale: 24)            // 24 fps
        static let pal25 = CMTime(value: 1, timescale: 25)             // 25 fps
        static let ntsc30 = CMTime(value: 1, timescale: 30)            // 30 fps
    }
    
    @Published var currentTint: Double = 0.0 // Range: -150 to +150
    private let tintRange = (-150.0...150.0)
    
    private var videoDeviceInput: AVCaptureDeviceInput?
    
    @Published var isAutoExposureEnabled: Bool = true {
        didSet {
            updateExposureMode()
        }
    }
    
    @Published var lutManager = LUTManager()
    private var ciContext = CIContext()
    
    override init() {
        super.init()
        print("\n=== Camera Initialization ===")
        
        do {
            try setupSession()
            
            // Print device capabilities
            if let device = device {
                print("ðŸ“Š Device Capabilities:")
                print("- Name: \(device.localizedName)")
                print("- Model ID: \(device.modelID)")
                
                print("\nðŸŽ¨ Supported Color Spaces:")
                device.formats.forEach { format in
                    let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
                    let codecType = CMFormatDescriptionGetMediaSubType(format.formatDescription)
                    print("""
                        Format: \(dimensions.width)x\(dimensions.height) - Codec: \(codecType)
                        - Color Spaces: \(format.supportedColorSpaces.map { $0.rawValue })
                        - Supports Apple Log: \(format.supportedColorSpaces.contains(.appleLog))
                        - Supports HDR: \(format.isVideoHDRSupported)
                        """)
                }
                
                isAppleLogSupported = device.formats.contains { format in
                    format.supportedColorSpaces.contains(.appleLog)
                }
                print("\nâœ… Apple Log Support: \(isAppleLogSupported)")
            }
            print("=== End Initialization ===\n")
            
            // Store default format
            if let device = device {
                defaultFormat = device.activeFormat
            }
            
            // Check Apple Log support
            isAppleLogSupported = device?.formats.contains { format in
                format.supportedColorSpaces.contains(.appleLog)
            } ?? false
        } catch {
            self.error = .setupFailed
            print("Failed to setup session: \(error)")
        }
        
        // Add orientation observer
        orientationObserver = NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { [weak self] _ in
                guard let self = self,
                      let connection = self.movieOutput.connection(with: .video) else { return }
                self.updateVideoOrientation(connection)
        }
        
        // After other initialization, set initial shutter angle to 180Â°
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.updateShutterAngle(180.0) // Set initial shutter angle to 180Â°
        }
    }
    
    deinit {
        if let observer = orientationObserver {
            NotificationCenter.default.removeObserver(observer)
        }
    }
    
    /// Returns a 4K (3840x2160) AppleProRes422 format that also supports Apple Log
    private func findBestAppleLogFormat(_ device: AVCaptureDevice) -> AVCaptureDevice.Format? {
        return device.formats.first { format in
            let desc = format.formatDescription
            let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
            let codecType = CMFormatDescriptionGetMediaSubType(desc)
            
            let is4K = (dimensions.width == 3840 && dimensions.height == 2160)
            let isProRes422 = (codecType == kCMVideoCodecType_AppleProRes422) // 'x422'
            let hasAppleLog = format.supportedColorSpaces.contains(.appleLog)
            
            return is4K && isProRes422 && hasAppleLog
        }
    }
    
    private func configureAppleLog() async throws {
        guard let device = device else {
            print("âŒ No camera device available")
            return
        }
        
        print("\n=== Apple Log Configuration ===")
        print("ðŸŽ¥ Current device: \(device.localizedName)")
        print("ðŸ“Š Current format: \(device.activeFormat.formatDescription)")
        print("ðŸŽ¨ Current color space: \(device.activeColorSpace.rawValue)")
        print("ðŸŽ¨ Wide color enabled: \(session.automaticallyConfiguresCaptureDeviceForWideColor)")
        
        // Ensure wide color is disabled
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        
        // Check if format supports Apple Log
        let supportsAppleLog = device.formats.contains { format in
            format.supportedColorSpaces.contains(.appleLog)
        }
        print("âœ“ Device supports Apple Log: \(supportsAppleLog)")
        
        do {
            session.stopRunning()
            print("â¸ï¸ Session stopped for reconfiguration")
            
            try await Task.sleep(for: .milliseconds(100))
            session.beginConfiguration()
            
            try device.lockForConfiguration()
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                
                // Fix orientation after configuration
                if let videoConnection = movieOutput.connection(with: .video) {
                    updateVideoOrientation(videoConnection)
                }
                
                session.startRunning()
            }
            
            // Find best Apple Log format
            if let format = device.formats.first(where: {
                let desc = $0.formatDescription
                let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
                let codecType = CMFormatDescriptionGetMediaSubType(desc)
                
                let is4K = (dimensions.width == 3840 && dimensions.height == 2160)
                // Check for ProRes422 or ProRes422HQ codec
                let isProRes = (codecType == kCMVideoCodecType_AppleProRes422 ||
                              codecType == kCMVideoCodecType_AppleProRes422HQ ||
                              codecType == 2016686642) // This is the codec we see in the logs
                let hasAppleLog = $0.supportedColorSpaces.contains(.appleLog)
                
                print("""
                    Checking format:
                    - Resolution: \(dimensions.width)x\(dimensions.height) (is4K: \(is4K))
                    - Codec: \(codecType) (isProRes: \(isProRes))
                    - Has Apple Log: \(hasAppleLog)
                    """)
                
                return (is4K || dimensions.width >= 1920) && isProRes && hasAppleLog
            }) {
                print("âœ… Found suitable Apple Log format")
                print("ðŸ“¹ Format details: \(format.formatDescription)")
                
                // Remove this part that was resetting frame rate
                // let frameRateRange = format.videoSupportedFrameRateRanges.first!
                // device.activeVideoMinFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.maxFrameRate))
                // device.activeVideoMaxFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.minFrameRate))
                
                // Instead, maintain current frame rate
                let duration = CMTimeMake(value: 1000, timescale: Int32(selectedFrameRate * 1000))
                device.activeVideoMinFrameDuration = duration
                device.activeVideoMaxFrameDuration = duration
                
                // Set format and color space
                device.activeFormat = format
                device.activeColorSpace = .appleLog
                print("ðŸŽ¨ Set color space to Apple Log")
                
                print("âœ… Successfully configured Apple Log format")
            } else {
                print("âŒ No suitable Apple Log format found")
                throw CameraError.configurationFailed
            }
            
            print("ðŸ’¾ Configuration committed")
            print("â–¶ï¸ Session restarted")
            
        } catch {
            print("âŒ Error configuring Apple Log: \(error.localizedDescription)")
            
            // Ensure we properly clean up on error
            device.unlockForConfiguration()
            session.commitConfiguration()
            session.startRunning()
            
            // Update UI on main thread
            await MainActor.run {
                self.error = .configurationFailed
            }
            
            print("ðŸ”„ Attempting session recovery")
            throw error
        }
        
        print("=== End Apple Log Configuration ===\n")
    }
    
    private func resetAppleLog() async throws {
        guard let device = device else {
            print("âŒ No camera device available")
            return
        }
        
        print("\n=== Resetting Apple Log Configuration ===")
        print("ðŸŽ¨ Wide color enabled: \(session.automaticallyConfiguresCaptureDeviceForWideColor)")
        
        // Ensure wide color is disabled
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        
        do {
            session.stopRunning()
            session.beginConfiguration()
            
            try device.lockForConfiguration()
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                
                // Fix orientation after configuration
                if let videoConnection = movieOutput.connection(with: .video) {
                    updateVideoOrientation(videoConnection)
                }
                
                session.startRunning()
            }
            
            if let defaultFormat = defaultFormat {
                device.activeFormat = defaultFormat
            }
            device.activeColorSpace = .sRGB
            
            session.commitConfiguration()
            session.startRunning()
            
            print("âœ… Successfully reset to sRGB color space")
        } catch {
            print("âŒ Error resetting Apple Log: \(error.localizedDescription)")
            self.error = .configurationFailed
            session.startRunning()
        }
        
        print("=== End Reset ===\n")
    }
    
    private func setupSession() throws {
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        session.beginConfiguration()
        
        guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                    for: .video,
                                                    position: .back) else {
            error = .cameraUnavailable
            status = .failed
            session.commitConfiguration()
            return
        }
        
        self.device = videoDevice
        
        do {
            // Initialize videoDeviceInput
            let input = try AVCaptureDeviceInput(device: videoDevice)
            self.videoDeviceInput = input
            
            // Configure Apple Log if enabled
            if isAppleLogEnabled, let appleLogFormat = findBestAppleLogFormat(videoDevice) {
                let frameRateRange = appleLogFormat.videoSupportedFrameRateRanges.first!
                try videoDevice.lockForConfiguration()
                videoDevice.activeVideoMinFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.maxFrameRate))
                videoDevice.activeVideoMaxFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.minFrameRate))
                videoDevice.activeFormat = appleLogFormat
                videoDevice.activeColorSpace = .appleLog
                print("Initial setup: Enabled Apple Log in 4K ProRes format")
            }
            
            // Add video input
            if session.canAddInput(input) {
                session.addInput(input)
            }
            
            // Add audio input
            if let audioDevice = AVCaptureDevice.default(for: .audio),
               let audioInput = try? AVCaptureDeviceInput(device: audioDevice),
               session.canAddInput(audioInput) {
                session.addInput(audioInput)
            }
            
            // Configure movie output
            if session.canAddOutput(movieOutput) {
                session.addOutput(movieOutput)
                
                // Configure for high quality
                movieOutput.movieFragmentInterval = .invalid
                
                if let connection = movieOutput.connection(with: .video) {
                    if connection.isVideoStabilizationSupported {
                        connection.preferredVideoStabilizationMode = .auto
                    }
                    updateVideoOrientation(connection)
                }
            }
            
            // Set initial frame rate
            if let device = device {
                try device.lockForConfiguration()
                let duration = CMTimeMake(value: 1000, timescale: Int32(selectedFrameRate * 1000))
                device.activeVideoMinFrameDuration = duration
                device.activeVideoMaxFrameDuration = duration
                device.unlockForConfiguration()
            }
            
        } catch {
            print("Error setting up camera: \(error)")
            self.error = .setupFailed
            session.commitConfiguration()
            return
        }
        
        session.commitConfiguration()
        
        // Configure session preset
        if session.canSetSessionPreset(.hd4K3840x2160) {
            session.sessionPreset = .hd4K3840x2160
        } else if session.canSetSessionPreset(.hd1920x1080) {
            session.sessionPreset = .hd1920x1080
        }
        
        // Start session
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.session.startRunning()
            DispatchQueue.main.async {
                self?.isSessionRunning = self?.session.isRunning ?? false
                self?.status = .running
            }
        }
        
        // Check Apple Log support
        isAppleLogSupported = device?.formats.contains { format in
            format.supportedColorSpaces.contains(.appleLog)
        } ?? false
        
        // Store default format
        defaultFormat = device?.activeFormat
    }
    
    // White Balance
    func updateWhiteBalance(_ temperature: Float) {
        guard let device = device else { return }
        do {
            try device.lockForConfiguration()
            let tnt = AVCaptureDevice.WhiteBalanceTemperatureAndTintValues(temperature: temperature, tint: 0.0)
            var gains = device.deviceWhiteBalanceGains(for: tnt)
            let maxGain = device.maxWhiteBalanceGain
            
            gains.redGain   = min(max(1.0, gains.redGain), maxGain)
            gains.greenGain = min(max(1.0, gains.greenGain), maxGain)
            gains.blueGain  = min(max(1.0, gains.blueGain), maxGain)
            
            device.setWhiteBalanceModeLocked(with: gains) { _ in }
            device.unlockForConfiguration()
            
            whiteBalance = temperature
        } catch {
            print("White balance error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    // ISO
    func updateISO(_ iso: Float) {
        guard let device = device else { return }
        do {
            try device.lockForConfiguration()
            let clamped = min(max(device.activeFormat.minISO, iso), device.activeFormat.maxISO)
            device.setExposureModeCustom(duration: device.exposureDuration, iso: clamped) { _ in }
            device.unlockForConfiguration()
            self.iso = clamped
        } catch {
            print("ISO error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    // Shutter
    func updateShutterSpeed(_ speed: CMTime) {
        guard let device = device else { return }
        do {
            print("\nâš¡ Updating Shutter Speed:")
            print("  - Input Time: \(speed.value)/\(speed.timescale)")
            print("  - Duration: \(speed.seconds) seconds")
            
            try device.lockForConfiguration()
            device.setExposureModeCustom(duration: speed, iso: device.iso) { _ in }
            device.unlockForConfiguration()
            
            // Update the published property on the main thread
            DispatchQueue.main.async {
                self.shutterSpeed = speed
                
                // Calculate and log the resulting angle
                let resultingAngle = Double(speed.value) / Double(speed.timescale) * self.selectedFrameRate * 360.0
                print("  - Resulting Angle: \(resultingAngle)Â°")
            }
        } catch {
            print("âŒ Shutter speed error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    // Start recording in ProRes422 at 4K with Apple Log if format allows
    func startRecording() {
        guard !isRecording && !isProcessingRecording else {
            print("Cannot start recording: Already in progress or processing")
            return
        }
        
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let videoName = "recording-\(Date().timeIntervalSince1970).mov"
        let videoPath = documentsPath.appendingPathComponent(videoName)
        currentRecordingURL = videoPath
        
        // Start recording
        movieOutput.startRecording(to: videoPath, recordingDelegate: self)
        isRecording = true
        print("Starting recording to: \(videoPath.path)")
    }
    
    func stopRecording() {
        guard isRecording else {
            print("Cannot stop recording: No ongoing recording")
            return
        }
        
        print("Stopping recording...")
        isProcessingRecording = true
        movieOutput.stopRecording()
    }
    
    private func updateVideoOrientation(_ connection: AVCaptureConnection) {
        // Check if rotation is supported for our required angles
        let requiredAngles: [CGFloat] = [0, 90, 180, 270]
        let supportsRotation = requiredAngles.allSatisfy { angle in
            connection.isVideoRotationAngleSupported(angle)
        }
        
        guard supportsRotation else { return }
        
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                let interfaceOrientation = windowScene.interfaceOrientation
                self.currentInterfaceOrientation = interfaceOrientation
                
                // Adjusted angles to fix upside down horizontal video
                switch interfaceOrientation {
                case .portrait:
                    connection.videoRotationAngle = 90   // Keep portrait the same
                case .portraitUpsideDown:
                    connection.videoRotationAngle = 270  // Keep upside down portrait the same
                case .landscapeLeft:
                    connection.videoRotationAngle = 180  // Changed from 0 to 180
                case .landscapeRight:
                    connection.videoRotationAngle = 0    // Changed from 180 to 0
                default:
                    connection.videoRotationAngle = 90   // Keep default the same
                }
            }
            
            if connection.isVideoMirroringSupported {
                connection.isVideoMirrored = false
            }
        }
    }
    
    // Add new method to find compatible format for frame rate
    private func findCompatibleFormat(for fps: Double) -> AVCaptureDevice.Format? {
        guard let device = device else { return nil }
        
        print("\n=== Checking Format Compatibility ===")
        print("Requested frame rate: \(fps) fps")
        
        let formats = device.formats.filter { format in
            // Get current dimensions
            let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
            let isHighRes = dimensions.width >= 1920 // At least 1080p
            
            // Check frame rate support
            let supportsFrameRate = format.videoSupportedFrameRateRanges.contains { range in
                range.minFrameRate <= fps && fps <= range.maxFrameRate
            }
            
            // For Apple Log, ensure format supports it
            if isAppleLogEnabled {
                return isHighRes && supportsFrameRate && format.supportedColorSpaces.contains(.appleLog)
            }
            
            return isHighRes && supportsFrameRate
        }
        
        // Log available formats
        formats.forEach { format in
            let dims = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
            let ranges = format.videoSupportedFrameRateRanges
            print("""
                Format: \(dims.width)x\(dims.height)
                - Frame rates: \(ranges.map { "\($0.minFrameRate)-\($0.maxFrameRate)" }.joined(separator: ", "))
                - Supports Apple Log: \(format.supportedColorSpaces.contains(.appleLog))
                """)
        }
        
        return formats.first
    }
    
    // Update frame rate setting method
    func updateFrameRate(_ fps: Double) {
        guard let device = device else { return }
        
        do {
            // Find compatible format first
            guard let compatibleFormat = findCompatibleFormat(for: fps) else {
                print("âŒ No compatible format found for \(fps) fps")
                return
            }
            
            try device.lockForConfiguration()
            
            // Set format if different from current
            if device.activeFormat != compatibleFormat {
                print("Switching to compatible format...")
                device.activeFormat = compatibleFormat
            }
            
            // Get precise frame duration
            let frameDuration: CMTime
            switch fps {
            case 23.976:
                frameDuration = FrameRates.ntsc23_976
            case 29.97:
                frameDuration = FrameRates.ntsc29_97
            case 24:
                frameDuration = FrameRates.film24
            case 25:
                frameDuration = FrameRates.pal25
            case 30:
                frameDuration = FrameRates.ntsc30
            default:
                frameDuration = CMTimeMake(value: 1, timescale: Int32(fps))
            }
            
            // Set both min and max to the same duration for precise timing
            device.activeVideoMinFrameDuration = frameDuration
            device.activeVideoMaxFrameDuration = frameDuration
            
            // Update state on main thread
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                self.selectedFrameRate = fps
                self.frameCount = 0
                self.frameRateAccumulator = 0
                self.lastFrameTime = nil
            }
            
            print("""
                âœ… Frame rate configured:
                - Rate: \(fps) fps
                - Duration: \(frameDuration.seconds) seconds
                - Format: \(CMVideoFormatDescriptionGetDimensions(compatibleFormat.formatDescription))
                """)
            
            device.unlockForConfiguration()
        } catch {
            print("âŒ Frame rate error: \(error)")
            self.error = .configurationFailed
        }
        
        // Store current shutter angle before changing frame rate
        let currentAngle = shutterAngle
        
        // After frame rate is updated, restore the shutter angle
        updateShutterAngle(currentAngle)
    }
    
    // Update frame rate monitoring to be less aggressive and use main thread
    private func adjustFrameRatePrecision(currentFPS: Double) {
        // Only adjust if deviation is significant (more than 2%)
        let deviation = abs(currentFPS - selectedFrameRate) / selectedFrameRate
        guard deviation > 0.02 else { return }
        
        // Add delay between adjustments
        let now = Date().timeIntervalSince1970
        guard (now - lastAdjustmentTime) > 1.0 else { return } // Wait at least 1 second between adjustments
        
        lastAdjustmentTime = now
        
        // Reset frame rate to selected value instead of trying to adjust
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.updateFrameRate(self.selectedFrameRate)
        }
    }
    
    // Add property to track last adjustment time
    private var lastAdjustmentTime: TimeInterval = 0
    
    // Add method to update orientation
    func updateInterfaceOrientation() {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                self.currentInterfaceOrientation = windowScene.interfaceOrientation
            }
        }
    }
    
    // Add HDR support
    private func configureHDR() {
        guard let device = device,
              device.activeFormat.isVideoHDRSupported else { return }
        
        do {
            try device.lockForConfiguration()
            device.automaticallyAdjustsVideoHDREnabled = false
            device.isVideoHDREnabled = true
            device.unlockForConfiguration()
        } catch {
            print("Error configuring HDR: \(error)")
        }
    }
    
    private func optimizeVideoCapture() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            
            // Stabilization
            if device.activeFormat.isVideoStabilizationModeSupported(.cinematic) {
                if let connection = movieOutput.connection(with: .video),
                   connection.isVideoStabilizationSupported {
                    connection.preferredVideoStabilizationMode = .cinematic
                }
            }
            
            // Auto focus system
            if device.isFocusModeSupported(.continuousAutoFocus) {
                device.focusMode = .continuousAutoFocus
            }
            
            // Auto exposure
            if device.isExposureModeSupported(.continuousAutoExposure) {
                device.exposureMode = .continuousAutoExposure
            }
            
            // White balance
            if device.isWhiteBalanceModeSupported(.continuousAutoWhiteBalance) {
                device.whiteBalanceMode = .continuousAutoWhiteBalance
            }
            
            device.unlockForConfiguration()
        } catch {
            print("Error optimizing video capture: \(error)")
        }
    }
    
    private func configureTintSettings() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            if device.isWhiteBalanceModeSupported(.locked) {
                device.whiteBalanceMode = .locked
                
                // Get current white balance gains
                let currentGains = device.deviceWhiteBalanceGains
                var newGains = currentGains
                
                // Calculate tint scale factor (-1.0 to 1.0)
                let tintScale = currentTint / 150.0 // Normalize to -1.0 to 1.0
                
                if tintScale > 0 {
                    // Green tint: increase green gain
                    newGains.greenGain = currentGains.greenGain * (1.0 + Float(tintScale))
                } else {
                    // Magenta tint: increase red and blue gains
                    let magentaScale = 1.0 + Float(abs(tintScale))
                    newGains.redGain = currentGains.redGain * magentaScale
                    newGains.blueGain = currentGains.blueGain * magentaScale
                }
                
                // Clamp gains to valid range
                let maxGain = device.maxWhiteBalanceGain
                newGains.redGain = min(max(1.0, newGains.redGain), maxGain)
                newGains.greenGain = min(max(1.0, newGains.greenGain), maxGain)
                newGains.blueGain = min(max(1.0, newGains.blueGain), maxGain)
                
                // Set the white balance gains
                device.setWhiteBalanceModeLocked(with: newGains) { _ in }
            }
            device.unlockForConfiguration()
        } catch {
            print("Error setting tint: \(error.localizedDescription)")
            self.error = .whiteBalanceError
        }
    }
    
    // Add this function to be called when tint slider changes
    func updateTint(_ newValue: Double) {
        currentTint = newValue.clamped(to: tintRange)
        configureTintSettings()
    }
    
    // Convert shutter speed to angle
    var shutterAngle: Double {
        get {
            // Calculate the actual angle from the current shutter speed and frame rate
            let angle = Double(shutterSpeed.value) / Double(shutterSpeed.timescale) * selectedFrameRate * 360.0
            let clampedAngle = min(max(angle, 1.1), 360.0)
            
            print("ðŸ“ Getting Shutter Angle:")
            print("  - Raw Angle: \(angle)Â°")
            print("  - Clamped Angle: \(clampedAngle)Â°")
            print("  - Current FPS: \(selectedFrameRate)")
            print("  - Shutter Speed: 1/\(1.0/shutterSpeed.seconds)")
            
            return clampedAngle
        }
        set {
            let clampedAngle = min(max(newValue, 1.1), 360.0)
            
            // Calculate the exact shutter duration needed for this angle
            // duration = (angle/360) * (1/fps)
            let duration = (clampedAngle/360.0) * (1.0/selectedFrameRate)
            
            print("ðŸ”„ Setting Shutter Angle:")
            print("  - Requested Angle: \(newValue)Â°")
            print("  - Clamped Angle: \(clampedAngle)Â°")
            print("  - Calculated Duration: \(duration)")
            
            let time = CMTimeMakeWithSeconds(duration, preferredTimescale: 1000000)
            
            // Update the camera settings first
            updateShutterSpeed(time)
            
            // Then update our stored value
            DispatchQueue.main.async {
                self.shutterSpeed = time
            }
        }
    }
    
    // Update method to handle angle directly
    func updateShutterAngle(_ angle: Double) {
        print("\nðŸŽ¯ Updating Shutter Angle:")
        print("  - Requested Angle: \(angle)Â°")
        
        // Set the shutter angle through the property
        self.shutterAngle = angle
    }
    
    // Add this method to handle exposure mode changes
    private func updateExposureMode() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            
            if isAutoExposureEnabled {
                if device.isExposureModeSupported(.continuousAutoExposure) {
                    device.exposureMode = .continuousAutoExposure
                    print("ðŸ“· Auto exposure enabled")
                }
            } else {
                if device.isExposureModeSupported(.custom) {
                    device.exposureMode = .custom
                    // Maintain current exposure settings when switching to manual
                    device.setExposureModeCustom(duration: device.exposureDuration,
                                               iso: device.iso) { _ in }
                    print("ðŸ“· Manual exposure enabled")
                }
            }
            
            device.unlockForConfiguration()
        } catch {
            print("âŒ Error setting exposure mode: \(error.localizedDescription)")
            self.error = .configurationFailed
        }
    }
    
    func processVideoFrame(_ sampleBuffer: CMSampleBuffer) -> CIImage? {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return nil }
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        if let lutFilter = lutManager.currentLUTFilter {
            lutFilter.setValue(ciImage, forKey: kCIInputImageKey)
            if let outputImage = lutFilter.outputImage {
                ciImage = outputImage
            }
        }
        
        return ciImage
    }
}

// MARK: - Sample Buffer Delegate
extension CameraViewModel: AVCaptureFileOutputRecordingDelegate {
    func fileOutput(_ output: AVCaptureFileOutput,
                   didStartRecordingTo fileURL: URL,
                   from connections: [AVCaptureConnection]) {
        print("Recording started successfully")
    }
    
    func fileOutput(_ output: AVCaptureFileOutput,
                   didFinishRecordingTo outputFileURL: URL,
                   from connections: [AVCaptureConnection],
                   error: Error?) {
        isRecording = false
        
        if let error = error {
            print("Recording failed: \(error.localizedDescription)")
            DispatchQueue.main.async {
                self.error = .recordingFailed
                self.isProcessingRecording = false
            }
            return
        }
        
        // Save to photo library
        PHPhotoLibrary.requestAuthorization { [weak self] status in
            guard status == .authorized else {
                DispatchQueue.main.async {
                    self?.error = .savingFailed
                    self?.isProcessingRecording = false
                    print("Photo library access denied")
                }
                return
            }
            
            PHPhotoLibrary.shared().performChanges({
                let options = PHAssetResourceCreationOptions()
                options.shouldMoveFile = true
                let creationRequest = PHAssetCreationRequest.forAsset()
                creationRequest.addResource(with: .video,
                                          fileURL: outputFileURL,
                                          options: options)
            }) { success, error in
                DispatchQueue.main.async {
                    if success {
                        print("Video saved to photo library")
                        self?.recordingFinished = true
                    } else {
                        print("Error saving video: \(String(describing: error))")
                        self?.error = .savingFailed
                    }
                    self?.isProcessingRecording = false
                }
            }
        }
    }
}

// MARK: - Orientation Helper
extension UIDeviceOrientation {
    var videoTransform: CGAffineTransform {
        switch self {
        case .landscapeRight:
            return CGAffineTransform(rotationAngle: CGFloat.pi)
        case .portraitUpsideDown:
            return CGAffineTransform(rotationAngle: -CGFloat.pi / 2)
        case .landscapeLeft:
            return .identity
        case .portrait:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        case .unknown, .faceUp, .faceDown:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        @unknown default:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        }
    }
}

// Add extension to AVFrameRateRange
extension AVFrameRateRange {
    func containsFrameRate(_ fps: Double) -> Bool {
        return fps >= minFrameRate && fps <= maxFrameRate
    }
}

private extension Double {
    func clamped(to range: ClosedRange<Double>) -> Double {
        return min(max(self, range.lowerBound), range.upperBound)
    }
}

================
File: camera/Features/Camera/DocumentPicker.swift
================
import SwiftUI
import UniformTypeIdentifiers

struct DocumentPicker: UIViewControllerRepresentable {
    let types: [UTType]
    let onPick: (URL) -> Void
    
    func makeUIViewController(context: Context) -> UIDocumentPickerViewController {
        let picker = UIDocumentPickerViewController(forOpeningContentTypes: types)
        picker.delegate = context.coordinator
        picker.allowsMultipleSelection = false
        return picker
    }
    
    func updateUIViewController(_ uiViewController: UIDocumentPickerViewController, context: Context) {}
    
    func makeCoordinator() -> Coordinator {
        Coordinator(onPick: onPick)
    }
    
    class Coordinator: NSObject, UIDocumentPickerDelegate {
        let onPick: (URL) -> Void
        
        init(onPick: @escaping (URL) -> Void) {
            self.onPick = onPick
        }
        
        func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
            guard let url = urls.first else { return }
            onPick(url)
        }
    }
}

================
File: camera/Features/Camera/ShutterAngle.swift
================
import Foundation

enum ShutterAngle: Double, CaseIterable {
    case angle_360 = 360.0  // 1/24
    case angle_345_6 = 345.6  // 1/25
    case angle_288 = 288.0  // 1/30
    case angle_262_2 = 262.2  // 1/33
    case angle_180 = 180.0  // 1/48
    case angle_172_8 = 172.8  // 1/50
    case angle_144 = 144.0  // 1/60
    case angle_90 = 90.0   // 1/96
    case angle_86_4 = 86.4  // 1/100
    case angle_72 = 72.0   // 1/120
    case angle_69_1 = 69.1  // 1/125
    case angle_34_6 = 34.6  // 1/250
    case angle_17_3 = 17.3  // 1/500
    case angle_8_6 = 8.6   // 1/1000
    case angle_4_3 = 4.3   // 1/2000
    case angle_2_2 = 2.2   // 1/4000
    case angle_1_1 = 1.1   // 1/8000
    
    var shutterSpeed: String {
        switch self {
        case .angle_360: return "1/24"
        case .angle_345_6: return "1/25"
        case .angle_288: return "1/30"
        case .angle_262_2: return "1/33"
        case .angle_180: return "1/48"
        case .angle_172_8: return "1/50"
        case .angle_144: return "1/60"
        case .angle_90: return "1/96"
        case .angle_86_4: return "1/100"
        case .angle_72: return "1/120"
        case .angle_69_1: return "1/125"
        case .angle_34_6: return "1/250"
        case .angle_17_3: return "1/500"
        case .angle_8_6: return "1/1000"
        case .angle_4_3: return "1/2000"
        case .angle_2_2: return "1/4000"
        case .angle_1_1: return "1/8000"
        }
    }
}

================
File: camera/Features/Camera/VideoOutputDelegate.swift
================
import AVFoundation
import CoreImage

class VideoOutputDelegate: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    let lutManager: LUTManager
    let viewModel: CameraViewModel
    let context = CIContext()
    
    init(lutManager: LUTManager, viewModel: CameraViewModel) {
        self.lutManager = lutManager
        self.viewModel = viewModel
        super.init()
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        let ciImage = CIImage(cvPixelBuffer: imageBuffer)
        
        // Process the image through our pipeline
        print("ðŸŽ¥ Processing image pipeline:")
        print("  - Input image: \(ciImage)")
        
        var processedImage = ciImage
        
        if viewModel.isAppleLogEnabled {
            print("  - Apple Log enabled, converting...")
            // Apple Log processing is handled by the camera format
        }
        
        if lutManager.currentLUTFilter != nil {
            print("  - Applying LUT filter...")
            if let lutImage = lutManager.applyLUT(to: processedImage) {
                processedImage = lutImage
                print("  âœ… LUT applied successfully")
            } else {
                print("  âŒ Failed to apply LUT")
            }
        } else {
            print("  â„¹ï¸ No LUT filter active")
        }
        
        print("  - Final output image: \(processedImage)")
        
        // Render the processed image back to the pixel buffer
        if let pixelBuffer = imageBuffer as CVPixelBuffer? {
            context.render(processedImage, to: pixelBuffer)
        }
    }
}

================
File: camera/Features/LUT/LUTManager.swift
================
import Foundation
import CoreImage
import UniformTypeIdentifiers

class LUTManager: ObservableObject {
    @Published var selectedLUTURL: URL?
    @Published var availableLUTs: [URL] = []
    @Published var currentLUTFilter: CIFilter?
    
    static let supportedTypes: [UTType] = {
        if let cubeType = UTType(tag: "cube",
                                tagClass: .filenameExtension,
                                conformingTo: .data) {
            return [cubeType]
        }
        return [.data]
    }()
    
    func loadLUT(from url: URL) {
        guard url.pathExtension.lowercased() == "cube" else {
            print("âŒ Invalid file type. Only .cube files are supported")
            return
        }
        
        do {
            guard url.startAccessingSecurityScopedResource() else {
                print("âŒ Failed to access security scoped resource")
                return
            }
            
            defer {
                url.stopAccessingSecurityScopedResource()
            }
            
            print("ðŸŽ¨ Loading LUT from: \(url.lastPathComponent)")
            let lutData = try Data(contentsOf: url)
            
            if let lutFilter = CIFilter(name: "CIColorCubeWithColorSpace") {
                lutFilter.setValue(lutData, forKey: "inputCubeData")
                
                if let colorSpace = CGColorSpace(name: CGColorSpace.itur_2100_HLG) {
                    lutFilter.setValue(colorSpace, forKey: "inputColorSpace")
                    print("âœ… Set color space to HLG for Apple Log")
                } else {
                    print("âš ï¸ Failed to set HLG color space, falling back to extended sRGB")
                    if let srgbSpace = CGColorSpace(name: CGColorSpace.extendedSRGB) {
                        lutFilter.setValue(srgbSpace, forKey: "inputColorSpace")
                        print("âœ… Set fallback color space to extended sRGB")
                    }
                }
                
                DispatchQueue.main.async {
                    self.selectedLUTURL = url
                    self.currentLUTFilter = lutFilter
                    print("âœ… LUT loaded successfully: \(url.lastPathComponent)")
                }
            } else {
                print("âŒ Failed to create color cube filter")
            }
        } catch {
            print("âŒ Failed to load LUT: \(error.localizedDescription)")
        }
    }
    
    func clearLUT() {
        DispatchQueue.main.async {
            print("ðŸ§¹ Clearing LUT")
            self.selectedLUTURL = nil
            self.currentLUTFilter = nil
            print("âœ… LUT cleared successfully")
        }
    }
    
    func applyLUT(to image: CIImage) -> CIImage? {
        guard let filter = currentLUTFilter else {
            print("âš ï¸ No LUT filter available to apply")
            return nil
        }
        
        print("ðŸŽ¨ Applying LUT to image")
        filter.setValue(image, forKey: kCIInputImageKey)
        
        if let outputImage = filter.outputImage {
            print("âœ… LUT applied successfully")
            return outputImage
        } else {
            print("âŒ Failed to apply LUT")
            return nil
        }
    }
}

================
File: camera/Features/Settings/SettingsModel.swift
================
import Foundation
import AVFoundation
import CoreMedia

class SettingsModel: ObservableObject {
    @Published var isAppleLogEnabled: Bool {
        didSet {
            UserDefaults.standard.set(isAppleLogEnabled, forKey: "isAppleLogEnabled")
            NotificationCenter.default.post(name: .appleLogSettingChanged, object: nil)
        }
    }
    
    var isAppleLogSupported: Bool {
        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
            return false
        }
        
        // Check if any format supports Apple Log
        return device.formats.contains { format in
            let colorSpaces = format.supportedColorSpaces.map { $0.rawValue }
            return colorSpaces.contains(AVCaptureColorSpace.appleLog.rawValue)
        }
    }
    
    init() {
        self.isAppleLogEnabled = UserDefaults.standard.bool(forKey: "isAppleLogEnabled")
    }
}

extension Notification.Name {
    static let appleLogSettingChanged = Notification.Name("appleLogSettingChanged")
}

================
File: camera/Features/Settings/SettingsView.swift
================
import SwiftUI
import UniformTypeIdentifiers

struct SettingsView: View {
    @Environment(\.dismiss) private var dismiss
    @ObservedObject var lutManager: LUTManager
    @StateObject private var documentDelegate: LUTDocumentPickerDelegate
    
    init(lutManager: LUTManager) {
        self.lutManager = lutManager
        // Initialize the document delegate with the LUT manager
        _documentDelegate = StateObject(wrappedValue: LUTDocumentPickerDelegate(lutManager: lutManager))
    }
    
    var body: some View {
        NavigationView {
            Form {
                Section(header: Text("Camera Settings")) {
                    // Other settings can go here
                }
                
                Section(header: Text("LUT Settings")) {
                    if let selectedLUT = lutManager.selectedLUTURL?.lastPathComponent {
                        HStack {
                            Text("Current LUT")
                            Spacer()
                            Text(selectedLUT)
                                .foregroundColor(.secondary)
                        }
                        
                        Button("Clear LUT") {
                            lutManager.clearLUT()
                        }
                        .foregroundColor(.red)
                    }
                    
                    Button("Import LUT") {
                        showDocumentPicker()
                    }
                }
            }
            .navigationTitle("Settings")
            .navigationBarItems(trailing: Button("Done") {
                dismiss()
            })
        }
    }
    
    private func showDocumentPicker() {
        let documentPicker = UIDocumentPickerViewController(
            forOpeningContentTypes: LUTManager.supportedTypes,
            asCopy: true
        )
        documentPicker.delegate = documentDelegate
        
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene,
           let window = windowScene.windows.first,
           let rootViewController = window.rootViewController {
            rootViewController.present(documentPicker, animated: true)
        }
    }
}

// Document picker delegate as a separate class
class LUTDocumentPickerDelegate: NSObject, UIDocumentPickerDelegate, ObservableObject {
    let lutManager: LUTManager
    
    init(lutManager: LUTManager) {
        self.lutManager = lutManager
        super.init()
    }
    
    func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
        guard let selectedURL = urls.first else { return }
        lutManager.loadLUT(from: selectedURL)
    }
}

#Preview {
    SettingsView(lutManager: LUTManager())
}

================
File: camera/Preview Content/Preview Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/cameraApp.swift
================
//
//  cameraApp.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import SwiftUI

@main
struct cameraApp: App {
    let persistenceController = PersistenceController.shared

    var body: some Scene {
        WindowGroup {
            ContentView()
                .environment(\.managedObjectContext, persistenceController.container.viewContext)
        }
    }
}

================
File: camera/ContentView.swift
================
import SwiftUI
import CoreData
import CoreMedia

struct ContentView: View {
    @StateObject private var viewModel = CameraViewModel()
    @State private var orientation = UIDevice.current.orientation
    @StateObject private var lutManager = LUTManager()
    @State private var isShowingSettings = false
    @State private var isShowingDocumentPicker = false
    
    var body: some View {
        GeometryReader { geometry in
            ZStack {
                if viewModel.isSessionRunning {
                    CameraPreviewView(
                        session: viewModel.session,
                        lutManager: lutManager,
                        viewModel: viewModel
                    )
                    .ignoresSafeArea()
                    .frame(width: geometry.size.width,
                           height: geometry.size.height)
                    
                    VStack {
                        Spacer()
                        controlsView
                            .frame(maxWidth: geometry.size.width * 0.9)
                            .padding(.bottom, 30)
                    }
                } else {
                    // Loading indicator while camera session initializes
                    ProgressView("Initializing Camera...")
                        .frame(maxWidth: .infinity, maxHeight: .infinity)
                        .background(Color.black.opacity(0.7))
                }
            }
            .edgesIgnoringSafeArea(.all)
            .onRotate { newOrientation in
                orientation = newOrientation
            }
        }
        .onAppear {
            viewModel.updateInterfaceOrientation()
        }
        .onChange(of: UIDevice.current.orientation) { oldValue, newValue in
            viewModel.updateInterfaceOrientation()
        }
        .alert(item: $viewModel.error) { error in
            Alert(title: Text("Error"),
                  message: Text(error.description),
                  dismissButton: .default(Text("OK")))
        }
        .sheet(isPresented: $isShowingSettings) {
            SettingsView(lutManager: lutManager)
        }
        .sheet(isPresented: $isShowingDocumentPicker) {
            DocumentPicker(types: LUTManager.supportedTypes) { url in
                lutManager.loadLUT(from: url)
                isShowingDocumentPicker = false
            }
        }
    }
    
    // Camera controls
    private var controlsView: some View {
        VStack(spacing: 15) {
            Text("Camera Controls")
                .font(.headline)
            
            // Frame Rate Picker
            HStack {
                Text("FPS:")
                Picker("Frame Rate", selection: $viewModel.selectedFrameRate) {
                    ForEach(viewModel.availableFrameRates, id: \.self) { fps in
                        Text(fps == 29.97 ? "29.97" : String(format: "%.0f", fps))
                            .tag(fps)
                    }
                }
                .pickerStyle(.menu)
                .onChange(of: viewModel.selectedFrameRate) { oldValue, newValue in
                    viewModel.updateFrameRate(newValue)
                }
            }
            
            // White Balance
            HStack {
                Text("WB: \(Int(viewModel.whiteBalance))K")
                Slider(value: $viewModel.whiteBalance,
                       in: 2000...8000,
                       step: 100) { _ in
                    viewModel.updateWhiteBalance(viewModel.whiteBalance)
                }
            }
            
            // Tint Control
            HStack {
                Text("Tint: \(Int(viewModel.currentTint))")
                Slider(
                    value: $viewModel.currentTint,
                    in: -150...150,
                    step: 1
                ) { _ in
                    viewModel.updateTint(viewModel.currentTint)
                }
                .tint(.green)
            }
            
            // ISO
            HStack {
                Text("ISO: \(Int(viewModel.iso))")
                Slider(value: $viewModel.iso,
                       in: viewModel.minISO...viewModel.maxISO,
                       step: 1) { _ in
                    viewModel.updateISO(viewModel.iso)
                }
            }
            .disabled(viewModel.isAutoExposureEnabled)
            .opacity(viewModel.isAutoExposureEnabled ? 0.6 : 1.0)
            
            // Shutter
            HStack {
                let currentAngle = viewModel.shutterAngle
                Text("Shutter: \(Int(currentAngle))Â° (\(ShutterAngle(rawValue: currentAngle)?.shutterSpeed ?? "Custom"))")
                
                Picker("Shutter Angle", selection: Binding(
                    get: {
                        // Find the closest standard angle
                        ShutterAngle.allCases.min(by: { abs($0.rawValue - viewModel.shutterAngle) < abs($1.rawValue - viewModel.shutterAngle) })?.rawValue ?? 180.0
                    },
                    set: { newValue in
                        print("\nðŸŽšï¸ Shutter Angle Changed:")
                        print("  - New Value: \(newValue)Â°")
                        viewModel.updateShutterAngle(newValue)
                    }
                )) {
                    ForEach(ShutterAngle.allCases, id: \.rawValue) { angle in
                        Text("\(Int(angle.rawValue))Â° (\(angle.shutterSpeed))")
                            .tag(angle.rawValue)
                    }
                }
                .pickerStyle(.menu)
            }
            .disabled(viewModel.isAutoExposureEnabled)
            .opacity(viewModel.isAutoExposureEnabled ? 0.6 : 1.0)
            
            // LUT Controls
            VStack(spacing: 8) {
                Toggle(isOn: Binding(
                    get: { lutManager.currentLUTFilter != nil },
                    set: { enabled in
                        if !enabled {
                            lutManager.clearLUT()
                        }
                    }
                )) {
                    HStack {
                        Text("LUT Preview")
                        if lutManager.currentLUTFilter != nil {
                            Image(systemName: "checkmark.circle.fill")
                                .foregroundColor(.green)
                        }
                    }
                }
                .tint(.green)
                
                if let lutName = lutManager.selectedLUTURL?.lastPathComponent {
                    HStack {
                        Text(lutName)
                            .font(.caption)
                            .foregroundColor(.secondary)
                        Spacer()
                        Button(action: {
                            lutManager.clearLUT()
                        }) {
                            Image(systemName: "xmark.circle.fill")
                                .foregroundColor(.red)
                        }
                    }
                }
                
                Button(action: {
                    isShowingDocumentPicker = true
                }) {
                    HStack {
                        Image(systemName: "photo.fill")
                        Text("Import LUT")
                    }
                    .frame(maxWidth: .infinity)
                    .padding(.vertical, 8)
                }
                .buttonStyle(.bordered)
                .tint(.blue)
            }
            .padding(.vertical, 4)
            
            // Auto Exposure toggle
            Toggle(isOn: $viewModel.isAutoExposureEnabled) {
                HStack {
                    Text("Auto Exposure")
                    if viewModel.isAutoExposureEnabled {
                        Image(systemName: "a.circle.fill")
                            .foregroundColor(.green)
                    } else {
                        Image(systemName: "m.circle.fill")
                            .foregroundColor(.orange)
                    }
                }
            }
            .tint(.green)
            
            // Apple Log toggle if supported
            if viewModel.isAppleLogSupported {
                Toggle(isOn: $viewModel.isAppleLogEnabled) {
                    HStack {
                        Text("Apple Log (4K ProRes)")
                        if viewModel.isAppleLogEnabled {
                            Image(systemName: "checkmark.circle.fill")
                                .foregroundColor(.green)
                        }
                    }
                }
                .tint(.green)
            }
            
            // Record button
            Button(action: {
                if viewModel.isRecording {
                    viewModel.stopRecording()
                } else {
                    viewModel.startRecording()
                }
            }) {
                Image(systemName: viewModel.isRecording ? "stop.circle" : "record.circle")
                    .font(.system(size: 60))
                    .foregroundColor(viewModel.isRecording ? .white : .red)
                    .opacity(viewModel.isProcessingRecording ? 0.5 : 1.0)
            }
            .disabled(viewModel.isProcessingRecording)
        }
        .padding()
        .background(Color.black.opacity(0.6))
        .cornerRadius(15)
        .foregroundColor(.white)
    }
}

// A rotation view modifier to track device orientation changes
struct DeviceRotationViewModifier: ViewModifier {
    let action: (UIDeviceOrientation) -> Void
    
    func body(content: Content) -> some View {
        content
            .onAppear()
            .onReceive(NotificationCenter.default.publisher(for: UIDevice.orientationDidChangeNotification)) { _ in
                action(UIDevice.current.orientation)
            }
    }
}

extension View {
    func onRotate(perform action: @escaping (UIDeviceOrientation) -> Void) -> some View {
        self.modifier(DeviceRotationViewModifier(action: action))
    }
}

================
File: camera/Info.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIApplicationSceneManifest</key>
	<dict>
		<key>UIApplicationSupportsMultipleScenes</key>
		<false/>
	</dict>
	<key>NSCameraUsageDescription</key>
	<string>This app needs camera access to record video</string>
	<key>NSMicrophoneUsageDescription</key>
	<string>This app needs microphone access to record audio</string>
	<key>LSSupportsOpeningDocumentsInPlace</key>
	<true/>
	<key>UIFileSharingEnabled</key>
	<true/>
</dict>
</plist>

================
File: camera/Persistence.swift
================
//
//  Persistence.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import CoreData

struct PersistenceController {
    static let shared = PersistenceController()

    @MainActor
    static let preview: PersistenceController = {
        let result = PersistenceController(inMemory: true)
        let viewContext = result.container.viewContext
        for _ in 0..<10 {
            let newItem = Item(context: viewContext)
            newItem.timestamp = Date()
        }
        do {
            try viewContext.save()
        } catch {
            // Replace this implementation with code to handle the error appropriately.
            // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.
            let nsError = error as NSError
            fatalError("Unresolved error \(nsError), \(nsError.userInfo)")
        }
        return result
    }()

    let container: NSPersistentContainer

    init(inMemory: Bool = false) {
        container = NSPersistentContainer(name: "camera")
        if inMemory {
            container.persistentStoreDescriptions.first!.url = URL(fileURLWithPath: "/dev/null")
        }
        container.loadPersistentStores(completionHandler: { (storeDescription, error) in
            if let error = error as NSError? {
                // Replace this implementation with code to handle the error appropriately.
                // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.

                /*
                 Typical reasons for an error here include:
                 * The parent directory does not exist, cannot be created, or disallows writing.
                 * The persistent store is not accessible, due to permissions or data protection when the device is locked.
                 * The device is out of space.
                 * The store could not be migrated to the current model version.
                 Check the error message to determine what the actual problem was.
                 */
                fatalError("Unresolved error \(error), \(error.userInfo)")
            }
        })
        container.viewContext.automaticallyMergesChangesFromParent = true
    }
}

================
File: camera.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcdebugger/Breakpoints_v2.xcbkptlist
================
<?xml version="1.0" encoding="UTF-8"?>
<Bucket
   uuid = "8C0EC471-D08D-4370-AFA9-6904E3CF5246"
   type = "1"
   version = "2.0">
</Bucket>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcschemes/xcschememanagement.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>SchemeUserState</key>
	<dict>
		<key>camera.xcscheme_^#shared#^_</key>
		<dict>
			<key>orderHint</key>
			<integer>0</integer>
		</dict>
	</dict>
</dict>
</plist>

================
File: camera.xcodeproj/project.pbxproj
================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXFileReference section */
		09562D7C2D18A1EC009A9B07 /* camera.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = camera.app; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = 09562D7B2D18A1EC009A9B07 /* camera */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		09562D7E2D18A1EC009A9B07 /* camera */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */,
			);
			path = camera;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		09562D792D18A1EC009A9B07 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		09562D732D18A1EC009A9B07 = {
			isa = PBXGroup;
			children = (
				09562D7E2D18A1EC009A9B07 /* camera */,
				09562D7D2D18A1EC009A9B07 /* Products */,
			);
			sourceTree = "<group>";
		};
		09562D7D2D18A1EC009A9B07 /* Products */ = {
			isa = PBXGroup;
			children = (
				09562D7C2D18A1EC009A9B07 /* camera.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		09562D7B2D18A1EC009A9B07 /* camera */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */;
			buildPhases = (
				09562D782D18A1EC009A9B07 /* Sources */,
				09562D792D18A1EC009A9B07 /* Frameworks */,
				09562D7A2D18A1EC009A9B07 /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				09562D7E2D18A1EC009A9B07 /* camera */,
			);
			name = camera;
			packageProductDependencies = (
			);
			productName = camera;
			productReference = 09562D7C2D18A1EC009A9B07 /* camera.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		09562D742D18A1EC009A9B07 /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1620;
				LastUpgradeCheck = 1620;
				TargetAttributes = {
					09562D7B2D18A1EC009A9B07 = {
						CreatedOnToolsVersion = 16.2;
					};
				};
			};
			buildConfigurationList = 09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 09562D732D18A1EC009A9B07;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = 09562D7D2D18A1EC009A9B07 /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				09562D7B2D18A1EC009A9B07 /* camera */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		09562D7A2D18A1EC009A9B07 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		09562D782D18A1EC009A9B07 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		09562D8D2D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		09562D8E2D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		09562D902D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		09562D912D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D8D2D18A1EE009A9B07 /* Debug */,
				09562D8E2D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D902D18A1EE009A9B07 /* Debug */,
				09562D912D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = 09562D742D18A1EC009A9B07 /* Project object */;
}
