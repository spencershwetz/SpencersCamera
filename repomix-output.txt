This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-04T01:25:20.318Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
camera/
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    Contents.json
  camera.xcdatamodeld/
    camera.xcdatamodel/
      contents
    .xccurrentversion
  Core/
    Extensions/
      CIContext+Shared.swift
  Features/
    Camera/
      CameraError.swift
      CameraPreviewView.swift
      CameraViewModel.swift
      DocumentPicker.swift
      ShutterAngle.swift
      VideoOutputDelegate.swift
    LUT/
      CubeLUTLoader.swift
      LUTManager.swift
    Settings/
      SettingsModel.swift
      SettingsView.swift
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  cameraApp.swift
  ContentView.swift
  Info.plist
  My3DLUTFile.cube
  Persistence.swift
  TestLUT.cube
camera.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcuserdata/
    spencer.xcuserdatad/
      xcdebugger/
        Breakpoints_v2.xcbkptlist
      xcschemes/
        xcschememanagement.plist
  project.pbxproj

================================================================
Repository Files
================================================================

================
File: camera/Assets.xcassets/AccentColor.colorset/Contents.json
================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/AppIcon.appiconset/Contents.json
================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/camera.xcdatamodeld/camera.xcdatamodel/contents
================
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<model type="com.apple.IDECoreDataModeler.DataModel" documentVersion="1.0" lastSavedToolsVersion="1" systemVersion="11A491" minimumToolsVersion="Automatic" sourceLanguage="Swift" usedWithCloudKit="false" userDefinedModelVersionIdentifier="">
    <entity name="Item" representedClassName="Item" syncable="YES" codeGenerationType="class">
        <attribute name="timestamp" optional="YES" attributeType="Date" usesScalarValueType="NO"/>
    </entity>
    <elements>
        <element name="Item" positionX="-63" positionY="-18" width="128" height="44"/>
    </elements>
</model>

================
File: camera/camera.xcdatamodeld/.xccurrentversion
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>_XCCurrentVersionName</key>
	<string>camera.xcdatamodel</string>
</dict>
</plist>

================
File: camera/Core/Extensions/CIContext+Shared.swift
================
import CoreImage

extension CIContext {
    static let shared: CIContext = {
        let options = [
            CIContextOption.workingColorSpace: CGColorSpace(name: CGColorSpace.displayP3)!,
            CIContextOption.useSoftwareRenderer: false
        ]
        return CIContext(options: options)
    }()
}

================
File: camera/Features/Camera/CameraError.swift
================
import Foundation

enum CameraError: Error, Identifiable {
    case cameraUnavailable
    case setupFailed
    case configurationFailed
    case recordingFailed
    case savingFailed
    case whiteBalanceError
    case custom(message: String)
    
    var id: String { description }
    
    var description: String {
        switch self {
        case .cameraUnavailable:
            return "Camera device not available"
        case .setupFailed:
            return "Failed to setup camera"
        case .configurationFailed:
            return "Failed to configure camera settings"
        case .recordingFailed:
            return "Failed to record video"
        case .savingFailed:
            return "Failed to save video to photo library"
        case .whiteBalanceError:
            return "Failed to adjust white balance settings"
        case .custom(let message):
            return message
        }
    }
}

================
File: camera/Features/Camera/CameraPreviewView.swift
================
import SwiftUI
import AVFoundation

struct CameraPreviewView: UIViewRepresentable {
    let session: AVCaptureSession
    let lutManager: LUTManager
    let viewModel: CameraViewModel
    
    class PreviewView: UIView {
        override class var layerClass: AnyClass {
            AVCaptureVideoPreviewLayer.self
        }
        
        var videoPreviewLayer: AVCaptureVideoPreviewLayer {
            return layer as! AVCaptureVideoPreviewLayer
        }
        
        var videoOutput: AVCaptureVideoDataOutput?
        
        func setupVideoOutput(session: AVCaptureSession, delegate: AVCaptureVideoDataOutputSampleBufferDelegate) {
            // Remove any existing output
            if let existingOutput = videoOutput {
                session.removeOutput(existingOutput)
            }
            
            // Create and configure video output
            let output = AVCaptureVideoDataOutput()
            output.setSampleBufferDelegate(delegate, queue: DispatchQueue(label: "videoQueue"))
            
            if session.canAddOutput(output) {
                session.addOutput(output)
                videoOutput = output
                print("✅ Video output added to session")
            } else {
                print("❌ Could not add video output to session")
            }
        }
    }
    
    func makeUIView(context: Context) -> PreviewView {
        let view = PreviewView()
        view.videoPreviewLayer.session = session
        view.videoPreviewLayer.videoGravity = .resizeAspect
        
        // Create and set up video output delegate
        let videoDelegate = VideoOutputDelegate(lutManager: lutManager, viewModel: viewModel, context: CIContext.shared)
        view.setupVideoOutput(session: session, delegate: videoDelegate)
        
        // Initial orientation setup
        updatePreviewLayerOrientation(view.videoPreviewLayer)
        
        // Add orientation change observer
        NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { _ in
                updatePreviewLayerOrientation(view.videoPreviewLayer)
            }
        
        return view
    }
    
    func updateUIView(_ uiView: PreviewView, context: Context) {
        CATransaction.begin()
        CATransaction.setAnimationDuration(0.25)
        updatePreviewLayerOrientation(uiView.videoPreviewLayer)
        CATransaction.commit()
    }
    
    private func updatePreviewLayerOrientation(_ layer: AVCaptureVideoPreviewLayer) {
        guard let connection = layer.connection else { return }
        
        let currentDevice = UIDevice.current
        let orientation = currentDevice.orientation
        
        if #available(iOS 17.0, *) {
            switch orientation {
            case .portrait:
                connection.videoRotationAngle = 90
            case .landscapeRight: // Device rotated left
                connection.videoRotationAngle = 180
            case .landscapeLeft: // Device rotated right
                connection.videoRotationAngle = 0
            case .portraitUpsideDown:
                connection.videoRotationAngle = 270
            default:
                connection.videoRotationAngle = 90
            }
        } else {
            switch orientation {
            case .portrait:
                connection.videoOrientation = .portrait
            case .landscapeRight: // Device rotated left
                connection.videoOrientation = .landscapeLeft
            case .landscapeLeft: // Device rotated right
                connection.videoOrientation = .landscapeRight
            case .portraitUpsideDown:
                connection.videoOrientation = .portraitUpsideDown
            default:
                connection.videoOrientation = .portrait
            }
        }
    }
    
    // Add this method to track image processing
    private func processImage(_ image: CIImage) -> CIImage {
        var processedImage = image
        
        // Log the initial state
        print("🎥 Processing image pipeline:")
        print("  - Input image: \(image)")
        
        if viewModel.isAppleLogEnabled {
            print("  - Apple Log enabled, converting...")
            // Apple Log processing here
        }
        
        // Check if LUT should be applied and apply it
        if lutManager.currentLUTFilter != nil {
            print("  - Applying LUT filter...")
            if let lutImage = lutManager.applyLUT(to: processedImage) {
                processedImage = lutImage
                print("  ✅ LUT applied successfully")
            } else {
                print("  ❌ Failed to apply LUT")
            }
        } else {
            print("  ℹ️ No LUT filter active")
        }
        
        print("  - Final output image: \(processedImage)")
        return processedImage
    }
}

================
File: camera/Features/Camera/CameraViewModel.swift
================
import AVFoundation
import SwiftUI
import Photos
import VideoToolbox
import CoreVideo
import os.log
import CoreImage

class CameraViewModel: NSObject, ObservableObject {
    enum Status {
        case unknown
        case running
        case failed
        case unauthorized
    }
    @Published private(set) var status: Status = .unknown
    
    enum CaptureMode {
        case photo
        case video
    }
    @Published var captureMode: CaptureMode = .video
    
    private let logger = Logger(subsystem: "com.camera", category: "CameraViewModel")
    
    @Published var isSessionRunning = false
    @Published var error: CameraError?
    @Published var whiteBalance: Float = 5000 // Kelvin
    @Published var iso: Float = 100
    @Published var shutterSpeed: CMTime = CMTimeMake(value: 1, timescale: 60) // Initialize for 180° at 30fps
    @Published var isRecording = false
    @Published var recordingFinished = false
    @Published var isSettingsPresented = false
    @Published var isProcessingRecording = false
    
    // Enable Apple Log (4K ProRes) by default if device supports it
    @Published var isAppleLogEnabled = false {
        didSet {
            print("\n=== Apple Log Toggle ===")
            print("🔄 Status: \(status)")
            print("📹 Capture Mode: \(captureMode)")
            print("✅ Attempting to set Apple Log to: \(isAppleLogEnabled)")
            
            guard status == .running, captureMode == .video else {
                print("❌ Cannot configure Apple Log - Status or mode incorrect")
                print("Required: status == .running (is: \(status))")
                print("Required: captureMode == .video (is: \(captureMode))")
                return
            }
            
            Task {
                do {
                    if isAppleLogEnabled {
                        print("🎥 Configuring Apple Log...")
                        try await configureAppleLog()
                    } else {
                        print("↩️ Resetting Apple Log...")
                        try await resetAppleLog()
                    }
                } catch {
                    await MainActor.run {
                        self.error = .configurationFailed
                    }
                    logger.error("Failed to configure Apple Log: \(error.localizedDescription)")
                    print("❌ Apple Log configuration failed: \(error)")
                }
            }
            print("=== End Apple Log Toggle ===\n")
        }
    }
    
    @Published private(set) var isAppleLogSupported = false
    
    let session = AVCaptureSession()
    private var device: AVCaptureDevice?
    
    // Add movie file output
    private let movieOutput = AVCaptureMovieFileOutput()
    private var currentRecordingURL: URL?
    
    private var defaultFormat: AVCaptureDevice.Format?
    
    var minISO: Float {
        device?.activeFormat.minISO ?? 50
    }
    var maxISO: Float {
        device?.activeFormat.maxISO ?? 1600
    }
    
    // Add new property for frame rate
    @Published var selectedFrameRate: Double = 30.0
    
    // Add available frame rates
    let availableFrameRates: [Double] = [23.976, 24.0, 25.0, 29.97, 30.0]
    
    private var orientationObserver: NSObjectProtocol?
    
    // Add property to track interface orientation
    @Published private(set) var currentInterfaceOrientation: UIInterfaceOrientation = .portrait
    
    private let processingQueue = DispatchQueue(
        label: "com.camera.processing",
        qos: .userInitiated,
        attributes: [],
        autoreleaseFrequency: .workItem
    )
    
    // Add properties for frame rate monitoring
    private var lastFrameTimestamp: CFAbsoluteTime = 0
    private var lastFrameTime: CMTime?
    private var frameCount: Int = 0
    private var frameRateAccumulator: Double = 0
    private var frameRateUpdateInterval: Int = 30 // Update every 30 frames
    
    // Add property to store supported frame rate range
    private var supportedFrameRateRange: AVFrameRateRange? {
        device?.activeFormat.videoSupportedFrameRateRanges.first
    }
    
    // Add properties for advanced configuration
    private var videoConfiguration: [String: Any] = [
        AVVideoCodecKey: AVVideoCodecType.proRes422,
        AVVideoCompressionPropertiesKey: [
            AVVideoAverageBitRateKey: 50_000_000, // 50 Mbps
            AVVideoMaxKeyFrameIntervalKey: 1, // Every frame is keyframe
            AVVideoAllowFrameReorderingKey: false,
            AVVideoExpectedSourceFrameRateKey: 30
        ]
    ]
    
    // Add these constants
    private struct FrameRates {
        // Non-integer frame rates need precise representation
        // For 23.976 fps, the exact fraction is 24000/1001 ≈ 23.976
        static let ntsc23_976 = CMTime(value: 1001, timescale: 24000)  // 23.976 fps (24000/1001)
        static let ntsc29_97 = CMTime(value: 1001, timescale: 30000)   // 29.97 fps
        static let film24 = CMTime(value: 1, timescale: 24)            // 24 fps
        static let pal25 = CMTime(value: 1, timescale: 25)             // 25 fps
        static let ntsc30 = CMTime(value: 1, timescale: 30)            // 30 fps
    }
    
    @Published var currentTint: Double = 0.0 // Range: -150 to +150
    private let tintRange = (-150.0...150.0)
    
    private var videoDeviceInput: AVCaptureDeviceInput?
    
    @Published var isAutoExposureEnabled: Bool = true {
        didSet {
            updateExposureMode()
        }
    }
    
    // Add LUT Manager
    @Published var lutManager = LUTManager()
    private var ciContext = CIContext()
    
    override init() {
        super.init()
        print("\n=== Camera Initialization ===")
        
        do {
            try setupSession()
            
            // Print device capabilities
            if let device = device {
                print("📊 Device Capabilities:")
                print("- Name: \(device.localizedName)")
                print("- Model ID: \(device.modelID)")
                
                print("\n🎨 Supported Color Spaces:")
                device.formats.forEach { format in
                    let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
                    let codecType = CMFormatDescriptionGetMediaSubType(format.formatDescription)
                    print("""
                        Format: \(dimensions.width)x\(dimensions.height) - Codec: \(codecType)
                        - Color Spaces: \(format.supportedColorSpaces.map { $0.rawValue })
                        - Supports Apple Log: \(format.supportedColorSpaces.contains(.appleLog))
                        - Supports HDR: \(format.isVideoHDRSupported)
                        """)
                }
                
                isAppleLogSupported = device.formats.contains { format in
                    format.supportedColorSpaces.contains(.appleLog)
                }
                print("\n✅ Apple Log Support: \(isAppleLogSupported)")
            }
            print("=== End Initialization ===\n")
            
            // Store default format
            if let device = device {
                defaultFormat = device.activeFormat
            }
            
            // Check Apple Log support
            isAppleLogSupported = device?.formats.contains { format in
                format.supportedColorSpaces.contains(.appleLog)
            } ?? false
        } catch {
            self.error = .setupFailed
            print("Failed to setup session: \(error)")
        }
        
        // Add orientation observer
        orientationObserver = NotificationCenter.default.addObserver(
            forName: UIDevice.orientationDidChangeNotification,
            object: nil,
            queue: .main) { [weak self] _ in
                guard let self = self,
                      let connection = self.movieOutput.connection(with: .video) else { return }
                self.updateVideoOrientation(connection)
        }
        
        // After other initialization, set initial shutter angle to 180°
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.updateShutterAngle(180.0) // Set initial shutter angle to 180°
        }
        
        // Load LUT with enhanced error handling and debugging
        print("📱 LUT Loading: Attempting to load LUT files")
        do {
            // First try the original file
            if let lutPath = Bundle.main.path(forResource: "My3DLUTFile", ofType: "cube") {
                print("📱 Found expected LUT: \(lutPath)")
                lutManager.loadLUT(named: "My3DLUTFile")
            } 
            // Then try our test file
            else if let testLutPath = Bundle.main.path(forResource: "TestLUT", ofType: "cube") {
                print("📱 Found test LUT: \(testLutPath)")
                lutManager.loadLUT(named: "TestLUT")
            }
            // If none of the above work, search for any .cube files
            else {
                print("⚠️ LUT Error: No predefined LUT files found in bundle!")
                
                // Try to find any existing .cube files
                let availableLUTs = Bundle.main.paths(forResourcesOfType: "cube", inDirectory: nil)
                print("📁 Bundle path: \(Bundle.main.bundlePath)")
                print("📁 Available .cube files: \(availableLUTs)")
                
                // Try using an absolute path to our test LUT files
                let fileManager = FileManager.default
                let documentsDirectory = fileManager.urls(for: .documentDirectory, in: .userDomainMask).first!
                let projectDirectory = Bundle.main.bundleURL.deletingLastPathComponent()
                
                let possibleLUTPaths = [
                    Bundle.main.bundleURL.appendingPathComponent("My3DLUTFile.cube"),
                    Bundle.main.bundleURL.appendingPathComponent("TestLUT.cube"),
                    documentsDirectory.appendingPathComponent("My3DLUTFile.cube"),
                    documentsDirectory.appendingPathComponent("TestLUT.cube"),
                    projectDirectory.appendingPathComponent("My3DLUTFile.cube"),
                    projectDirectory.appendingPathComponent("TestLUT.cube")
                ]
                
                print("🔍 Searching for LUTs in alternative locations...")
                for lutPath in possibleLUTPaths {
                    if fileManager.fileExists(atPath: lutPath.path) {
                        print("✅ Found LUT at: \(lutPath.path)")
                        lutManager.loadLUT(from: lutPath)
                        break
                    }
                }
                
                // If still no LUT file found, create a programmatic default
                if lutManager.currentLUTFilter == nil {
                    print("⚠️ No LUT files found. Creating a default programmatic LUT...")
                    createDefaultLUT()
                }
            }
        } catch {
            print("❌ LUT Loading Error: \(error.localizedDescription)")
            print("⚠️ Creating a default programmatic LUT instead...")
            createDefaultLUT()
        }
    }
    
    /// Creates a basic programmatic LUT when no files are available
    private func createDefaultLUT() {
        // Create a simple 2x2x2 LUT programmatically (very basic)
        let dimension = 2
        let data: [Float] = [
            0.0, 0.0, 0.0,  // (0,0,0) -> (0,0,0)
            1.0, 0.9, 0.9,  // (1,0,0) -> slightly warm red
            0.9, 1.0, 0.9,  // (0,1,0) -> slightly cool green
            1.0, 1.0, 0.9,  // (1,1,0) -> warmer yellow
            0.9, 0.9, 1.0,  // (0,0,1) -> slightly cool blue
            1.0, 0.9, 1.0,  // (1,0,1) -> warmer magenta
            0.9, 1.0, 1.0,  // (0,1,1) -> slightly cool cyan
            1.0, 1.0, 1.0   // (1,1,1) -> white (no change)
        ]
        
        print("🎨 Created programmatic LUT: dimension=\(dimension), points=\(data.count/3)")
        lutManager.setupProgrammaticLUT(dimension: dimension, data: data)
    }
    
    deinit {
        if let observer = orientationObserver {
            NotificationCenter.default.removeObserver(observer)
        }
    }
    
    /// Returns a 4K (3840x2160) AppleProRes422 format that also supports Apple Log
    private func findBestAppleLogFormat(_ device: AVCaptureDevice) -> AVCaptureDevice.Format? {
        return device.formats.first { format in
            let desc = format.formatDescription
            let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
            let codecType = CMFormatDescriptionGetMediaSubType(desc)
            
            let is4K = (dimensions.width == 3840 && dimensions.height == 2160)
            let isProRes422 = (codecType == kCMVideoCodecType_AppleProRes422) // 'x422'
            let hasAppleLog = format.supportedColorSpaces.contains(.appleLog)
            
            return is4K && isProRes422 && hasAppleLog
        }
    }
    
    private func configureAppleLog() async throws {
        guard let device = device else {
            print("❌ No camera device available")
            return
        }
        
        print("\n=== Apple Log Configuration ===")
        print("🎥 Current device: \(device.localizedName)")
        print("📊 Current format: \(device.activeFormat.formatDescription)")
        print("🎨 Current color space: \(device.activeColorSpace.rawValue)")
        print("🎨 Wide color enabled: \(session.automaticallyConfiguresCaptureDeviceForWideColor)")
        
        // Ensure wide color is disabled
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        
        // Check if format supports Apple Log
        let supportsAppleLog = device.formats.contains { format in
            format.supportedColorSpaces.contains(.appleLog)
        }
        print("✓ Device supports Apple Log: \(supportsAppleLog)")
        
        do {
            session.stopRunning()
            print("⏸️ Session stopped for reconfiguration")
            
            try await Task.sleep(for: .milliseconds(100))
            session.beginConfiguration()
            
            try device.lockForConfiguration()
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                
                // Fix orientation after configuration
                if let videoConnection = movieOutput.connection(with: .video) {
                    updateVideoOrientation(videoConnection)
                }
                
                session.startRunning()
            }
            
            // Find best Apple Log format
            if let format = device.formats.first(where: {
                let desc = $0.formatDescription
                let dimensions = CMVideoFormatDescriptionGetDimensions(desc)
                let codecType = CMFormatDescriptionGetMediaSubType(desc)
                
                let is4K = (dimensions.width == 3840 && dimensions.height == 2160)
                let isProRes = (codecType == kCMVideoCodecType_AppleProRes422 ||
                              codecType == kCMVideoCodecType_AppleProRes422HQ ||
                              codecType == 2016686642)
                let hasAppleLog = $0.supportedColorSpaces.contains(.appleLog)
                
                print("""
                    Checking format:
                    - Resolution: \(dimensions.width)x\(dimensions.height) (is4K: \(is4K))
                    - Codec: \(codecType) (isProRes: \(isProRes))
                    - Has Apple Log: \(hasAppleLog)
                    """)
                
                return (is4K || dimensions.width >= 1920) && isProRes && hasAppleLog
            }) {
                print("✅ Found suitable Apple Log format")
                print("📹 Format details: \(format.formatDescription)")
                
                let duration = CMTimeMake(value: 1000, timescale: Int32(selectedFrameRate * 1000))
                device.activeVideoMinFrameDuration = duration
                device.activeVideoMaxFrameDuration = duration
                
                device.activeFormat = format
                device.activeColorSpace = .appleLog
                print("🎨 Set color space to Apple Log")
                
                print("✅ Successfully configured Apple Log format")
            } else {
                print("❌ No suitable Apple Log format found")
                throw CameraError.configurationFailed
            }
            
            print("💾 Configuration committed")
            print("▶️ Session restarted")
            
        } catch {
            print("❌ Error configuring Apple Log: \(error.localizedDescription)")
            device.unlockForConfiguration()
            session.commitConfiguration()
            session.startRunning()
            await MainActor.run {
                self.error = .configurationFailed
            }
            print("🔄 Attempting session recovery")
            throw error
        }
        
        print("=== End Apple Log Configuration ===\n")
    }
    
    private func resetAppleLog() async throws {
        guard let device = device else {
            print("❌ No camera device available")
            return
        }
        
        print("\n=== Resetting Apple Log Configuration ===")
        print("🎨 Wide color enabled: \(session.automaticallyConfiguresCaptureDeviceForWideColor)")
        
        // Ensure wide color is disabled
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        
        do {
            session.stopRunning()
            session.beginConfiguration()
            
            try device.lockForConfiguration()
            defer {
                device.unlockForConfiguration()
                session.commitConfiguration()
                
                if let videoConnection = movieOutput.connection(with: .video) {
                    updateVideoOrientation(videoConnection)
                }
                
                session.startRunning()
            }
            
            if let defaultFormat = defaultFormat {
                device.activeFormat = defaultFormat
            }
            device.activeColorSpace = .sRGB
            
            session.commitConfiguration()
            session.startRunning()
            
            print("✅ Successfully reset to sRGB color space")
        } catch {
            print("❌ Error resetting Apple Log: \(error.localizedDescription)")
            self.error = .configurationFailed
            session.startRunning()
        }
        
        print("=== End Reset ===\n")
    }
    
    private func setupSession() throws {
        session.automaticallyConfiguresCaptureDeviceForWideColor = false
        session.beginConfiguration()
        
        guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                    for: .video,
                                                    position: .back) else {
            error = .cameraUnavailable
            status = .failed
            session.commitConfiguration()
            return
        }
        
        self.device = videoDevice
        
        do {
            let input = try AVCaptureDeviceInput(device: videoDevice)
            self.videoDeviceInput = input
            
            // Configure Apple Log if enabled
            if isAppleLogEnabled, let appleLogFormat = findBestAppleLogFormat(videoDevice) {
                let frameRateRange = appleLogFormat.videoSupportedFrameRateRanges.first!
                try videoDevice.lockForConfiguration()
                videoDevice.activeVideoMinFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.maxFrameRate))
                videoDevice.activeVideoMaxFrameDuration = CMTime(value: 1, timescale: Int32(frameRateRange.minFrameRate))
                videoDevice.activeFormat = appleLogFormat
                videoDevice.activeColorSpace = .appleLog
                print("Initial setup: Enabled Apple Log in 4K ProRes format")
            }
            
            if session.canAddInput(input) {
                session.addInput(input)
            }
            
            if let audioDevice = AVCaptureDevice.default(for: .audio),
               let audioInput = try? AVCaptureDeviceInput(device: audioDevice),
               session.canAddInput(audioInput) {
                session.addInput(audioInput)
            }
            
            if session.canAddOutput(movieOutput) {
                session.addOutput(movieOutput)
                
                movieOutput.movieFragmentInterval = .invalid
                
                if let connection = movieOutput.connection(with: .video) {
                    if connection.isVideoStabilizationSupported {
                        connection.preferredVideoStabilizationMode = .auto
                    }
                    updateVideoOrientation(connection)
                }
            }
            
            if let device = device {
                try device.lockForConfiguration()
                let duration = CMTimeMake(value: 1000, timescale: Int32(selectedFrameRate * 1000))
                device.activeVideoMinFrameDuration = duration
                device.activeVideoMaxFrameDuration = duration
                device.unlockForConfiguration()
            }
            
        } catch {
            print("Error setting up camera: \(error)")
            self.error = .setupFailed
            session.commitConfiguration()
            return
        }
        
        session.commitConfiguration()
        
        if session.canSetSessionPreset(.hd4K3840x2160) {
            session.sessionPreset = .hd4K3840x2160
        } else if session.canSetSessionPreset(.hd1920x1080) {
            session.sessionPreset = .hd1920x1080
        }
        
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.session.startRunning()
            DispatchQueue.main.async {
                self?.isSessionRunning = self?.session.isRunning ?? false
                self?.status = .running
            }
        }
        
        isAppleLogSupported = device?.formats.contains { format in
            format.supportedColorSpaces.contains(.appleLog)
        } ?? false
        
        defaultFormat = device?.activeFormat
    }
    
    func updateWhiteBalance(_ temperature: Float) {
        guard let device = device else { return }
        do {
            try device.lockForConfiguration()
            let tnt = AVCaptureDevice.WhiteBalanceTemperatureAndTintValues(temperature: temperature, tint: 0.0)
            var gains = device.deviceWhiteBalanceGains(for: tnt)
            let maxGain = device.maxWhiteBalanceGain
            
            gains.redGain   = min(max(1.0, gains.redGain), maxGain)
            gains.greenGain = min(max(1.0, gains.greenGain), maxGain)
            gains.blueGain  = min(max(1.0, gains.blueGain), maxGain)
            
            device.setWhiteBalanceModeLocked(with: gains) { _ in }
            device.unlockForConfiguration()
            
            whiteBalance = temperature
        } catch {
            print("White balance error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    func updateISO(_ iso: Float) {
        guard let device = device else { return }
        do {
            try device.lockForConfiguration()
            let clamped = min(max(device.activeFormat.minISO, iso), device.activeFormat.maxISO)
            device.setExposureModeCustom(duration: device.exposureDuration, iso: clamped) { _ in }
            device.unlockForConfiguration()
            self.iso = clamped
        } catch {
            print("ISO error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    func updateShutterSpeed(_ speed: CMTime) {
        guard let device = device else { return }
        do {
            print("\n⚡ Updating Shutter Speed:")
            print("  - Input Time: \(speed.value)/\(speed.timescale)")
            print("  - Duration: \(speed.seconds) seconds")
            
            try device.lockForConfiguration()
            device.setExposureModeCustom(duration: speed, iso: device.iso) { _ in }
            device.unlockForConfiguration()
            
            DispatchQueue.main.async {
                self.shutterSpeed = speed
                let resultingAngle = Double(speed.value) / Double(speed.timescale) * self.selectedFrameRate * 360.0
                print("  - Resulting Angle: \(resultingAngle)°")
            }
        } catch {
            print("❌ Shutter speed error: \(error)")
            self.error = .configurationFailed
        }
    }
    
    func startRecording() {
        guard !isRecording && !isProcessingRecording else {
            print("Cannot start recording: Already in progress or processing")
            return
        }
        
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let videoName = "recording-\(Date().timeIntervalSince1970).mov"
        let videoPath = documentsPath.appendingPathComponent(videoName)
        currentRecordingURL = videoPath
        
        movieOutput.startRecording(to: videoPath, recordingDelegate: self)
        isRecording = true
        print("Starting recording to: \(videoPath.path)")
    }
    
    func stopRecording() {
        guard isRecording else {
            print("Cannot stop recording: No ongoing recording")
            return
        }
        
        print("Stopping recording...")
        isProcessingRecording = true
        movieOutput.stopRecording()
    }
    
    private func updateVideoOrientation(_ connection: AVCaptureConnection) {
        let requiredAngles: [CGFloat] = [0, 90, 180, 270]
        let supportsRotation = requiredAngles.allSatisfy { angle in
            connection.isVideoRotationAngleSupported(angle)
        }
        
        guard supportsRotation else { return }
        
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                let interfaceOrientation = windowScene.interfaceOrientation
                self.currentInterfaceOrientation = interfaceOrientation
                
                switch interfaceOrientation {
                case .portrait:
                    connection.videoRotationAngle = 90
                case .portraitUpsideDown:
                    connection.videoRotationAngle = 270
                case .landscapeLeft:
                    connection.videoRotationAngle = 180
                case .landscapeRight:
                    connection.videoRotationAngle = 0
                default:
                    connection.videoRotationAngle = 90
                }
            }
            
            if connection.isVideoMirroringSupported {
                connection.isVideoMirrored = false
            }
        }
    }
    
    private func findCompatibleFormat(for fps: Double) -> AVCaptureDevice.Format? {
        guard let device = device else { return nil }
        
        print("\n=== Checking Format Compatibility ===")
        print("Requested frame rate: \(fps) fps")
        
        // For 23.976 fps, we need to be more flexible with the range check
        // Some devices might report slightly different values like 23.97 or 23.98
        let targetFps = fps
        let tolerance = 0.01 // Allow for small rounding differences
        
        let formats = device.formats.filter { format in
            let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
            let isHighRes = dimensions.width >= 1920
            let supportsFrameRate = format.videoSupportedFrameRateRanges.contains { range in
                // For 23.976 fps specifically, use a slightly more flexible check
                if abs(targetFps - 23.976) < 0.001 {
                    // Check if any range contains ~23.976 fps with tolerance
                    return range.minFrameRate <= (targetFps - tolerance) && 
                           (targetFps + tolerance) <= range.maxFrameRate
                } else {
                    // Standard check for other frame rates
                    return range.minFrameRate <= targetFps && targetFps <= range.maxFrameRate
                }
            }
            
            if isAppleLogEnabled {
                return isHighRes && supportsFrameRate && format.supportedColorSpaces.contains(.appleLog)
            }
            return isHighRes && supportsFrameRate
        }
        
        formats.forEach { format in
            let dims = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
            let ranges = format.videoSupportedFrameRateRanges
            print("""
                Format: \(dims.width)x\(dims.height)
                - Frame rates: \(ranges.map { "\($0.minFrameRate)-\($0.maxFrameRate)" }.joined(separator: ", "))
                - Supports Apple Log: \(format.supportedColorSpaces.contains(.appleLog))
                """)
        }
        
        return formats.first
    }
    
    func updateFrameRate(_ fps: Double) {
        guard let device = device else { return }
        
        do {
            guard let compatibleFormat = findCompatibleFormat(for: fps) else {
                print("❌ No compatible format found for \(fps) fps")
                // Set error to allow user feedback
                DispatchQueue.main.async {
                    self.error = .configurationFailed(message: "This device doesn't support \(fps) fps recording")
                }
                return
            }
            
            try device.lockForConfiguration()
            
            if device.activeFormat != compatibleFormat {
                print("Switching to compatible format...")
                device.activeFormat = compatibleFormat
            }
            
            let frameDuration: CMTime
            switch fps {
            case 23.976:
                // For 23.976 fps, use the precise fraction 24000/1001
                frameDuration = FrameRates.ntsc23_976
                print("Setting 23.976 fps with duration \(FrameRates.ntsc23_976.value)/\(FrameRates.ntsc23_976.timescale)")
            case 29.97:
                frameDuration = FrameRates.ntsc29_97
            case 24:
                frameDuration = FrameRates.film24
            case 25:
                frameDuration = FrameRates.pal25
            case 30:
                frameDuration = FrameRates.ntsc30
            default:
                frameDuration = CMTimeMake(value: 1, timescale: Int32(fps))
            }
            
            // Set both min and max durations to lock the frame rate
            device.activeVideoMinFrameDuration = frameDuration
            device.activeVideoMaxFrameDuration = frameDuration
            
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                self.selectedFrameRate = fps
                self.frameCount = 0
                self.frameRateAccumulator = 0
                self.lastFrameTime = nil
            }
            
            print("""
                ✅ Frame rate configured:
                - Rate: \(fps) fps
                - Duration: \(frameDuration.seconds) seconds
                - Timescale/Value: \(frameDuration.timescale)/\(frameDuration.value)
                - Format: \(CMVideoFormatDescriptionGetDimensions(compatibleFormat.formatDescription))
                """)
            
            device.unlockForConfiguration()
            
            // Preserve current shutter angle by updating the shutter speed
            let currentAngle = shutterAngle
            updateShutterAngle(currentAngle)
        } catch {
            print("❌ Frame rate error: \(error)")
            self.error = .configurationFailed(message: "Failed to set \(fps) fps: \(error.localizedDescription)")
        }
    }
    
    private func adjustFrameRatePrecision(currentFPS: Double) {
        let deviation = abs(currentFPS - selectedFrameRate) / selectedFrameRate
        guard deviation > 0.02 else { return }
        
        let now = Date().timeIntervalSince1970
        guard (now - lastAdjustmentTime) > 1.0 else { return }
        
        lastAdjustmentTime = now
        
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.updateFrameRate(self.selectedFrameRate)
        }
    }
    
    private var lastAdjustmentTime: TimeInterval = 0
    
    func updateInterfaceOrientation() {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene {
                self.currentInterfaceOrientation = windowScene.interfaceOrientation
            }
        }
    }
    
    private func configureHDR() {
        guard let device = device,
              device.activeFormat.isVideoHDRSupported else { return }
        
        do {
            try device.lockForConfiguration()
            device.automaticallyAdjustsVideoHDREnabled = false
            device.isVideoHDREnabled = true
            device.unlockForConfiguration()
        } catch {
            print("Error configuring HDR: \(error)")
        }
    }
    
    private func optimizeVideoCapture() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            
            if device.activeFormat.isVideoStabilizationModeSupported(.cinematic) {
                if let connection = movieOutput.connection(with: .video),
                   connection.isVideoStabilizationSupported {
                    connection.preferredVideoStabilizationMode = .cinematic
                }
            }
            
            if device.isFocusModeSupported(.continuousAutoFocus) {
                device.focusMode = .continuousAutoFocus
            }
            
            if device.isExposureModeSupported(.continuousAutoExposure) {
                device.exposureMode = .continuousAutoExposure
            }
            
            if device.isWhiteBalanceModeSupported(.continuousAutoWhiteBalance) {
                device.whiteBalanceMode = .continuousAutoWhiteBalance
            }
            
            device.unlockForConfiguration()
        } catch {
            print("Error optimizing video capture: \(error)")
        }
    }
    
    private func configureTintSettings() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            if device.isWhiteBalanceModeSupported(.locked) {
                device.whiteBalanceMode = .locked
                
                let currentGains = device.deviceWhiteBalanceGains
                var newGains = currentGains
                let tintScale = currentTint / 150.0
                
                if tintScale > 0 {
                    newGains.greenGain = currentGains.greenGain * (1.0 + Float(tintScale))
                } else {
                    let magentaScale = 1.0 + Float(abs(tintScale))
                    newGains.redGain = currentGains.redGain * magentaScale
                    newGains.blueGain = currentGains.blueGain * magentaScale
                }
                
                let maxGain = device.maxWhiteBalanceGain
                newGains.redGain = min(max(1.0, newGains.redGain), maxGain)
                newGains.greenGain = min(max(1.0, newGains.greenGain), maxGain)
                newGains.blueGain = min(max(1.0, newGains.blueGain), maxGain)
                
                device.setWhiteBalanceModeLocked(with: newGains) { _ in }
            }
            device.unlockForConfiguration()
        } catch {
            print("Error setting tint: \(error.localizedDescription)")
            self.error = .whiteBalanceError
        }
    }
    
    func updateTint(_ newValue: Double) {
        currentTint = newValue.clamped(to: tintRange)
        configureTintSettings()
    }
    
    var shutterAngle: Double {
        get {
            let angle = Double(shutterSpeed.value) / Double(shutterSpeed.timescale) * selectedFrameRate * 360.0
            let clampedAngle = min(max(angle, 1.1), 360.0)
            
            return clampedAngle
        }
        set {
            let clampedAngle = min(max(newValue, 1.1), 360.0)
            let duration = (clampedAngle/360.0) * (1.0/selectedFrameRate)
            
            let time = CMTimeMakeWithSeconds(duration, preferredTimescale: 1000000)
            updateShutterSpeed(time)
            
            DispatchQueue.main.async {
                self.shutterSpeed = time
            }
        }
    }
    
    func updateShutterAngle(_ angle: Double) {
        print("\n🎯 Updating Shutter Angle:")
        print("  - Requested Angle: \(angle)°")
        self.shutterAngle = angle
    }
    
    private func updateExposureMode() {
        guard let device = device else { return }
        
        do {
            try device.lockForConfiguration()
            
            if isAutoExposureEnabled {
                if device.isExposureModeSupported(.continuousAutoExposure) {
                    device.exposureMode = .continuousAutoExposure
                    print("📷 Auto exposure enabled")
                }
            } else {
                if device.isExposureModeSupported(.custom) {
                    device.exposureMode = .custom
                    device.setExposureModeCustom(duration: device.exposureDuration,
                                                 iso: device.iso) { _ in }
                    print("📷 Manual exposure enabled")
                }
            }
            
            device.unlockForConfiguration()
        } catch {
            print("❌ Error setting exposure mode: \(error.localizedDescription)")
            self.error = .configurationFailed
        }
    }
    
    func processVideoFrame(_ sampleBuffer: CMSampleBuffer) -> CIImage? {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return nil }
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // Apply LUT if present
        if let lutFilter = lutManager.currentLUTFilter {
            lutFilter.setValue(ciImage, forKey: kCIInputImageKey)
            if let outputImage = lutFilter.outputImage {
                ciImage = outputImage
            }
        }
        
        return ciImage
    }
}

// MARK: - AVCaptureFileOutputRecordingDelegate
extension CameraViewModel: AVCaptureFileOutputRecordingDelegate {
    func fileOutput(_ output: AVCaptureFileOutput,
                    didStartRecordingTo fileURL: URL,
                    from connections: [AVCaptureConnection]) {
        print("Recording started successfully")
    }
    
    func fileOutput(_ output: AVCaptureFileOutput,
                    didFinishRecordingTo outputFileURL: URL,
                    from connections: [AVCaptureConnection],
                    error: Error?) {
        isRecording = false
        
        if let error = error {
            print("Recording failed: \(error.localizedDescription)")
            DispatchQueue.main.async {
                self.error = .recordingFailed
                self.isProcessingRecording = false
            }
            return
        }
        
        // Save to photo library
        PHPhotoLibrary.requestAuthorization { [weak self] status in
            guard status == .authorized else {
                DispatchQueue.main.async {
                    self?.error = .savingFailed
                    self?.isProcessingRecording = false
                    print("Photo library access denied")
                }
                return
            }
            
            PHPhotoLibrary.shared().performChanges({
                let options = PHAssetResourceCreationOptions()
                options.shouldMoveFile = true
                let creationRequest = PHAssetCreationRequest.forAsset()
                creationRequest.addResource(with: .video,
                                            fileURL: outputFileURL,
                                            options: options)
            }) { success, error in
                DispatchQueue.main.async {
                    if success {
                        print("Video saved to photo library")
                        self?.recordingFinished = true
                    } else {
                        print("Error saving video: \(String(describing: error))")
                        self?.error = .savingFailed
                    }
                    self?.isProcessingRecording = false
                }
            }
        }
    }
}

// MARK: - Orientation Helper
extension UIDeviceOrientation {
    var videoTransform: CGAffineTransform {
        switch self {
        case .landscapeRight:
            return CGAffineTransform(rotationAngle: CGFloat.pi)
        case .portraitUpsideDown:
            return CGAffineTransform(rotationAngle: -CGFloat.pi / 2)
        case .landscapeLeft:
            return .identity
        case .portrait:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        case .unknown, .faceUp, .faceDown:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        @unknown default:
            return CGAffineTransform(rotationAngle: CGFloat.pi / 2)
        }
    }
}

extension AVFrameRateRange {
    func containsFrameRate(_ fps: Double) -> Bool {
        return fps >= minFrameRate && fps <= maxFrameRate
    }
}

private extension Double {
    func clamped(to range: ClosedRange<Double>) -> Double {
        return min(max(self, range.lowerBound), range.upperBound)
    }
}

extension CameraError {
    static func configurationFailed(message: String = "Camera configuration failed") -> CameraError {
        return .custom(message: message)
    }
}

================
File: camera/Features/Camera/DocumentPicker.swift
================
import SwiftUI
import UniformTypeIdentifiers

struct DocumentPicker: UIViewControllerRepresentable {
    let types: [UTType]
    let onPick: (URL) -> Void
    
    func makeUIViewController(context: Context) -> UIDocumentPickerViewController {
        print("📄 DocumentPicker: Creating document picker for types: \(types.map { $0.identifier })")
        let picker = UIDocumentPickerViewController(forOpeningContentTypes: types)
        picker.delegate = context.coordinator
        picker.allowsMultipleSelection = false
        picker.shouldShowFileExtensions = true
        print("📄 DocumentPicker: Document picker created successfully")
        return picker
    }
    
    func updateUIViewController(_ uiViewController: UIDocumentPickerViewController, context: Context) {}
    
    func makeCoordinator() -> Coordinator {
        print("📄 DocumentPicker: Creating coordinator")
        return Coordinator(onPick: onPick)
    }
    
    class Coordinator: NSObject, UIDocumentPickerDelegate {
        let onPick: (URL) -> Void
        
        init(onPick: @escaping (URL) -> Void) {
            self.onPick = onPick
            super.init()
            print("📄 DocumentPicker: Coordinator initialized")
        }
        
        func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
            guard let url = urls.first else {
                print("❌ DocumentPicker: No document was selected")
                return
            }
            
            print("✅ DocumentPicker: Document selected at URL: \(url.path)")
            
            if !url.startAccessingSecurityScopedResource() {
                print("❌ DocumentPicker: Failed to access security scoped resource at \(url.path)")
                return
            }
            
            defer {
                url.stopAccessingSecurityScopedResource()
                print("📄 DocumentPicker: Stopped accessing security scoped resource")
            }
            
            guard FileManager.default.fileExists(atPath: url.path) else {
                print("❌ DocumentPicker: File does not exist at path: \(url.path)")
                return
            }
            
            do {
                let tempURL = FileManager.default.temporaryDirectory.appendingPathComponent(url.lastPathComponent)
                
                if FileManager.default.fileExists(atPath: tempURL.path) {
                    try FileManager.default.removeItem(at: tempURL)
                    print("📄 DocumentPicker: Removed existing file at temp location")
                }
                
                try FileManager.default.copyItem(at: url, to: tempURL)
                print("✅ DocumentPicker: Successfully copied file to: \(tempURL.path)")
                
                let fileAttributes = try FileManager.default.attributesOfItem(atPath: tempURL.path)
                if let fileSize = fileAttributes[.size] as? NSNumber {
                    print("📄 DocumentPicker: File size: \(fileSize.intValue) bytes")
                }
                
                DispatchQueue.main.async {
                    self.onPick(tempURL)
                }
            } catch {
                print("❌ DocumentPicker: Error handling selected file: \(error.localizedDescription)")
            }
        }
        
        func documentPickerWasCancelled(_ controller: UIDocumentPickerViewController) {
            print("📄 DocumentPicker: Document selection was cancelled")
        }
    }
}

================
File: camera/Features/Camera/ShutterAngle.swift
================
import Foundation

enum ShutterAngle: Double, CaseIterable {
    case angle_360 = 360.0  // 1/24
    case angle_345_6 = 345.6  // 1/25
    case angle_288 = 288.0  // 1/30
    case angle_262_2 = 262.2  // 1/33
    case angle_180 = 180.0  // 1/48
    case angle_172_8 = 172.8  // 1/50
    case angle_144 = 144.0  // 1/60
    case angle_90 = 90.0   // 1/96
    case angle_86_4 = 86.4  // 1/100
    case angle_72 = 72.0   // 1/120
    case angle_69_1 = 69.1  // 1/125
    case angle_34_6 = 34.6  // 1/250
    case angle_17_3 = 17.3  // 1/500
    case angle_8_6 = 8.6   // 1/1000
    case angle_4_3 = 4.3   // 1/2000
    case angle_2_2 = 2.2   // 1/4000
    case angle_1_1 = 1.1   // 1/8000
    
    var shutterSpeed: String {
        switch self {
        case .angle_360: return "1/24"
        case .angle_345_6: return "1/25"
        case .angle_288: return "1/30"
        case .angle_262_2: return "1/33"
        case .angle_180: return "1/48"
        case .angle_172_8: return "1/50"
        case .angle_144: return "1/60"
        case .angle_90: return "1/96"
        case .angle_86_4: return "1/100"
        case .angle_72: return "1/120"
        case .angle_69_1: return "1/125"
        case .angle_34_6: return "1/250"
        case .angle_17_3: return "1/500"
        case .angle_8_6: return "1/1000"
        case .angle_4_3: return "1/2000"
        case .angle_2_2: return "1/4000"
        case .angle_1_1: return "1/8000"
        }
    }
}

================
File: camera/Features/Camera/VideoOutputDelegate.swift
================
import AVFoundation
import CoreImage
import os.log

class VideoOutputDelegate: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    
    private let lutManager: LUTManager
    private let viewModel: CameraViewModel
    private let context: CIContext
    
    // Static variables to track frame count and LUT application
    private static var frameCount = 0
    private static var hasLUTBeenApplied = false
    
    init(lutManager: LUTManager, viewModel: CameraViewModel, context: CIContext) {
        self.lutManager = lutManager
        self.viewModel = viewModel
        self.context = context
        super.init()
        
        print("LUTManager status at init - Current LUT filter: \(lutManager.currentLUTFilter != nil ? "Available" : "Not available")")
        print("VideoOutputDelegate initialized successfully")
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // Increment frame count
        VideoOutputDelegate.frameCount += 1
        
        // Get the pixel buffer from the sample buffer
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            if VideoOutputDelegate.frameCount % 60 == 0 {
                print("⚠️ Could not get pixel buffer from sample buffer")
            }
            return
        }
        
        // Create a CIImage from the pixel buffer
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        var processedImage = ciImage
        
        // Apply LUT if available
        if let lutFilter = lutManager.currentLUTFilter {
            // Log LUT status every 60 frames or if it's the first time we've applied a LUT
            if VideoOutputDelegate.frameCount % 60 == 0 || !VideoOutputDelegate.hasLUTBeenApplied {
                print("🎨 LUT filter available - attempting to apply")
            }
            
            // Create a copy of the filter to avoid thread safety issues
            if let filterCopy = lutFilter.copy() as? CIFilter {
                // Set the input image
                filterCopy.setValue(ciImage, forKey: kCIInputImageKey)
                
                // Get the output image
                if let outputImage = filterCopy.outputImage {
                    // Use the processed image
                    processedImage = outputImage
                    
                    // Render the processed image back to the pixel buffer
                    context.render(outputImage, to: pixelBuffer)
                    
                    // Log success if it's the first time or every 60 frames
                    if !VideoOutputDelegate.hasLUTBeenApplied || VideoOutputDelegate.frameCount % 60 == 0 {
                        print("✅ LUT applied successfully to frame \(VideoOutputDelegate.frameCount)")
                        VideoOutputDelegate.hasLUTBeenApplied = true
                    }
                } else {
                    // Log failure if it's the first time or every 60 frames
                    if !VideoOutputDelegate.hasLUTBeenApplied || VideoOutputDelegate.frameCount % 60 == 0 {
                        print("❌ Failed to get output image from LUT filter")
                    }
                }
            } else {
                // Log failure if it's the first time or every 60 frames
                if !VideoOutputDelegate.hasLUTBeenApplied || VideoOutputDelegate.frameCount % 60 == 0 {
                    print("❌ Failed to copy LUT filter")
                }
            }
        } else {
            // Log LUT status every 120 frames
            if VideoOutputDelegate.frameCount % 120 == 0 {
                print("ℹ️ No LUT filter available")
            }
        }
        
        // Update the preview with the processed image
        viewModel.updatePreviewImage(ciImage: processedImage)
    }
}

================
File: camera/Features/LUT/CubeLUTLoader.swift
================
//
//  CubeLUTLoader.swift
//  camera
//
//  Created by spencer on 2024-12-30.
//

import Foundation
import CoreImage

/// Loads a .cube file and returns data for CIColorCube filter
class CubeLUTLoader {
    
    /// Parse .cube file into Float array and return dimension + LUT data
    static func loadCubeFile(name: String) throws -> (dimension: Int, data: [Float]) {
        guard let filePath = Bundle.main.url(forResource: name, withExtension: "cube") else {
            print("❌ CubeLUTLoader: LUT file '\(name).cube' not found in bundle")
            throw NSError(domain: "CubeLUTLoader", code: 1, userInfo: [NSLocalizedDescriptionKey: "LUT file not found"])
        }
        return try loadCubeFile(from: filePath)
    }
    
    /// Parse .cube file into Float array and return dimension + LUT data
    static func loadCubeFile(from url: URL) throws -> (dimension: Int, data: [Float]) {
        print("\n🔄 CubeLUTLoader: Loading LUT from \(url.path)")
        
        // First check if URL can be accessed (file might be security-scoped)
        if !FileManager.default.fileExists(atPath: url.path) {
            print("❌ CubeLUTLoader: File doesn't exist at path: \(url.path)")
            throw NSError(domain: "CubeLUTLoader", code: 5, userInfo: [
                NSLocalizedDescriptionKey: "LUT file not found at specified path"
            ])
        }
        
        // Start accessing security-scoped resource if needed
        var didStartAccess = false
        if url.startAccessingSecurityScopedResource() {
            didStartAccess = true
            print("✅ CubeLUTLoader: Started accessing security-scoped resource")
        }
        
        // Make sure we stop accessing the resource when we're done
        defer {
            if didStartAccess {
                url.stopAccessingSecurityScopedResource()
                print("✅ CubeLUTLoader: Stopped accessing security-scoped resource")
            }
        }
        
        // Validate file exists and is readable
        try validateLUTFile(at: url)
        
        let fileContents: String
        do {
            fileContents = try String(contentsOf: url, encoding: .utf8)
            print("✅ Read \(fileContents.count) characters from LUT file")
            
            // Print first few lines of the file for debugging
            let previewLines = fileContents.components(separatedBy: .newlines).prefix(5).joined(separator: "\n")
            print("📃 LUT File Preview (first 5 lines):\n\(previewLines)")
        } catch {
            print("❌ Failed to read LUT file: \(error.localizedDescription)")
            if let nsError = error as NSError? {
                print("❌ Error details - Domain: \(nsError.domain), Code: \(nsError.code)")
                for (key, value) in nsError.userInfo {
                    print("❌ UserInfo[\(key)] = \(value)")
                }
            }
            
            // Try alternate approach for binary files
            if (try? url.resourceValues(forKeys: [.contentTypeKey]))?.contentType?.conforms(to: .data) == true {
                print("⚠️ File appears to be binary data, attempting alternate loading method")
                throw NSError(domain: "CubeLUTLoader", code: 3, userInfo: [
                    NSLocalizedDescriptionKey: "Binary .cube files are not yet supported",
                    NSUnderlyingErrorKey: error
                ])
            }
            
            throw NSError(domain: "CubeLUTLoader", code: 3, userInfo: [
                NSLocalizedDescriptionKey: "Failed to read LUT file: \(error.localizedDescription)",
                NSUnderlyingErrorKey: error
            ])
        }
        
        let lines = fileContents.components(separatedBy: .newlines)
        print("📝 Parsing \(lines.count) lines from LUT file")
        
        var dimension = 0
        var cubeData = [Float]()
        var foundSize = false
        var headerEnded = false
        var linesProcessed = 0
        var dataLinesProcessed = 0
        var headerLines: [String] = []
        
        for line in lines {
            linesProcessed += 1
            let trimmed = line.trimmingCharacters(in: .whitespacesAndNewlines)
            
            // Skip comments and empty lines
            if trimmed.hasPrefix("#") || trimmed.isEmpty {
                if !headerEnded && !trimmed.isEmpty {
                    headerLines.append(trimmed)
                }
                continue
            }
            
            // Process LUT size
            if trimmed.lowercased().contains("lut_3d_size") {
                let parts = trimmed.components(separatedBy: CharacterSet.whitespaces)
                if let sizeString = parts.last, let size = Int(sizeString) {
                    dimension = size
                    let totalCount = size * size * size * 3
                    cubeData.reserveCapacity(totalCount)
                    foundSize = true
                    print("✅ Found LUT_3D_SIZE: \(size), expecting \(totalCount) data points")
                    headerLines.append(trimmed)
                } else {
                    print("⚠️ Found LUT_3D_SIZE but couldn't parse value: \(trimmed)")
                }
            } 
            // Process data lines
            else if dimension > 0 {
                if !headerEnded {
                    headerEnded = true
                    print("📋 LUT Header:\n\(headerLines.joined(separator: "\n"))")
                }
                
                // Parse RGB values on this line
                let components = trimmed.components(separatedBy: CharacterSet.whitespaces)
                    .filter { !$0.isEmpty }
                    .compactMap { Float($0) }
                
                if components.count == 3 {
                    cubeData.append(contentsOf: components)
                    dataLinesProcessed += 1
                    
                    // Print the first few data points for debugging
                    if dataLinesProcessed <= 3 {
                        print("📊 Data Line \(dataLinesProcessed): \(components)")
                    }
                } else if !trimmed.isEmpty {
                    print("⚠️ Line \(linesProcessed): Invalid data format - expected 3 values, got \(components.count): \(trimmed)")
                }
            }
        }
        
        let expectedDataLines = dimension * dimension * dimension
        print("📊 LUT Stats: Processed \(dataLinesProcessed) data lines out of expected \(expectedDataLines)")
        print("📊 Parsed \(cubeData.count) values, expected \(dimension * dimension * dimension * 3)")
        
        if dimension == 0 || !foundSize {
            print("❌ Missing LUT_3D_SIZE in file")
            throw NSError(domain: "CubeLUTLoader", code: 2, userInfo: [NSLocalizedDescriptionKey: "Invalid .cube file - Missing LUT_3D_SIZE"])
        }
        
        if cubeData.isEmpty {
            print("❌ No valid data found in LUT file")
            throw NSError(domain: "CubeLUTLoader", code: 2, userInfo: [NSLocalizedDescriptionKey: "Invalid or empty .cube file - No data found"])
        }
        
        // If we have at least some data but not the complete amount, warn but continue
        let expectedDataCount = dimension * dimension * dimension * 3
        if cubeData.count < expectedDataCount {
            print("⚠️ Incomplete LUT data: Found \(cubeData.count) values, expected \(expectedDataCount)")
            
            // If we're significantly short, throw an error
            if cubeData.count < expectedDataCount / 2 {
                print("❌ Insufficient LUT data: Found \(cubeData.count) values, expected \(expectedDataCount)")
                throw NSError(domain: "CubeLUTLoader", code: 4, userInfo: [
                    NSLocalizedDescriptionKey: "Incomplete LUT data: Only \(cubeData.count)/\(expectedDataCount) values found"
                ])
            }
            
            // For minor shortfalls, pad with zeros to maintain expected dimensions
            let shortfall = expectedDataCount - cubeData.count
            if shortfall > 0 && shortfall < expectedDataCount / 10 {  // Less than 10% missing
                print("⚠️ Padding \(shortfall) missing values with zeros")
                cubeData.append(contentsOf: Array(repeating: 0.0, count: shortfall))
            }
        }
        
        // Validate values are in reasonable range (0.0-1.0)
        let outOfRangeValues = cubeData.filter { $0 < 0.0 || $0 > 1.0 }
        if !outOfRangeValues.isEmpty {
            print("⚠️ Found \(outOfRangeValues.count) values outside the expected 0.0-1.0 range")
            print("⚠️ Example out-of-range values: \(outOfRangeValues.prefix(5))")
            
            // Clamp values to valid range
            for i in 0..<cubeData.count {
                cubeData[i] = max(0.0, min(1.0, cubeData[i]))
            }
            print("✅ Clamped all values to 0.0-1.0 range for compatibility")
        }
        
        print("✅ Successfully parsed LUT file with dimension \(dimension) and \(cubeData.count) values")
        return (dimension, cubeData)
    }
    
    /// Validates that a LUT file exists and is readable
    static func validateLUTFile(at url: URL) throws {
        print("🔍 Validating LUT file at \(url.path)")
        
        // Check if the file exists
        let fileManager = FileManager.default
        guard fileManager.fileExists(atPath: url.path) else {
            print("❌ LUT file does not exist at \(url.path)")
            throw NSError(domain: "CubeLUTLoader", code: 5, userInfo: [
                NSLocalizedDescriptionKey: "LUT file does not exist at path"
            ])
        }
        
        // Check if file is readable
        do {
            let resourceValues = try url.resourceValues(forKeys: [.isReadableKey, .fileSizeKey, .contentTypeKey])
            guard resourceValues.isReadable == true else {
                print("❌ LUT file is not readable")
                throw NSError(domain: "CubeLUTLoader", code: 6, userInfo: [
                    NSLocalizedDescriptionKey: "LUT file is not readable"
                ])
            }
            
            if let fileSize = resourceValues.fileSize {
                print("✅ LUT file size: \(fileSize) bytes")
                if fileSize == 0 {
                    print("❌ LUT file is empty (0 bytes)")
                    throw NSError(domain: "CubeLUTLoader", code: 7, userInfo: [
                        NSLocalizedDescriptionKey: "LUT file is empty"
                    ])
                }
            }
            
            if let contentType = resourceValues.contentType {
                print("✅ LUT file content type: \(contentType.identifier)")
            }
        } catch {
            print("❌ Failed to get resource values: \(error.localizedDescription)")
            throw NSError(domain: "CubeLUTLoader", code: 8, userInfo: [
                NSLocalizedDescriptionKey: "Failed to read LUT file properties",
                NSUnderlyingErrorKey: error
            ])
        }
        
        // Attempt to check file permissions
        do {
            let attributes = try fileManager.attributesOfItem(atPath: url.path)
            if let permissions = attributes[.posixPermissions] as? NSNumber {
                print("✅ LUT file permissions: \(String(format: "%o", permissions.intValue))")
            }
        } catch {
            print("⚠️ Could not read file attributes: \(error.localizedDescription)")
        }
        
        print("✅ LUT file validation passed")
    }
}

================
File: camera/Features/LUT/LUTManager.swift
================
import SwiftUI
import CoreImage
import UniformTypeIdentifiers

class LUTManager: ObservableObject {
    
    @Published var currentLUTFilter: CIFilter?
    private var dimension: Int = 0
    @Published var selectedLUTURL: URL?
    @Published var recentLUTs: [String: URL]? = [:]
    
    // Maximum number of recent LUTs to store
    private let maxRecentLUTs = 5
    
    // UserDefaults key for storing recent LUTs
    private let recentLUTsKey = "recentLUTs"
    
    public static let supportedTypes: [UTType] = [
        UTType(filenameExtension: "cube")!,  // .cube LUT files
        UTType(filenameExtension: "3dl")!    // .3dl LUT files
    ]
    
    init() {
        loadRecentLUTs()
    }
    
    func loadLUT(named fileName: String) {
        print("🔍 LUTManager: Attempting to load LUT file named '\(fileName)'")
        do {
            // First check if file exists
            guard let fileURL = Bundle.main.url(forResource: fileName, withExtension: "cube") else {
                print("❌ LUTManager Error: File '\(fileName).cube' not found in bundle")
                print("📂 Bundle path: \(Bundle.main.bundlePath)")
                print("📂 Available resources: \(Bundle.main.paths(forResourcesOfType: "cube", inDirectory: nil))")
                throw NSError(domain: "LUTManager", code: 1, userInfo: [NSLocalizedDescriptionKey: "LUT file not found in bundle"])
            }
            
            print("✅ LUT file found at: \(fileURL.path)")
            let lutInfo = try CubeLUTLoader.loadCubeFile(name: fileName)
            print("✅ LUT data loaded: dimension=\(lutInfo.dimension), data.count=\(lutInfo.data.count)")
            setupLUTFilter(lutInfo: lutInfo)
            
            // Save filename to recent LUTs
            addToRecentLUTs(url: fileURL)
            print("✅ LUT successfully loaded and configured")
        } catch {
            print("❌ LUTManager Error: Failed to load LUT '\(fileName)': \(error.localizedDescription)")
        }
    }
    
    func loadLUT(from url: URL) {
        print("\n📊 LUTManager: Attempting to load LUT from URL: \(url.path)")
        print("📊 LUTManager: URL is file URL: \(url.isFileURL)")
        print("📊 LUTManager: File exists: \(FileManager.default.fileExists(atPath: url.path))")
        
        // Check file attributes
        do {
            let attributes = try FileManager.default.attributesOfItem(atPath: url.path)
            if let fileSize = attributes[.size] as? NSNumber {
                print("📊 LUTManager: File size: \(fileSize.intValue) bytes")
            }
            if let fileType = attributes[.type] as? String {
                print("📊 LUTManager: File type: \(fileType)")
            }
        } catch {
            print("❌ LUTManager Error: Could not read file attributes: \(error.localizedDescription)")
        }
        
        // Try to read the first few bytes of the file to ensure it's readable
        do {
            let handle = try FileHandle(forReadingFrom: url)
            defer {
                try? handle.close()
            }
            let data = try handle.read(upToCount: 100)
            if let data = data, let preview = String(data: data, encoding: .utf8) {
                print("📊 LUTManager: File content preview: \(preview.prefix(50))")
            } else {
                print("📊 LUTManager: Could not read file content preview (may be binary data)")
            }
        } catch {
            print("❌ LUTManager Error: Could not read file content: \(error.localizedDescription)")
        }
        
        // Store the selected URL before trying to load
        selectedLUTURL = url
        
        do {
            // CRITICAL CHANGE: Since we're loading directly from the provided URL, 
            // we don't need to make another copy which could lead to file access issues
            let lutInfo = try CubeLUTLoader.loadCubeFile(from: url)
            print("✅ LUT data loaded from URL: dimension=\(lutInfo.dimension), data.count=\(lutInfo.data.count)")
            
            // Setup the filter
            setupLUTFilter(lutInfo: lutInfo)
            
            // Add to recent LUTs - use the original URL
            addToRecentLUTs(url: url)
            print("✅ LUT successfully loaded from URL and configured")
            
        } catch let error as NSError {
            print("❌ LUTManager Error: Failed to load LUT from URL: \(error.localizedDescription)")
            print("❌ LUTManager Error: Domain: \(error.domain), Code: \(error.code)")
            if let underlyingError = error.userInfo[NSUnderlyingErrorKey] as? NSError {
                print("❌ LUTManager Error: Underlying error: \(underlyingError.localizedDescription)")
            }
            
            for (key, value) in error.userInfo {
                print("❌ LUTManager Error: UserInfo[\(key)] = \(value)")
            }
            
            // If this is a file error, check file permissions and existence
            if error.domain == NSCocoaErrorDomain {
                print("❌ LUTManager Error: This is a Cocoa error, checking file details...")
                do {
                    let fileManager = FileManager.default
                    let resourceValues = try url.resourceValues(forKeys: [.isReadableKey, .isWritableKey])
                    print("❌ LUTManager Error: File is readable: \(resourceValues.isReadable ?? false)")
                    print("❌ LUTManager Error: File is writable: \(resourceValues.isWritable ?? false)")
                    print("❌ LUTManager Error: File exists: \(fileManager.fileExists(atPath: url.path))")
                } catch {
                    print("❌ LUTManager Error: Could not check resource values: \(error)")
                }
            }
            
            DispatchQueue.main.async {
                self.currentLUTFilter = nil
                self.selectedLUTURL = nil
            }
        }
    }
    
    private func setupLUTFilter(lutInfo: (dimension: Int, data: [Float])) {
        print("⚙️ Setting up LUT filter: dimension=\(lutInfo.dimension)")
        dimension = lutInfo.dimension
        
        // Create CIColorCube filter
        guard let filter = CIFilter(name: "CIColorCube") else {
            print("❌ Failed to create CIColorCube filter")
            return
        }
        print("✅ Created CIColorCube filter")
        
        // Convert [Float] of RGB to RGBA
        let rgbData = lutInfo.data
        var rgbaData = [Float](repeating: 0, count: dimension * dimension * dimension * 4)
        var idx = 0
        for i in stride(from: 0, to: rgbData.count, by: 3) {
            if i + 2 < rgbData.count {  // Safety check
                rgbaData[idx]   = rgbData[i]     // R
                rgbaData[idx+1] = rgbData[i+1]   // G
                rgbaData[idx+2] = rgbData[i+2]   // B
                rgbaData[idx+3] = 1.0            // A
                idx += 4
            }
        }
        print("✅ Converted RGB to RGBA data: count=\(rgbaData.count)")
        
        // Pass data to the filter using withUnsafeBufferPointer for safe memory management
        rgbaData.withUnsafeBufferPointer { pointer in
            let data = Data(buffer: pointer)
            filter.setValue(dimension, forKey: "inputCubeDimension")
            filter.setValue(data, forKey: "inputCubeData")
        }
        print("✅ LUT data passed to filter")
        
        DispatchQueue.main.async {
            self.currentLUTFilter = filter
            print("✅ Filter assigned to currentLUTFilter")
        }
    }
    
    func clearLUT() {
        currentLUTFilter = nil
        selectedLUTURL = nil
    }
    
    func applyLUT(to inputImage: CIImage) -> CIImage? {
        guard let filter = currentLUTFilter else { 
            print("❌ applyLUT: No filter available")
            return nil 
        }
        
        filter.setValue(inputImage, forKey: kCIInputImageKey)
        let outputImage = filter.outputImage
        
        if outputImage == nil {
            print("❌ applyLUT: Filter returned nil output image")
        }
        
        return outputImage
    }
    
    /// Sets up a LUT programmatically without needing a .cube file
    func setupProgrammaticLUT(dimension: Int, data: [Float]) {
        print("⚙️ Setting up programmatic LUT: dimension=\(dimension), data.count=\(data.count)")
        self.dimension = dimension
        
        // Create CIColorCube filter
        guard let filter = CIFilter(name: "CIColorCube") else {
            print("❌ Failed to create CIColorCube filter")
            return
        }
        print("✅ Created CIColorCube filter")
        
        // Convert [Float] of RGB to RGBA
        let rgbData = data
        var rgbaData = [Float](repeating: 0, count: dimension * dimension * dimension * 4)
        var idx = 0
        for i in stride(from: 0, to: rgbData.count, by: 3) {
            if i + 2 < rgbData.count {  // Safety check
                rgbaData[idx]   = rgbData[i]     // R
                rgbaData[idx+1] = rgbData[i+1]   // G
                rgbaData[idx+2] = rgbData[i+2]   // B
                rgbaData[idx+3] = 1.0            // A
                idx += 4
            }
        }
        print("✅ Converted RGB to RGBA data: count=\(rgbaData.count)")
        
        // Pass data to the filter
        rgbaData.withUnsafeBufferPointer { pointer in
            let data = Data(buffer: pointer)
            filter.setValue(dimension, forKey: "inputCubeDimension")
            filter.setValue(data, forKey: "inputCubeData")
        }
        print("✅ LUT data passed to filter")
        
        DispatchQueue.main.async {
            self.currentLUTFilter = filter
            self.selectedLUTURL = nil // No URL since this is programmatic
            print("✅ Programmatic filter assigned to currentLUTFilter")
        }
    }
    
    // MARK: - Recent LUTs Management
    
    /// Add a LUT to the recent LUTs list
    private func addToRecentLUTs(url: URL) {
        let fileName = url.lastPathComponent
        
        // Create a new dictionary for atomic update
        var updatedRecentLUTs = recentLUTs ?? [:]
        
        // Add or update the entry
        updatedRecentLUTs[fileName] = url
        
        // Trim if we exceed the maximum number
        if updatedRecentLUTs.count > maxRecentLUTs {
            // Remove oldest entries (sort by key alphabetically as a simple approach)
            let sortedKeys = updatedRecentLUTs.keys.sorted()
            let keysToRemove = sortedKeys.prefix(updatedRecentLUTs.count - maxRecentLUTs)
            for key in keysToRemove {
                updatedRecentLUTs.removeValue(forKey: key)
            }
        }
        
        // Update the published property
        recentLUTs = updatedRecentLUTs
        
        // Persist to UserDefaults as bookmarks
        saveRecentLUTs()
    }
    
    /// Save recent LUTs to UserDefaults
    private func saveRecentLUTs() {
        guard let recentLUTs = recentLUTs, !recentLUTs.isEmpty else {
            UserDefaults.standard.removeObject(forKey: recentLUTsKey)
            return
        }
        
        // Convert to serializable format
        var bookmarkData: [String: Data] = [:]
        
        for (key, url) in recentLUTs {
            do {
                let data = try url.bookmarkData(options: .minimalBookmark, includingResourceValuesForKeys: nil, relativeTo: nil)
                bookmarkData[key] = data
            } catch {
                print("Failed to create bookmark for LUT: \(error)")
            }
        }
        
        // Save to UserDefaults
        UserDefaults.standard.set(bookmarkData, forKey: recentLUTsKey)
    }
    
    /// Load recent LUTs from UserDefaults
    private func loadRecentLUTs() {
        guard let bookmarkData = UserDefaults.standard.dictionary(forKey: recentLUTsKey) as? [String: Data] else {
            return
        }
        
        var loadedLUTs: [String: URL] = [:]
        
        for (key, data) in bookmarkData {
            do {
                var isStale = false
                let url = try URL(resolvingBookmarkData: data, options: .withoutUI, relativeTo: nil, bookmarkDataIsStale: &isStale)
                
                if !isStale {
                    loadedLUTs[key] = url
                }
            } catch {
                print("Failed to resolve bookmark for LUT: \(error)")
            }
        }
        
        recentLUTs = loadedLUTs
    }
}

================
File: camera/Features/Settings/SettingsModel.swift
================
import Foundation
import AVFoundation
import CoreMedia

class SettingsModel: ObservableObject {
    @Published var isAppleLogEnabled: Bool {
        didSet {
            UserDefaults.standard.set(isAppleLogEnabled, forKey: "isAppleLogEnabled")
            NotificationCenter.default.post(name: .appleLogSettingChanged, object: nil)
        }
    }
    
    var isAppleLogSupported: Bool {
        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
            return false
        }
        
        // Check if any format supports Apple Log
        return device.formats.contains { format in
            let colorSpaces = format.supportedColorSpaces.map { $0.rawValue }
            return colorSpaces.contains(AVCaptureColorSpace.appleLog.rawValue)
        }
    }
    
    init() {
        self.isAppleLogEnabled = UserDefaults.standard.bool(forKey: "isAppleLogEnabled")
    }
}

extension Notification.Name {
    static let appleLogSettingChanged = Notification.Name("appleLogSettingChanged")
}

================
File: camera/Features/Settings/SettingsView.swift
================
import SwiftUI
import UniformTypeIdentifiers

struct SettingsView: View {
    @Environment(\.dismiss) private var dismiss
    @ObservedObject var lutManager: LUTManager
    @StateObject private var documentDelegate: LUTDocumentPickerDelegate
    @State private var showLUTHistory = false
    
    init(lutManager: LUTManager) {
        self.lutManager = lutManager
        // Initialize the document delegate with the LUT manager
        _documentDelegate = StateObject(wrappedValue: LUTDocumentPickerDelegate(lutManager: lutManager))
    }
    
    var body: some View {
        NavigationView {
            Form {
                Section(header: Text("Camera Settings")) {
                    // Other settings can go here
                }
                
                Section(header: Text("LUT Management")) {
                    // Current LUT display
                    if let selectedLUT = lutManager.selectedLUTURL?.lastPathComponent {
                        HStack {
                            Label {
                                Text("Current LUT")
                            } icon: {
                                Image(systemName: "photo.artframe")
                                    .foregroundColor(.blue)
                            }
                            Spacer()
                            Text(selectedLUT)
                                .foregroundColor(.secondary)
                        }
                        
                        Button(role: .destructive, action: {
                            lutManager.clearLUT()
                        }) {
                            Label("Clear LUT", systemImage: "xmark.circle")
                        }
                    } else {
                        Text("No LUT selected")
                            .foregroundColor(.secondary)
                    }
                    
                    // LUT import button
                    Button {
                        showDocumentPicker()
                    } label: {
                        Label("Import New LUT", systemImage: "square.and.arrow.down")
                    }
                    .foregroundColor(.blue)
                    
                    // Recently used LUTs
                    if let recentLUTs = lutManager.recentLUTs, !recentLUTs.isEmpty {
                        DisclosureGroup("Recent LUTs", isExpanded: $showLUTHistory) {
                            ForEach(recentLUTs.keys.sorted(), id: \.self) { lutName in
                                if let url = recentLUTs[lutName] {
                                    Button {
                                        lutManager.loadLUT(from: url)
                                    } label: {
                                        HStack {
                                            Image(systemName: "photo")
                                                .foregroundColor(.blue)
                                            Text(lutName)
                                            Spacer()
                                            if lutManager.selectedLUTURL == url {
                                                Image(systemName: "checkmark")
                                                    .foregroundColor(.green)
                                            }
                                        }
                                    }
                                    .foregroundColor(.primary)
                                }
                            }
                        }
                    }
                }
                
                Section(header: Text("LUT Info"), footer: Text("LUTs are applied after LOG conversion to ensure proper display")) {
                    Text("Supported formats: .cube, .3dl")
                        .font(.footnote)
                        .foregroundColor(.secondary)
                }
            }
            .navigationTitle("Settings")
            .navigationBarItems(trailing: Button("Done") {
                dismiss()
            })
        }
    }
    
    private func showDocumentPicker() {
        let picker = UIDocumentPickerViewController(
            forOpeningContentTypes: LUTManager.supportedTypes,
            asCopy: true
        )
        picker.delegate = documentDelegate
        
        if let windowScene = UIApplication.shared.connectedScenes.first as? UIWindowScene,
           let window = windowScene.windows.first,
           let rootViewController = window.rootViewController {
            rootViewController.present(picker, animated: true)
        }
    }
}

// Document picker delegate as a separate class
class LUTDocumentPickerDelegate: NSObject, UIDocumentPickerDelegate, ObservableObject {
    let lutManager: LUTManager
    
    init(lutManager: LUTManager) {
        self.lutManager = lutManager
        super.init()
        print("📄 LUTDocumentPickerDelegate: Initialized with LUTManager")
    }
    
    func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
        guard let selectedURL = urls.first else {
            print("❌ LUTDocumentPickerDelegate: No document was selected")
            return
        }
        
        print("✅ LUTDocumentPickerDelegate: Document selected at URL: \(selectedURL.path)")
        
        // Start accessing the security-scoped resource
        let securityScopedAccess = selectedURL.startAccessingSecurityScopedResource()
        if !securityScopedAccess {
            print("❌ LUTDocumentPickerDelegate: Failed to access security scoped resource at \(selectedURL.path)")
        } else {
            print("✅ LUTDocumentPickerDelegate: Successfully accessed security scoped resource")
        }
        
        // Make sure we stop accessing the resource when we're done
        defer {
            if securityScopedAccess {
                selectedURL.stopAccessingSecurityScopedResource()
                print("📄 LUTDocumentPickerDelegate: Stopped accessing security scoped resource")
            }
        }
        
        // Check if file exists
        guard FileManager.default.fileExists(atPath: selectedURL.path) else {
            print("❌ LUTDocumentPickerDelegate: File does not exist at path: \(selectedURL.path)")
            return
        }
        
        // Due to the issue with creating copies, we'll create a bookmark to the file instead
        // This allows the app to access the file later without needing to copy it
        do {
            let bookmarkData = try selectedURL.bookmarkData(options: .minimalBookmark, 
                                                  includingResourceValuesForKeys: nil, 
                                                  relativeTo: nil)
            
            // Use the bookmark to create a URL that we can use later
            var isStale = false
            let resolvedURL = try URL(resolvingBookmarkData: bookmarkData, 
                                     options: .withoutUI, 
                                     relativeTo: nil, 
                                     bookmarkDataIsStale: &isStale)
            
            if isStale {
                print("⚠️ LUTDocumentPickerDelegate: Bookmark is stale, creating a new one")
                // You might want to recreate the bookmark here if needed
            }
            
            print("✅ LUTDocumentPickerDelegate: Created bookmark for file")
            
            // Check if file is readable
            try checkFileIsReadable(at: selectedURL)
            
            // Pass the URL directly to LUTManager on the main thread
            DispatchQueue.main.async {
                self.lutManager.loadLUT(from: selectedURL)
            }
        } catch {
            print("❌ LUTDocumentPickerDelegate: Error handling selected file: \(error.localizedDescription)")
            if let nsError = error as NSError? {
                print("❌ Error details - Domain: \(nsError.domain), Code: \(nsError.code)")
                
                // If this is related to bookmark creation, try a fallback approach
                if nsError.domain == NSCocoaErrorDomain && nsError.code == 260 {
                    print("⚠️ LUTDocumentPickerDelegate: Trying fallback approach for file access")
                    
                    // Pass the URL directly to LUTManager as a fallback
                    DispatchQueue.main.async {
                        self.lutManager.loadLUT(from: selectedURL)
                    }
                }
            }
        }
    }
    
    func documentPickerWasCancelled(_ controller: UIDocumentPickerViewController) {
        print("📄 LUTDocumentPickerDelegate: Document selection was cancelled")
    }
    
    private func checkFileIsReadable(at url: URL) throws {
        print("🔍 LUTDocumentPickerDelegate: Checking if file is readable: \(url.path)")
        
        // Check if we can open the file for reading
        let fileHandle = try FileHandle(forReadingFrom: url)
        defer {
            try? fileHandle.close()
        }
        
        // Try to read a small chunk of data
        if let data = try fileHandle.read(upToCount: 100), !data.isEmpty {
            print("✅ LUTDocumentPickerDelegate: File is readable, read \(data.count) bytes")
            
            // If it's a text file, print a preview
            if let textPreview = String(data: data, encoding: .utf8) {
                print("📄 Content preview: \(textPreview.prefix(50))")
            } else {
                print("📄 File contains binary data (not UTF-8 text)")
            }
        } else {
            print("⚠️ LUTDocumentPickerDelegate: File is empty or unreadable")
            throw NSError(domain: "LUTDocumentPicker", code: 1, userInfo: [
                NSLocalizedDescriptionKey: "File is empty or could not be read"
            ])
        }
    }
}

#Preview {
    SettingsView(lutManager: LUTManager())
}

================
File: camera/Preview Content/Preview Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: camera/cameraApp.swift
================
//
//  cameraApp.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import SwiftUI

@main
struct cameraApp: App {
    let persistenceController = PersistenceController.shared

    var body: some Scene {
        WindowGroup {
            ContentView()
                .environment(\.managedObjectContext, persistenceController.container.viewContext)
        }
    }
}

================
File: camera/ContentView.swift
================
import SwiftUI
import CoreData
import CoreMedia

struct ContentView: View {
    @StateObject private var viewModel = CameraViewModel()
    @State private var orientation = UIDevice.current.orientation
    @StateObject private var lutManager = LUTManager()
    @State private var isShowingSettings = false
    @State private var isShowingDocumentPicker = false
    
    var body: some View {
        GeometryReader { geometry in
            ZStack {
                if viewModel.isSessionRunning {
                    CameraPreviewView(
                        session: viewModel.session,
                        lutManager: lutManager,
                        viewModel: viewModel
                    )
                    .ignoresSafeArea()
                    .frame(width: geometry.size.width,
                           height: geometry.size.height)
                    
                    VStack {
                        Spacer()
                        controlsView
                            .frame(maxWidth: geometry.size.width * 0.9)
                            .padding(.bottom, 30)
                    }
                } else {
                    // Loading indicator while camera session initializes
                    ProgressView("Initializing Camera...")
                        .frame(maxWidth: .infinity, maxHeight: .infinity)
                        .background(Color.black.opacity(0.7))
                }
            }
            .edgesIgnoringSafeArea(.all)
            .onRotate { newOrientation in
                orientation = newOrientation
            }
        }
        .onAppear {
            viewModel.updateInterfaceOrientation()
        }
        .onChange(of: UIDevice.current.orientation) { oldValue, newValue in
            viewModel.updateInterfaceOrientation()
        }
        .alert(item: $viewModel.error) { error in
            Alert(title: Text("Error"),
                  message: Text(error.description),
                  dismissButton: .default(Text("OK")))
        }
        .sheet(isPresented: $isShowingSettings) {
            SettingsView(lutManager: lutManager)
        }
        .sheet(isPresented: $isShowingDocumentPicker) {
            DocumentPicker(types: LUTManager.supportedTypes) { url in
                // Use main thread for UI updates
                DispatchQueue.main.async {
                    handleLUTImport(url: url)
                    isShowingDocumentPicker = false
                }
            }
        }
    }
    
    // Camera controls
    private var controlsView: some View {
        VStack(spacing: 15) {
            Text("Camera Controls")
                .font(.headline)
            
            // Frame Rate Picker
            HStack {
                Text("FPS:")
                Picker("Frame Rate", selection: $viewModel.selectedFrameRate) {
                    ForEach(viewModel.availableFrameRates, id: \.self) { fps in
                        Text(
                            fps == 29.97 ? "29.97" : 
                            fps == 23.976 ? "23.98" : 
                            String(format: "%.0f", fps)
                        )
                        .tag(fps)
                    }
                }
                .pickerStyle(.menu)
                .onChange(of: viewModel.selectedFrameRate) { oldValue, newValue in
                    viewModel.updateFrameRate(newValue)
                }
            }
            
            // White Balance
            HStack {
                Text("WB: \(Int(viewModel.whiteBalance))K")
                Slider(value: $viewModel.whiteBalance,
                       in: 2000...8000,
                       step: 100) { _ in
                    viewModel.updateWhiteBalance(viewModel.whiteBalance)
                }
            }
            
            // Tint Control
            HStack {
                Text("Tint: \(Int(viewModel.currentTint))")
                Slider(
                    value: $viewModel.currentTint,
                    in: -150...150,
                    step: 1
                ) { _ in
                    viewModel.updateTint(viewModel.currentTint)
                }
                .tint(.green)
            }
            
            // ISO
            HStack {
                Text("ISO: \(Int(viewModel.iso))")
                Slider(value: $viewModel.iso,
                       in: viewModel.minISO...viewModel.maxISO,
                       step: 1) { _ in
                    viewModel.updateISO(viewModel.iso)
                }
            }
            .disabled(viewModel.isAutoExposureEnabled)
            .opacity(viewModel.isAutoExposureEnabled ? 0.6 : 1.0)
            
            // Shutter
            HStack {
                let currentAngle = viewModel.shutterAngle
                Text("Shutter: \(Int(currentAngle))° (\(ShutterAngle(rawValue: currentAngle)?.shutterSpeed ?? "Custom"))")
                
                Picker("Shutter Angle", selection: Binding(
                    get: {
                        // Find the closest standard angle
                        ShutterAngle.allCases.min(by: { abs($0.rawValue - viewModel.shutterAngle) < abs($1.rawValue - viewModel.shutterAngle) })?.rawValue ?? 180.0
                    },
                    set: { newValue in
                        print("\n🎚️ Shutter Angle Changed:")
                        print("  - New Value: \(newValue)°")
                        viewModel.updateShutterAngle(newValue)
                    }
                )) {
                    ForEach(ShutterAngle.allCases, id: \.rawValue) { angle in
                        Text("\(Int(angle.rawValue))° (\(angle.shutterSpeed))")
                            .tag(angle.rawValue)
                    }
                }
                .pickerStyle(.menu)
            }
            .disabled(viewModel.isAutoExposureEnabled)
            .opacity(viewModel.isAutoExposureEnabled ? 0.6 : 1.0)
            
            // LUT Controls
            VStack(spacing: 8) {
                HStack {
                    Text("LUT Preview")
                    Spacer()
                    if lutManager.currentLUTFilter != nil {
                        Image(systemName: "checkmark.circle.fill")
                            .foregroundColor(.green)
                    }
                    Toggle("", isOn: Binding(
                        get: { lutManager.currentLUTFilter != nil },
                        set: { enabled in
                            if !enabled {
                                lutManager.clearLUT()
                            } else if let url = lutManager.selectedLUTURL {
                                lutManager.loadLUT(from: url)
                            } else {
                                isShowingDocumentPicker = true
                            }
                        }
                    ))
                    .labelsHidden()
                }
                .tint(.green)
                
                if let lutName = lutManager.selectedLUTURL?.lastPathComponent {
                    HStack {
                        Text(lutName)
                            .font(.caption)
                            .foregroundColor(.secondary)
                        Spacer()
                        Button(action: {
                            lutManager.clearLUT()
                        }) {
                            Image(systemName: "xmark.circle.fill")
                                .foregroundColor(.red)
                        }
                    }
                }
                
                Button(action: {
                    isShowingDocumentPicker = true
                }) {
                    HStack {
                        Image(systemName: "photo.fill")
                        Text("Import LUT")
                    }
                    .frame(maxWidth: .infinity)
                    .padding(.vertical, 8)
                }
                .buttonStyle(.bordered)
                .tint(.blue)
            }
            .padding(.vertical, 4)

            // Auto Exposure toggle
            Toggle(isOn: $viewModel.isAutoExposureEnabled) {
                HStack {
                    Text("Auto Exposure")
                    if viewModel.isAutoExposureEnabled {
                        Image(systemName: "a.circle.fill")
                            .foregroundColor(.green)
                    } else {
                        Image(systemName: "m.circle.fill")
                            .foregroundColor(.orange)
                    }
                }
            }
            .tint(.green)
            
            // Apple Log toggle if supported
            if viewModel.isAppleLogSupported {
                Toggle(isOn: $viewModel.isAppleLogEnabled) {
                    HStack {
                        Text("Enable LOG")
                        if viewModel.isAppleLogEnabled {
                            Image(systemName: "checkmark.circle.fill")
                                .foregroundColor(.green)
                        }
                    }
                }
                .tint(.green)
            }
            
            // Record button
            Button(action: {
                if viewModel.isRecording {
                    viewModel.stopRecording()
                } else {
                    viewModel.startRecording()
                }
            }) {
                Image(systemName: viewModel.isRecording ? "stop.circle" : "record.circle")
                    .font(.system(size: 60))
                    .foregroundColor(viewModel.isRecording ? .white : .red)
                    .opacity(viewModel.isProcessingRecording ? 0.5 : 1.0)
            }
            .disabled(viewModel.isProcessingRecording)
        }
        .padding()
        .background(Color.black.opacity(0.6))
        .cornerRadius(15)
        .foregroundColor(.white)
    }
    
    // MARK: - LUT File Handling
    private func handleLUTImport(url: URL) {
        print("\n📱 ContentView: Handling LUT import from: \(url.path)")
        
        // Add a longer delay to ensure the UI has fully updated before performing file operations
        // This helps prevent view service termination errors
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            do {
                // Check if file exists and is accessible
                guard FileManager.default.fileExists(atPath: url.path) else {
                    print("❌ ContentView: LUT file does not exist at path: \(url.path)")
                    return
                }
                
                // Get file attributes
                let attributes = try FileManager.default.attributesOfItem(atPath: url.path)
                if let fileSize = attributes[.size] as? NSNumber {
                    print("📱 ContentView: LUT file size: \(fileSize.intValue) bytes")
                    
                    // Warn if file is suspiciously small
                    if fileSize.intValue < 100 {
                        print("⚠️ ContentView: LUT file seems unusually small (\(fileSize.intValue) bytes)")
                    }
                }
                
                // Directly pass the URL to LUTManager, which now handles the file properly
                // without trying to create additional copies
                self.lutManager.loadLUT(from: url)
                
            } catch {
                print("❌ ContentView: Error handling LUT file: \(error.localizedDescription)")
                if let nsError = error as NSError? {
                    print("❌ ContentView: Error domain: \(nsError.domain), code: \(nsError.code)")
                    if let underlyingError = nsError.userInfo[NSUnderlyingErrorKey] as? NSError {
                        print("❌ ContentView: Underlying error: \(underlyingError.localizedDescription)")
                    }
                }
            }
        }
    }
}

// A rotation view modifier to track device orientation changes
struct DeviceRotationViewModifier: ViewModifier {
    let action: (UIDeviceOrientation) -> Void
    
    func body(content: Content) -> some View {
        content
            .onAppear()
            .onReceive(NotificationCenter.default.publisher(for: UIDevice.orientationDidChangeNotification)) { _ in
                action(UIDevice.current.orientation)
            }
    }
}

extension View {
    func onRotate(perform action: @escaping (UIDeviceOrientation) -> Void) -> some View {
        self.modifier(DeviceRotationViewModifier(action: action))
    }
}

================
File: camera/Info.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIApplicationSceneManifest</key>
	<dict>
		<key>UIApplicationSupportsMultipleScenes</key>
		<false/>
	</dict>
	<key>NSCameraUsageDescription</key>
	<string>This app needs camera access to record video</string>
	<key>NSMicrophoneUsageDescription</key>
	<string>This app needs microphone access to record audio</string>
	<key>LSSupportsOpeningDocumentsInPlace</key>
	<true/>
	<key>UIFileSharingEnabled</key>
	<true/>
</dict>
</plist>

================
File: camera/My3DLUTFile.cube
================
# My3DLUTFile.cube - Default LUT file for camera app
# A simple 3D LUT that enhances colors slightly for LOG footage

LUT_3D_SIZE 8
LUT_3D_INPUT_RANGE 0.0 1.0

# LUT data follows (R G B values)
# Enhanced contrast with slightly boosted colors
0.0 0.0 0.0
0.15 0.12 0.15
0.32 0.30 0.35
0.52 0.50 0.54
0.73 0.70 0.73
0.95 0.92 0.95
0.15 0.12 0.15
0.32 0.30 0.35
0.52 0.50 0.54
0.73 0.70 0.73
0.87 0.85 0.88
0.97 0.95 0.97
0.32 0.30 0.35
0.46 0.44 0.48
0.62 0.59 0.63
0.78 0.75 0.79
0.90 0.88 0.91
0.98 0.96 0.98
0.52 0.50 0.54
0.65 0.63 0.67
0.76 0.73 0.77
0.86 0.84 0.87
0.94 0.92 0.95
0.99 0.97 0.99
0.73 0.70 0.73
0.80 0.78 0.82
0.87 0.85 0.88
0.93 0.91 0.94
0.97 0.95 0.97
1.0 0.98 1.0
0.95 0.92 0.95
0.97 0.95 0.97
0.98 0.96 0.98
0.99 0.97 0.99
1.0 0.98 1.0
1.0 1.0 1.0
# Additional entries to complete the 8x8x8 grid (simplified for brevity)
# In a real implementation, we would need all 512 entries for an 8x8x8 LUT
0.0 0.0 0.0
0.15 0.12 0.15
0.32 0.30 0.35
0.52 0.50 0.54
0.73 0.70 0.73
0.95 0.92 0.95
0.15 0.12 0.15
0.32 0.30 0.35
0.52 0.50 0.54
0.73 0.70 0.73
0.87 0.85 0.88
0.97 0.95 0.97
0.32 0.30 0.35
0.46 0.44 0.48
0.62 0.59 0.63
0.78 0.75 0.79
0.90 0.88 0.91
0.98 0.96 0.98
0.52 0.50 0.54
0.65 0.63 0.67
0.76 0.73 0.77
0.86 0.84 0.87
0.94 0.92 0.95
0.99 0.97 0.99
0.73 0.70 0.73
0.80 0.78 0.82
0.87 0.85 0.88
0.93 0.91 0.94
0.97 0.95 0.97
1.0 0.98 1.0
0.95 0.92 0.95
0.97 0.95 0.97
0.98 0.96 0.98
0.99 0.97 0.99
1.0 0.98 1.0
1.0 1.0 1.0
# For a complete LUT, we would need to continue with all 512 entries

================
File: camera/Persistence.swift
================
//
//  Persistence.swift
//  camera
//
//  Created by spencer on 2024-12-22.
//

import CoreData

struct PersistenceController {
    static let shared = PersistenceController()

    @MainActor
    static let preview: PersistenceController = {
        let result = PersistenceController(inMemory: true)
        let viewContext = result.container.viewContext
        for _ in 0..<10 {
            let newItem = Item(context: viewContext)
            newItem.timestamp = Date()
        }
        do {
            try viewContext.save()
        } catch {
            // Replace this implementation with code to handle the error appropriately.
            // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.
            let nsError = error as NSError
            fatalError("Unresolved error \(nsError), \(nsError.userInfo)")
        }
        return result
    }()

    let container: NSPersistentContainer

    init(inMemory: Bool = false) {
        container = NSPersistentContainer(name: "camera")
        if inMemory {
            container.persistentStoreDescriptions.first!.url = URL(fileURLWithPath: "/dev/null")
        }
        container.loadPersistentStores(completionHandler: { (storeDescription, error) in
            if let error = error as NSError? {
                // Replace this implementation with code to handle the error appropriately.
                // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.

                /*
                 Typical reasons for an error here include:
                 * The parent directory does not exist, cannot be created, or disallows writing.
                 * The persistent store is not accessible, due to permissions or data protection when the device is locked.
                 * The device is out of space.
                 * The store could not be migrated to the current model version.
                 Check the error message to determine what the actual problem was.
                 */
                fatalError("Unresolved error \(error), \(error.userInfo)")
            }
        })
        container.viewContext.automaticallyMergesChangesFromParent = true
    }
}

================
File: camera/TestLUT.cube
================
# Simple warm tone 3D LUT for camera app
# Created for testing purposes

LUT_3D_SIZE 8
LUT_3D_INPUT_RANGE 0.0 1.0

# LUT data follows (R G B values)
# Slightly warm color look
0.0 0.0 0.0
0.2 0.15 0.1
0.4 0.32 0.25
0.6 0.53 0.45
0.8 0.75 0.65
1.0 0.95 0.85
0.2 0.15 0.1
0.34 0.28 0.2
0.5 0.42 0.32
0.7 0.62 0.52
0.85 0.8 0.7
1.0 0.95 0.85
0.4 0.32 0.25
0.5 0.42 0.32
0.6 0.53 0.45
0.75 0.68 0.58
0.9 0.85 0.75
1.0 0.95 0.85
0.6 0.53 0.45
0.7 0.62 0.52
0.75 0.68 0.58
0.85 0.78 0.68
0.95 0.9 0.8
1.0 0.95 0.85
0.8 0.75 0.65
0.85 0.8 0.7
0.9 0.85 0.75
0.95 0.9 0.8
0.98 0.94 0.84
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 1.0 0.9
# Additional entries to complete the 8x8x8 grid (simplified for brevity)
# Fill with values that create a warm look
0.0 0.0 0.0
0.2 0.15 0.1
0.4 0.32 0.25
0.6 0.53 0.45
0.8 0.75 0.65
1.0 0.95 0.85
0.2 0.15 0.1
0.34 0.28 0.2
0.5 0.42 0.32
0.7 0.62 0.52
0.85 0.8 0.7
1.0 0.95 0.85
0.4 0.32 0.25
0.5 0.42 0.32
0.6 0.53 0.45
0.75 0.68 0.58
0.9 0.85 0.75
1.0 0.95 0.85
0.6 0.53 0.45
0.7 0.62 0.52
0.75 0.68 0.58
0.85 0.78 0.68
0.95 0.9 0.8
1.0 0.95 0.85
0.8 0.75 0.65
0.85 0.8 0.7
0.9 0.85 0.75
0.95 0.9 0.8
0.98 0.94 0.84
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 0.95 0.85
1.0 1.0 0.9
# Continue for all 512 entries (8^3)
# For brevity, we're not including all entries
# A real LUT would have all 512 entries for an 8x8x8 LUT

================
File: camera.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcdebugger/Breakpoints_v2.xcbkptlist
================
<?xml version="1.0" encoding="UTF-8"?>
<Bucket
   uuid = "8C0EC471-D08D-4370-AFA9-6904E3CF5246"
   type = "1"
   version = "2.0">
</Bucket>

================
File: camera.xcodeproj/xcuserdata/spencer.xcuserdatad/xcschemes/xcschememanagement.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>SchemeUserState</key>
	<dict>
		<key>camera.xcscheme_^#shared#^_</key>
		<dict>
			<key>orderHint</key>
			<integer>0</integer>
		</dict>
	</dict>
</dict>
</plist>

================
File: camera.xcodeproj/project.pbxproj
================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXFileReference section */
		09562D7C2D18A1EC009A9B07 /* camera.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = camera.app; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = 09562D7B2D18A1EC009A9B07 /* camera */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		09562D7E2D18A1EC009A9B07 /* camera */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				09562DB12D18A28C009A9B07 /* Exceptions for "camera" folder in "camera" target */,
			);
			path = camera;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		09562D792D18A1EC009A9B07 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		09562D732D18A1EC009A9B07 = {
			isa = PBXGroup;
			children = (
				09562D7E2D18A1EC009A9B07 /* camera */,
				09562D7D2D18A1EC009A9B07 /* Products */,
			);
			sourceTree = "<group>";
		};
		09562D7D2D18A1EC009A9B07 /* Products */ = {
			isa = PBXGroup;
			children = (
				09562D7C2D18A1EC009A9B07 /* camera.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		09562D7B2D18A1EC009A9B07 /* camera */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */;
			buildPhases = (
				09562D782D18A1EC009A9B07 /* Sources */,
				09562D792D18A1EC009A9B07 /* Frameworks */,
				09562D7A2D18A1EC009A9B07 /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				09562D7E2D18A1EC009A9B07 /* camera */,
			);
			name = camera;
			packageProductDependencies = (
			);
			productName = camera;
			productReference = 09562D7C2D18A1EC009A9B07 /* camera.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		09562D742D18A1EC009A9B07 /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1620;
				LastUpgradeCheck = 1620;
				TargetAttributes = {
					09562D7B2D18A1EC009A9B07 = {
						CreatedOnToolsVersion = 16.2;
					};
				};
			};
			buildConfigurationList = 09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 09562D732D18A1EC009A9B07;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = 09562D7D2D18A1EC009A9B07 /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				09562D7B2D18A1EC009A9B07 /* camera */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		09562D7A2D18A1EC009A9B07 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		09562D782D18A1EC009A9B07 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		09562D8D2D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		09562D8E2D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.2;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		09562D902D18A1EE009A9B07 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		09562D912D18A1EE009A9B07 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"camera/Preview Content\"";
				DEVELOPMENT_TEAM = 3B883XKLK8;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = camera/Info.plist;
				INFOPLIST_KEY_NSCameraUsageDescription = "This app needs camera access to record video";
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "This app needs microphone access to record audio";
				INFOPLIST_KEY_NSPhotoLibraryAddUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "This app needs access to save recorded videos";
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.spencershwetz.camera;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		09562D772D18A1EC009A9B07 /* Build configuration list for PBXProject "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D8D2D18A1EE009A9B07 /* Debug */,
				09562D8E2D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		09562D8F2D18A1EE009A9B07 /* Build configuration list for PBXNativeTarget "camera" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				09562D902D18A1EE009A9B07 /* Debug */,
				09562D912D18A1EE009A9B07 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = 09562D742D18A1EC009A9B07 /* Project object */;
}
